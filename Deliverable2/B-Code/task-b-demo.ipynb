{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71917d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and embedding matrix loaded successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4d6d1eed474111a3eaa11f00f9fbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: data\\predictionsDemo.csv\n"
     ]
    }
   ],
   "source": [
    "#DEMO SCRIPT - loads a trained model and runs inference on a test set\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 1. Define model\n",
    "########################################\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
    "        self.v = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_outputs, mask=None):\n",
    "        # lstm_outputs: (B, L, 2H)\n",
    "        score = torch.tanh(self.W(lstm_outputs))    # (B, L, 2H)\n",
    "        score = self.v(score).squeeze(-1)          # (B, L)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = F.softmax(score, dim=-1)    # (B, L)\n",
    "        attn_weights = attn_weights.unsqueeze(1)   # (B, 1, L)\n",
    "        context = torch.bmm(attn_weights, lstm_outputs)  # (B, 1, 2H)\n",
    "        context = context.squeeze(1)               # (B, 2H)\n",
    "        return context\n",
    "\n",
    "\n",
    "class CustomBiLSTMModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 embed_dim=300,\n",
    "                 hidden_dim=256, \n",
    "                 num_labels=2,\n",
    "                 num_layers=2, \n",
    "                 dropout=0.3, \n",
    "                 use_attention=True,\n",
    "                 use_focal_loss=False, \n",
    "                 gamma=2.0, \n",
    "                 label_smoothing=0.0,\n",
    "                 embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            with torch.no_grad():\n",
    "                self.embedding.weight.copy_(embedding_matrix)\n",
    "\n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Optional attention\n",
    "        if self.use_attention:\n",
    "            self.attn = SimpleAttention(self.hidden_dim)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(2 * self.hidden_dim, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        # Embeddings\n",
    "        embeds = self.embedding(input_ids)\n",
    "        if attention_mask is not None:\n",
    "            expand_mask = attention_mask.unsqueeze(-1).float()\n",
    "            embeds = embeds * expand_mask\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_outputs, (h, c) = self.lstm(embeds)  # (B, L, 2H), h shape: (2*num_layers, B, H)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            context = self.attn(lstm_outputs, mask=attention_mask) \n",
    "        else:\n",
    "            # Take the last forward and backward states from the top LSTM layer\n",
    "            h_forward = h[-2]  # last layer's forward state\n",
    "            h_backward = h[-1] # last layer's backward state\n",
    "            context = torch.cat((h_forward, h_backward), dim=-1)\n",
    "\n",
    "        logits = self.classifier(context)\n",
    "        return {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 2. Load the Trained Model Checkpoint\n",
    "########################################\n",
    "\n",
    "BEST_MODEL_PATH = \"data\\\\taskB\\\\ED_B_Model.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    BEST_MODEL_PATH, \n",
    "    map_location=device, \n",
    "    weights_only=False\n",
    ")\n",
    "\n",
    "# Extract hyperparams & embedding matrix from the checkpoint\n",
    "hyperparams = checkpoint[\"hyperparams\"]\n",
    "embedding_matrix_tensor = checkpoint[\"embedding_matrix\"].to(device)\n",
    "\n",
    "# Re-instantiate the model\n",
    "loaded_model = CustomBiLSTMModel(\n",
    "    vocab_size      = hyperparams[\"vocab_size\"],\n",
    "    embed_dim       = hyperparams[\"embed_dim\"],\n",
    "    hidden_dim      = hyperparams[\"hidden_dim\"],\n",
    "    num_labels      = hyperparams[\"num_labels\"],\n",
    "    num_layers      = hyperparams[\"num_layers\"],\n",
    "    dropout         = hyperparams[\"dropout\"],\n",
    "    use_attention   = hyperparams[\"use_attention\"],\n",
    "    use_focal_loss  = hyperparams[\"use_focal_loss\"],\n",
    "    gamma           = hyperparams[\"gamma\"],\n",
    "    label_smoothing = hyperparams[\"label_smoothing\"],\n",
    "    embedding_matrix=embedding_matrix_tensor\n",
    ")\n",
    "\n",
    "loaded_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model and embedding matrix loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3. Load and Preprocess Test Data\n",
    "########################################\n",
    "TEST_PATH = \"data\\\\test.csv\"  \n",
    "OUTPUT_PATH = \"data\\\\predictionsDemo.csv\"\n",
    "\n",
    "# Load test CSV with pandas\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "test_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "TOKENIZER_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"claim\"],\n",
    "        examples[\"evidence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenize\n",
    "encoded_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original text columns \n",
    "encoded_test = encoded_test.remove_columns([\"claim\", \"evidence\"])\n",
    "\n",
    "# Set to PyTorch format\n",
    "encoded_test.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 4. Run Inference on the Test Set\n",
    "########################################\n",
    "test_loader = DataLoader(encoded_test, batch_size=8)\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move each tensor in the batch to the same device as the model\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = loaded_model(**batch)   # forward pass\n",
    "        logits = outputs[\"logits\"]        # shape: (B, num_labels)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 5. Save Predictions\n",
    "########################################\n",
    "# Create a DataFrame with the predicted labels\n",
    "test_pred_df = pd.DataFrame({\"prediction\": all_preds})\n",
    "\n",
    "# Save to CSV\n",
    "test_pred_df.to_csv(OUTPUT_PATH, index=False, header=True)\n",
    "print(f\"Predictions saved to: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
