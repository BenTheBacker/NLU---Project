{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:44:42.457578Z",
     "iopub.status.busy": "2025-04-01T17:44:42.457314Z",
     "iopub.status.idle": "2025-04-01T17:45:03.004922Z",
     "shell.execute_reply": "2025-04-01T17:45:03.004134Z",
     "shell.execute_reply.started": "2025-04-01T17:44:42.457550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Global Variables and Flags\n",
    "# ----------------------------------------------------------------------------\n",
    "TESTING_FLAG = True  # If True, print debug info\n",
    "DOWNLOAD_FLAG = True # If True, handle NLTK data\n",
    "\n",
    "NLTK_DATA_DIR = \"data\\\\nltk_data\"\n",
    "TRAIN_PATH = \"data\\\\train.csv\"\n",
    "DEV_PATH   = \"data\\\\dev.csv\"\n",
    "TEST_PATH  = \"data\\\\test.csv\" \n",
    "\n",
    "BEST_MODEL_PATH = \"data\\\\taskB\\\\ED_B_Model.pt\"\n",
    "OUTPUT_PATH = \"data\\\\predictions.csv\"\n",
    "\n",
    "AUGMENTED_COPY_CHANCE = 0.15\n",
    "EPOCH_OPTIONS = [2, 3, 4]\n",
    "BATCH_OPTIONS  = [4, 8, 16]\n",
    "USE_FOCAL_OPTIONS = [False, True]\n",
    "\n",
    "SEARCH_SPACE = {\n",
    "    \"learning_rate\":   hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(5e-4)),\n",
    "    \"epochs\":          hp.choice(\"epochs\", EPOCH_OPTIONS),\n",
    "    \"batch_size\":      hp.choice(\"batch_size\", BATCH_OPTIONS),\n",
    "    \"use_focal_loss\":  hp.choice(\"use_focal_loss\", USE_FOCAL_OPTIONS),\n",
    "    \"gamma\":           hp.quniform(\"gamma\", 1.0, 5.0, 0.5),\n",
    "    \"label_smoothing\": hp.uniform(\"label_smoothing\", 0.0, 0.2)\n",
    "}\n",
    "\n",
    "MAX_EVALS = 10  \n",
    "EVAL_BATCH_SIZE = 8\n",
    "BEST_MODEL_METRIC = \"f1\"\n",
    "\n",
    "GLOVE_PATH = \"data\\\\glove.6B.300d.txt\"  \n",
    "EMBED_DIM = 300         \n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3           \n",
    "USE_ATTENTION = True  \n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if TESTING_FLAG:\n",
    "    print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:45:03.006576Z",
     "iopub.status.busy": "2025-04-01T17:45:03.005758Z",
     "iopub.status.idle": "2025-04-01T18:37:20.034394Z",
     "shell.execute_reply": "2025-04-01T18:37:20.033499Z",
     "shell.execute_reply.started": "2025-04-01T17:45:03.006551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded NLTK data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Backe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Backe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 1. Fetch NLTK data\n",
    "# ----------------------------------------------------------------------------\n",
    "if DOWNLOAD_FLAG:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "    #nltk.data.path.append(NLTK_DATA_DIR)\n",
    "    #nltk.download(\"wordnet\", download_dir=NLTK_DATA_DIR)\n",
    "    #nltk.download(\"omw-1.4\", download_dir=NLTK_DATA_DIR)\n",
    "    #!unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora/\n",
    "    #!unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora/\n",
    "    print(\"Downloaded NLTK data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:20.035832Z",
     "iopub.status.busy": "2025-04-01T18:37:20.035498Z",
     "iopub.status.idle": "2025-04-01T18:37:20.336769Z",
     "shell.execute_reply": "2025-04-01T18:37:20.335907Z",
     "shell.execute_reply.started": "2025-04-01T18:37:20.035806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 21508\n",
      "Dev samples: 5926\n",
      "Test samples: 4688\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 2. Load Data\n",
    "# ----------------------------------------------------------------------------\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df   = pd.read_csv(DEV_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "train_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "dev_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "test_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "\n",
    "if TESTING_FLAG:\n",
    "    print(\"Train samples:\", len(train_df))\n",
    "    print(\"Dev samples:\", len(dev_df))\n",
    "    print(\"Test samples:\", len(test_df))\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "dev_df[\"label\"]   = dev_df[\"label\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:20.339215Z",
     "iopub.status.busy": "2025-04-01T18:37:20.338987Z",
     "iopub.status.idle": "2025-04-01T18:37:23.600658Z",
     "shell.execute_reply": "2025-04-01T18:37:23.599880Z",
     "shell.execute_reply.started": "2025-04-01T18:37:20.339194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: 21508 => After augmentation: 24819\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 3. Data Augmentation (Synonym Replacement)\n",
    "# ----------------------------------------------------------------------------\n",
    "random.seed(42)\n",
    "\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "    indices_to_replace = random.sample(range(len(words)), k=min(n, len(words)))\n",
    "    new_words = words[:]\n",
    "    for i in indices_to_replace:\n",
    "        word = words[i]\n",
    "        syns = wordnet.synsets(word)\n",
    "        if not syns:\n",
    "            continue\n",
    "        lemmas = syns[0].lemma_names()\n",
    "        lemmas = [l for l in lemmas if l.lower() != word.lower()]\n",
    "        if len(lemmas) == 0:\n",
    "            continue\n",
    "        new_words[i] = random.choice(lemmas)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def augment_dataframe(df):\n",
    "    augmented_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        augmented_rows.append(row.to_dict())\n",
    "        if random.random() < AUGMENTED_COPY_CHANCE:\n",
    "            new_row = row.copy()\n",
    "            if random.random() < 0.5:\n",
    "                new_row[\"claim\"] = synonym_replacement(row[\"claim\"], n=1)\n",
    "            else:\n",
    "                new_row[\"evidence\"] = synonym_replacement(row[\"evidence\"], n=1)\n",
    "            augmented_rows.append(new_row.to_dict())\n",
    "    return pd.DataFrame(augmented_rows)\n",
    "\n",
    "augmented_train_df = augment_dataframe(train_df)\n",
    "if TESTING_FLAG:\n",
    "    print(\"Original train size:\", len(train_df),\n",
    "          \"=> After augmentation:\", len(augmented_train_df))\n",
    "\n",
    "train_df = augmented_train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:23.602490Z",
     "iopub.status.busy": "2025-04-01T18:37:23.602190Z",
     "iopub.status.idle": "2025-04-01T18:37:23.723215Z",
     "shell.execute_reply": "2025-04-01T18:37:23.722511Z",
     "shell.execute_reply.started": "2025-04-01T18:37:23.602466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['claim', 'evidence', 'label'],\n",
      "        num_rows: 24819\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['claim', 'evidence', 'label'],\n",
      "        num_rows: 5926\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['claim', 'evidence'],\n",
      "        num_rows: 4688\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 4. Create Hugging Face Datasets\n",
    "# ----------------------------------------------------------------------------\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset   = Dataset.from_pandas(dev_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"dev\":   dev_dataset,\n",
    "    \"test\":  test_dataset\n",
    "})\n",
    "\n",
    "if TESTING_FLAG:\n",
    "    print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:23.724235Z",
     "iopub.status.busy": "2025-04-01T18:37:23.724033Z",
     "iopub.status.idle": "2025-04-01T18:37:33.660871Z",
     "shell.execute_reply": "2025-04-01T18:37:33.659994Z",
     "shell.execute_reply.started": "2025-04-01T18:37:23.724217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7dce947c8f480692ef933569478812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833ada05e850408e9e98d1cd04b92aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d85831a3514ecdac6a2e55605beb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 5. Tokenization\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "TOKENIZER_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"claim\"],\n",
    "        examples[\"evidence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "encoded_dataset = dataset_dict.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:33.662050Z",
     "iopub.status.busy": "2025-04-01T18:37:33.661785Z",
     "iopub.status.idle": "2025-04-01T18:37:33.674074Z",
     "shell.execute_reply": "2025-04-01T18:37:33.673443Z",
     "shell.execute_reply.started": "2025-04-01T18:37:33.662016Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 5.1. Format for PyTorch\n",
    "# ----------------------------------------------------------------------------\n",
    "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].rename_column(\"label\", \"labels\")\n",
    "encoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].rename_column(\"label\", \"labels\")\n",
    "\n",
    "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].remove_columns([\"claim\", \"evidence\"])\n",
    "encoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].remove_columns([\"claim\", \"evidence\"])\n",
    "encoded_dataset[\"test\"]  = encoded_dataset[\"test\"].remove_columns([\"claim\", \"evidence\"])\n",
    "\n",
    "encoded_dataset[\"train\"].set_format(\"torch\")\n",
    "encoded_dataset[\"dev\"].set_format(\"torch\")\n",
    "encoded_dataset[\"test\"].set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:33.675178Z",
     "iopub.status.busy": "2025-04-01T18:37:33.674972Z",
     "iopub.status.idle": "2025-04-01T18:37:34.016712Z",
     "shell.execute_reply": "2025-04-01T18:37:34.015818Z",
     "shell.execute_reply.started": "2025-04-01T18:37:33.675160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe from data\\glove.6B.300d.txt...\n",
      "Initialized embedding_matrix with 26695 GloVe tokens matched out of 30522\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 6. Load GloVe embeddings & Build an Embedding Matrix\n",
    "# ----------------------------------------------------------------------------\n",
    "def load_glove_embeddings(glove_file, vocab, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load GloVe 300d vectors and align them with the given vocab.\n",
    "    vocab: a dict {token_string: token_index}\n",
    "    Returns: a numpy array [vocab_size, embedding_dim]\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.random.normal(\n",
    "        scale=0.1, \n",
    "        size=(len(vocab), embedding_dim)\n",
    "    ).astype(np.float32)\n",
    "    found = 0\n",
    "\n",
    "    if not os.path.isfile(glove_file):\n",
    "        print(f\"GloVe file not found at {glove_file}, using random init.\")\n",
    "        return embedding_matrix, found\n",
    "\n",
    "    print(f\"Loading GloVe from {glove_file}...\")\n",
    "    glove_dict = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            if len(values) != embedding_dim + 1:\n",
    "                continue\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove_dict[word] = coefs\n",
    "\n",
    "    # For each token in the BERT-based vocab, see if it matches a GloVe word\n",
    "    for token, idx in vocab.items():\n",
    "        normalized = token.replace(\"##\", \"\").lower()\n",
    "        if normalized in glove_dict:\n",
    "            embedding_matrix[idx] = glove_dict[normalized]\n",
    "            found += 1\n",
    "\n",
    "    print(f\"Initialized embedding_matrix with {found} GloVe tokens matched out of {len(vocab)}\")\n",
    "    return embedding_matrix, found\n",
    "\n",
    "vocab_dict = tokenizer.get_vocab()  # {token_str: token_id}\n",
    "embedding_matrix_np, glove_found = load_glove_embeddings(GLOVE_PATH, vocab_dict, EMBED_DIM)\n",
    "embedding_matrix_tensor = torch.tensor(embedding_matrix_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:34.017986Z",
     "iopub.status.busy": "2025-04-01T18:37:34.017647Z",
     "iopub.status.idle": "2025-04-01T18:37:34.030585Z",
     "shell.execute_reply": "2025-04-01T18:37:34.029921Z",
     "shell.execute_reply.started": "2025-04-01T18:37:34.017954Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 7. Custom BiLSTM with optional attention\n",
    "# ----------------------------------------------------------------------------\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple additive attention: \n",
    "    score = tanh(W1*H + W2*h_context), \n",
    "    then softmax over time steps, \n",
    "    output = sum of weighted hidden states\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.W = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
    "        self.v = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        lstm_outputs: (B, L, 2*hidden_dim)\n",
    "        mask: (B, L) if needed (1 for real tokens, 0 for pad)\n",
    "        Returns: (B, 2*hidden_dim) - the weighted sum\n",
    "        \"\"\"\n",
    "        # Score calculation\n",
    "        score = torch.tanh(self.W(lstm_outputs))  # (B, L, 2H)\n",
    "        score = self.v(score).squeeze(-1)         # (B, L)\n",
    "        \n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_weights = F.softmax(score, dim=-1)   # (B, L)\n",
    "\n",
    "        # Weighted sum\n",
    "        attn_weights = attn_weights.unsqueeze(1)  # (B, 1, L)\n",
    "        context = torch.bmm(attn_weights, lstm_outputs)  # (B, 1, 2H)\n",
    "        context = context.squeeze(1)              # (B, 2H)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class CustomBiLSTMModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embed_dim=300, \n",
    "                 hidden_dim=256, \n",
    "                 num_labels=2, \n",
    "                 num_layers=2,\n",
    "                 dropout=0.3,\n",
    "                 use_attention=True,\n",
    "                 use_focal_loss=False, \n",
    "                 gamma=2.0, \n",
    "                 label_smoothing=0.0,\n",
    "                 embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            with torch.no_grad():\n",
    "                self.embedding.weight.copy_(embedding_matrix)\n",
    "\n",
    "        # BiLSTM with multiple layers & dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Optional attention\n",
    "        if self.use_attention:\n",
    "            self.attn = SimpleAttention(self.hidden_dim)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(2 * self.hidden_dim, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        # Embeddings\n",
    "        embeds = self.embedding(input_ids)\n",
    "        # zero out padding\n",
    "        if attention_mask is not None:\n",
    "            expand_mask = attention_mask.unsqueeze(-1).float()\n",
    "            embeds = embeds * expand_mask\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_outputs, (h, c) = self.lstm(embeds)\n",
    "        # shape of lstm_outputs: (B, L, 2H)\n",
    "\n",
    "        if self.use_attention:\n",
    "            # Weighted sum of outputs\n",
    "            context = self.attn(lstm_outputs, mask=attention_mask)\n",
    "        else:\n",
    "            # h shape: (num_layers*2, B, H)\n",
    "            h_forward = h[-2]  # last layer's forward state\n",
    "            h_backward = h[-1] # last layer's backward state\n",
    "            context = torch.cat((h_forward, h_backward), dim=-1)  # (B, 2H)\n",
    "\n",
    "        logits = self.classifier(context)\n",
    "\n",
    "        # Loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.use_focal_loss:\n",
    "                loss = self.focal_loss(logits, labels, self.gamma)\n",
    "            else:\n",
    "                loss = self.label_smoothing_loss(logits, labels, self.label_smoothing)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def focal_loss(self, logits, targets, gamma=2.0):\n",
    "        ce = nn.CrossEntropyLoss(reduction='none')(logits, targets)\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = (1 - pt)**gamma * ce\n",
    "        return focal.mean()\n",
    "\n",
    "    def label_smoothing_loss(self, logits, targets, smoothing=0.0):\n",
    "        if smoothing == 0.0:\n",
    "            return nn.CrossEntropyLoss()(logits, targets)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        n_class = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(smoothing / (n_class - 1))\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:34.031558Z",
     "iopub.status.busy": "2025-04-01T18:37:34.031361Z",
     "iopub.status.idle": "2025-04-01T21:12:52.343743Z",
     "shell.execute_reply": "2025-04-01T21:12:52.343084Z",
     "shell.execute_reply.started": "2025-04-01T18:37:34.031541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 10:00, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.038686</td>\n",
       "      <td>0.803801</td>\n",
       "      <td>0.805434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.037117</td>\n",
       "      <td>0.814423</td>\n",
       "      <td>0.814546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.037885</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.821127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.041007</td>\n",
       "      <td>0.820118</td>\n",
       "      <td>0.823152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 4, 'gamma': 3.5, 'label_smoothing': 0.08212135447736467, 'learning_rate': 0.000128486801463992, 'use_focal_loss': True} => F1=0.8201\n",
      " 10%|█         | 1/10 [10:14<1:32:08, 614.23s/trial, best loss: -0.8201183725656067]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 06:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.755569</td>\n",
       "      <td>0.772190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.082674</td>\n",
       "      <td>0.777375</td>\n",
       "      <td>0.784171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>0.085564</td>\n",
       "      <td>0.780069</td>\n",
       "      <td>0.788053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 2.5, 'label_smoothing': 0.03241339306919242, 'learning_rate': 3.203722949774699e-05, 'use_focal_loss': True} => F1=0.7801\n",
      " 20%|██        | 2/10 [17:17<1:06:56, 502.03s/trial, best loss: -0.8201183725656067]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4656' max='4656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4656/4656 04:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.062078</td>\n",
       "      <td>0.739292</td>\n",
       "      <td>0.764090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.059637</td>\n",
       "      <td>0.761492</td>\n",
       "      <td>0.772865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.059280</td>\n",
       "      <td>0.763091</td>\n",
       "      <td>0.775059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 3, 'gamma': 3.0, 'label_smoothing': 0.13536990039247074, 'learning_rate': 2.9094017853862346e-05, 'use_focal_loss': True} => F1=0.7631\n",
      " 30%|███       | 3/10 [21:32<45:22, 388.92s/trial, best loss: -0.8201183725656067]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12410' max='12410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12410/12410 10:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.487100</td>\n",
       "      <td>0.482049</td>\n",
       "      <td>0.797483</td>\n",
       "      <td>0.801890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.470738</td>\n",
       "      <td>0.813139</td>\n",
       "      <td>0.818934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 2, 'gamma': 4.5, 'label_smoothing': 0.029043148896219263, 'learning_rate': 0.00029653493053302253, 'use_focal_loss': False} => F1=0.8131\n",
      " 40%|████      | 4/10 [31:51<47:59, 479.88s/trial, best loss: -0.8201183725656067]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 07:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.209646</td>\n",
       "      <td>0.805460</td>\n",
       "      <td>0.803915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.200790</td>\n",
       "      <td>0.814747</td>\n",
       "      <td>0.818259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.254356</td>\n",
       "      <td>0.811642</td>\n",
       "      <td>0.813871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 1.0, 'label_smoothing': 0.07636777850181752, 'learning_rate': 0.00039332294219049336, 'use_focal_loss': True} => F1=0.8147\n",
      " 50%|█████     | 5/10 [39:07<38:39, 463.96s/trial, best loss: -0.8201183725656067]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 05:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.547700</td>\n",
       "      <td>0.522890</td>\n",
       "      <td>0.789401</td>\n",
       "      <td>0.794128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.497600</td>\n",
       "      <td>0.506956</td>\n",
       "      <td>0.804670</td>\n",
       "      <td>0.812352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>0.505083</td>\n",
       "      <td>0.809640</td>\n",
       "      <td>0.816909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.505331</td>\n",
       "      <td>0.814034</td>\n",
       "      <td>0.818765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 2.5, 'label_smoothing': 0.08027873709560314, 'learning_rate': 0.00010711311564179699, 'use_focal_loss': False} => F1=0.8140\n",
      " 60%|██████    | 6/10 [44:55<28:19, 424.76s/trial, best loss: -0.8201183725656067]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 03:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.580100</td>\n",
       "      <td>0.565388</td>\n",
       "      <td>0.758739</td>\n",
       "      <td>0.775397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.546500</td>\n",
       "      <td>0.558494</td>\n",
       "      <td>0.772179</td>\n",
       "      <td>0.785521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 2.5, 'label_smoothing': 0.11296662853916198, 'learning_rate': 6.314723415788638e-05, 'use_focal_loss': False} => F1=0.7722\n",
      " 70%|███████   | 7/10 [49:06<18:23, 367.88s/trial, best loss: -0.8201183725656067]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 10:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>0.156180</td>\n",
       "      <td>0.789546</td>\n",
       "      <td>0.792103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>0.148525</td>\n",
       "      <td>0.802482</td>\n",
       "      <td>0.806446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.156273</td>\n",
       "      <td>0.806461</td>\n",
       "      <td>0.812184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 1.5, 'label_smoothing': 0.15982432891442486, 'learning_rate': 8.199005032718306e-05, 'use_focal_loss': True} => F1=0.8065\n",
      " 80%|████████  | 8/10 [59:39<15:04, 452.40s/trial, best loss: -0.8201183725656067]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 04:45, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.038841</td>\n",
       "      <td>0.804051</td>\n",
       "      <td>0.803577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.038090</td>\n",
       "      <td>0.807629</td>\n",
       "      <td>0.814040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 3.5, 'label_smoothing': 0.021942683480999727, 'learning_rate': 0.00021051981518692482, 'use_focal_loss': True} => F1=0.8076\n",
      " 90%|█████████ | 9/10 [1:04:38<06:44, 404.36s/trial, best loss: -0.8201183725656067]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12410' max='12410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12410/12410 09:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.565800</td>\n",
       "      <td>0.558422</td>\n",
       "      <td>0.774955</td>\n",
       "      <td>0.789234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.535600</td>\n",
       "      <td>0.553152</td>\n",
       "      <td>0.777958</td>\n",
       "      <td>0.793621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 2, 'gamma': 1.0, 'label_smoothing': 0.09319025126344603, 'learning_rate': 4.2072403216879116e-05, 'use_focal_loss': False} => F1=0.7780\n",
      "100%|██████████| 10/10 [1:14:39<00:00, 447.94s/trial, best loss: -0.8201183725656067]\n",
      "\n",
      "Hyperopt best param indices: {'batch_size': np.int64(1), 'epochs': np.int64(2), 'gamma': np.float64(3.5), 'label_smoothing': np.float64(0.08212135447736467), 'learning_rate': np.float64(0.000128486801463992), 'use_focal_loss': np.int64(1)}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 8. Build Trainer & Hyperopt\n",
    "# ----------------------------------------------------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "def make_trainer(model, train_ds, dev_ds, space):\n",
    "    learning_rate = space[\"learning_rate\"]\n",
    "    epochs        = int(space[\"epochs\"])\n",
    "    batch_size    = int(space[\"batch_size\"])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./enhanced-bilstm-ed-checkpoints\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=BEST_MODEL_METRIC,\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        logging_steps=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=dev_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def objective(space):\n",
    "    use_focal_loss = space[\"use_focal_loss\"]\n",
    "    gamma = space[\"gamma\"]\n",
    "    label_smoothing = space[\"label_smoothing\"]\n",
    "\n",
    "    # Build model\n",
    "    model = CustomBiLSTMModel(\n",
    "        vocab_size=len(tokenizer.get_vocab()),\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_labels=2,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        use_attention=USE_ATTENTION,\n",
    "        use_focal_loss=use_focal_loss,\n",
    "        gamma=gamma,\n",
    "        label_smoothing=label_smoothing,\n",
    "        embedding_matrix=embedding_matrix_tensor\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    trainer = make_trainer(model, encoded_dataset[\"train\"], encoded_dataset[\"dev\"], space)\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate(encoded_dataset[\"dev\"])\n",
    "    f1 = metrics[\"eval_f1\"]\n",
    "\n",
    "    if TESTING_FLAG:\n",
    "        print(f\"[Hyperopt] params={space} => F1={f1:.4f}\")\n",
    "    return {\"loss\": -f1, \"status\": STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=SEARCH_SPACE,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=MAX_EVALS,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "if TESTING_FLAG:\n",
    "    print(\"\\nHyperopt best param indices:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-01T21:12:52.344772Z",
     "iopub.status.busy": "2025-04-01T21:12:52.344498Z",
     "iopub.status.idle": "2025-04-01T21:12:52.349967Z",
     "shell.execute_reply": "2025-04-01T21:12:52.349331Z",
     "shell.execute_reply.started": "2025-04-01T21:12:52.344747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreted best hyperparams:\n",
      " {'learning_rate': np.float64(0.000128486801463992), 'epochs': 4, 'batch_size': 8, 'use_focal_loss': True, 'gamma': np.float64(3.5), 'label_smoothing': np.float64(0.08212135447736467)}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 8.1 Interpret Best Hyperparams\n",
    "# ----------------------------------------------------------------------------\n",
    "EPOCH_OPTIONS = [2, 3, 4]\n",
    "BATCH_OPTIONS = [4, 8, 16]\n",
    "USE_FOCAL_OPTIONS = [False, True]\n",
    "\n",
    "final_params = {\n",
    "    \"learning_rate\":    best[\"learning_rate\"],\n",
    "    \"epochs\":           EPOCH_OPTIONS[ best[\"epochs\"] ],\n",
    "    \"batch_size\":       BATCH_OPTIONS[ best[\"batch_size\"] ],\n",
    "    \"use_focal_loss\":   USE_FOCAL_OPTIONS[ best[\"use_focal_loss\"] ],\n",
    "    \"gamma\":            best[\"gamma\"],\n",
    "    \"label_smoothing\":  best[\"label_smoothing\"]\n",
    "}\n",
    "\n",
    "if TESTING_FLAG:\n",
    "    print(\"Interpreted best hyperparams:\\n\", final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:12:52.351043Z",
     "iopub.status.busy": "2025-04-01T21:12:52.350765Z",
     "iopub.status.idle": "2025-04-01T21:21:27.556406Z",
     "shell.execute_reply": "2025-04-01T21:21:27.555691Z",
     "shell.execute_reply.started": "2025-04-01T21:12:52.351016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 09:16, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.039102</td>\n",
       "      <td>0.800073</td>\n",
       "      <td>0.799190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.037912</td>\n",
       "      <td>0.806167</td>\n",
       "      <td>0.805434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.039154</td>\n",
       "      <td>0.810510</td>\n",
       "      <td>0.814377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.044327</td>\n",
       "      <td>0.814528</td>\n",
       "      <td>0.817415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dev Results: {'eval_loss': 0.04432706534862518, 'eval_f1': 0.8145284908612143, 'eval_accuracy': 0.817414782315221, 'eval_runtime': 13.3147, 'eval_samples_per_second': 445.073, 'eval_steps_per_second': 55.653, 'epoch': 4.0}\n",
      "\n",
      "Detailed Classification Report (Dev):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8608    0.8917    0.8760      4286\n",
      "           1     0.6878    0.6232    0.6539      1640\n",
      "\n",
      "    accuracy                         0.8174      5926\n",
      "   macro avg     0.7743    0.7575    0.7649      5926\n",
      "weighted avg     0.8129    0.8174    0.8145      5926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 9. Train Final Model\n",
    "# ----------------------------------------------------------------------------\n",
    "best_model = CustomBiLSTMModel(\n",
    "    vocab_size=len(tokenizer.get_vocab()),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    use_attention=USE_ATTENTION,\n",
    "    num_labels=2,\n",
    "    use_focal_loss=final_params[\"use_focal_loss\"],\n",
    "    gamma=final_params[\"gamma\"],\n",
    "    label_smoothing=final_params[\"label_smoothing\"],\n",
    "    embedding_matrix=embedding_matrix_tensor\n",
    ")\n",
    "best_model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final-enhanced-bilstm-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=final_params[\"learning_rate\"],\n",
    "    num_train_epochs=final_params[\"epochs\"],\n",
    "    per_device_train_batch_size=final_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=BEST_MODEL_METRIC,\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"dev\"],\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results_dev = trainer.evaluate(encoded_dataset[\"dev\"])\n",
    "if TESTING_FLAG:\n",
    "    print(\"Final Dev Results:\", results_dev)\n",
    "\n",
    "preds_output = trainer.predict(encoded_dataset[\"dev\"])\n",
    "dev_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "dev_labels = preds_output.label_ids\n",
    "if TESTING_FLAG:\n",
    "    print(\"\\nDetailed Classification Report (Dev):\")\n",
    "    print(classification_report(dev_labels, dev_preds, digits=4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model + hyperparams + embedding matrix saved to: data\\taskB\\ED_B_Model.pt\n"
     ]
    }
   ],
   "source": [
    "#Save the best model\n",
    "\n",
    "#Prepare the dictionary to save\n",
    "save_dict = {\n",
    "    \"model_state_dict\": trainer.model.state_dict(),   # (A) model weights\n",
    "    \"hyperparams\": {                                  # (B) essential model config\n",
    "        \"vocab_size\": len(tokenizer.get_vocab()),\n",
    "        \"embed_dim\": EMBED_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_labels\": 2,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"use_attention\": USE_ATTENTION,\n",
    "        \"use_focal_loss\": final_params[\"use_focal_loss\"],\n",
    "        \"gamma\": final_params[\"gamma\"],\n",
    "        \"label_smoothing\": final_params[\"label_smoothing\"]\n",
    "    },\n",
    "    \"embedding_matrix\": embedding_matrix_tensor.cpu()  # (C) embedding weights\n",
    "}\n",
    "\n",
    "#Save the entire dictionary to BEST_MODEL_PATH\n",
    "torch.save(save_dict, BEST_MODEL_PATH)\n",
    "print(f\"Best model + hyperparams + embedding matrix saved to: {BEST_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:21:27.558768Z",
     "iopub.status.busy": "2025-04-01T21:21:27.558526Z",
     "iopub.status.idle": "2025-04-01T21:21:51.176503Z",
     "shell.execute_reply": "2025-04-01T21:21:51.175676Z",
     "shell.execute_reply.started": "2025-04-01T21:21:27.558748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1) Remove unwanted columns so that only input_ids, attention_mask (and labels if needed) remain\n",
    "encoded_dataset[\"test\"] = encoded_dataset[\"test\"].remove_columns([\"claim\", \"evidence\"])\n",
    "\n",
    "# 2) Convert to torch format\n",
    "encoded_dataset[\"test\"].set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\"]  # Add \"labels\" if your model needs them\n",
    ")\n",
    "\n",
    "# 3) Build the DataLoader\n",
    "test_loader = DataLoader(encoded_dataset[\"test\"], batch_size=8)\n",
    "\n",
    "# 4) Evaluate\n",
    "trainer.model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # batch[\"input_ids\"] and batch[\"attention_mask\"] are now Tensors\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = trainer.model(**batch)\n",
    "        logits = outputs[\"logits\"]\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "# 5) Save predictions\n",
    "test_pred_df = pd.DataFrame({\"prediction\": all_preds})\n",
    "test_pred_df.to_csv(\"data\\\\predictions.csv\", index=False)\n",
    "print(\"Saved predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev dataset columns before any changes: ['label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Dev dataset columns after set_format: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Saved dev predictions to dev_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 10.x. Inference on Dev Set (Using Freshly Trained Model)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) Check current columns for dev\n",
    "print(\"Dev dataset columns before any changes:\", encoded_dataset[\"dev\"].column_names)\n",
    "# e.g. might see: ['label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "# 2) If your model expects a 'labels' argument (typical in HF Trainer),\n",
    "#    rename 'label' -> 'labels' if not done already.\n",
    "if \"label\" in encoded_dataset[\"dev\"].column_names and \"labels\" not in encoded_dataset[\"dev\"].column_names:\n",
    "    encoded_dataset[\"dev\"] = encoded_dataset[\"dev\"].rename_column(\"label\", \"labels\")\n",
    "\n",
    "# 3) Now set the format to return PyTorch tensors. Include only the columns your model needs.\n",
    "#    Usually that's 'input_ids', 'attention_mask', and optionally 'labels' if you want to compare or compute a loss.\n",
    "desired_columns = []\n",
    "if \"input_ids\" in encoded_dataset[\"dev\"].column_names:\n",
    "    desired_columns.append(\"input_ids\")\n",
    "if \"attention_mask\" in encoded_dataset[\"dev\"].column_names:\n",
    "    desired_columns.append(\"attention_mask\")\n",
    "if \"labels\" in encoded_dataset[\"dev\"].column_names:\n",
    "    desired_columns.append(\"labels\")\n",
    "if \"token_type_ids\" in encoded_dataset[\"dev\"].column_names:\n",
    "    # If your model does not need token_type_ids, you can omit it or remove it\n",
    "    desired_columns.append(\"token_type_ids\")\n",
    "\n",
    "encoded_dataset[\"dev\"].set_format(\n",
    "    type=\"torch\",\n",
    "    columns=desired_columns\n",
    ")\n",
    "\n",
    "print(\"Dev dataset columns after set_format:\", encoded_dataset[\"dev\"].column_names)\n",
    "# Should look like the columns you included\n",
    "\n",
    "# 4) Create the DataLoader\n",
    "dev_loader = DataLoader(encoded_dataset[\"dev\"], batch_size=8)\n",
    "\n",
    "# 5) Perform inference with the freshly trained model\n",
    "trainer.model.eval()\n",
    "all_dev_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "        # Move only tensors to device\n",
    "        for key, value in batch.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                batch[key] = value.to(device)\n",
    "\n",
    "        outputs = trainer.model(**batch)        # forward pass\n",
    "        logits = outputs[\"logits\"]             # shape: (B, num_labels)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_dev_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "# 6) Save predictions to a CSV\n",
    "dev_pred_df = pd.DataFrame({\"prediction\": all_dev_preds})\n",
    "dev_pred_df.to_csv(\"dev_predictions.csv\", index=False, header=True)\n",
    "print(\"Saved dev predictions to dev_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6952367,
     "sourceId": 11144868,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
