{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:12.810376Z",
     "iopub.status.busy": "2025-04-02T15:33:12.810000Z",
     "iopub.status.idle": "2025-04-02T15:33:28.494143Z",
     "shell.execute_reply": "2025-04-02T15:33:28.493379Z",
     "shell.execute_reply.started": "2025-04-02T15:33:12.810339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Backe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Backe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Download necessary NLTK data (WordNet for synonyms)\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "#  1.1 Check GPU availability\n",
    "# ----------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:28.495800Z",
     "iopub.status.busy": "2025-04-02T15:33:28.495135Z",
     "iopub.status.idle": "2025-04-02T15:33:28.498982Z",
     "shell.execute_reply": "2025-04-02T15:33:28.498179Z",
     "shell.execute_reply.started": "2025-04-02T15:33:28.495772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora/\n",
    "#!unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:28.501064Z",
     "iopub.status.busy": "2025-04-02T15:33:28.500827Z",
     "iopub.status.idle": "2025-04-02T15:33:28.785416Z",
     "shell.execute_reply": "2025-04-02T15:33:28.784539Z",
     "shell.execute_reply.started": "2025-04-02T15:33:28.501033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 21508\n",
      "Dev samples: 5926\n",
      "Test samples: 4688\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "claim",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "evidence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "718303ca-ec42-487c-af4f-ce4b40c781ad",
       "rows": [
        [
         "0",
         "We should introduce school vouchers",
         "Among the many educational reform efforts, such as charter schools, school vouchers, magnet schools, and alternative schools, the full-service community school model is one of many educational reform efforts that are intended to increase student achievement, but the full-service community school model specifically focuses on the development of the community as a whole.",
         "0"
        ],
        [
         "1",
         "We should legalize insider trading",
         "The U.S. Securities and Exchange Commission was established the following year, which helped combat insider trading and reducing transaction risk.  ",
         "0"
        ],
        [
         "2",
         "We should subsidize investigative journalism",
         "The film won an Emmy Award (1980), George Polk Award[REF] for investigative journalism on TV, Hugh M. Hefner First Amendment Award,[REF] and Best Documentary at the Mannheim Film Festival [REF].",
         "0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We should introduce school vouchers</td>\n",
       "      <td>Among the many educational reform efforts, suc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We should legalize insider trading</td>\n",
       "      <td>The U.S. Securities and Exchange Commission wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We should subsidize investigative journalism</td>\n",
       "      <td>The film won an Emmy Award (1980), George Polk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          claim  \\\n",
       "0           We should introduce school vouchers   \n",
       "1            We should legalize insider trading   \n",
       "2  We should subsidize investigative journalism   \n",
       "\n",
       "                                            evidence  label  \n",
       "0  Among the many educational reform efforts, suc...      0  \n",
       "1  The U.S. Securities and Exchange Commission wa...      0  \n",
       "2  The film won an Emmy Award (1980), George Polk...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  2. LOAD YOUR DATA\n",
    "#     Adjust these file paths to your environment. \n",
    "#     The CSVs must contain columns:\n",
    "#       train.csv: claim, evidence, label\n",
    "#       dev.csv:   claim, evidence, label\n",
    "#       test.csv:  claim, evidence, (no label)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "TRAIN_PATH = \"data\\\\train.csv\"\n",
    "DEV_PATH   = \"data\\\\dev.csv\"\n",
    "TEST_PATH  = \"data\\\\test.csv\"\n",
    "\n",
    "BEST_MODEL_PATH = \"data\\\\taskC\\\\best_deberta_model.pt\"\n",
    "OUTPUT_PATH = \"data\\\\taskC\\\\predictions.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df   = pd.read_csv(DEV_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "train_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "dev_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "test_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "\n",
    "print(\"Train samples:\", len(train_df))\n",
    "print(\"Dev samples:\", len(dev_df))\n",
    "print(\"Test samples:\", len(test_df))\n",
    "\n",
    "# If labels are strings, map them to integer {0,1} or {0,1,2,...}.\n",
    "# For ED, assume 2 classes: 0 = not evidence, 1 = relevant evidence\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "dev_df[\"label\"]   = dev_df[\"label\"].astype(int)\n",
    "\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:28.787430Z",
     "iopub.status.busy": "2025-04-02T15:33:28.787111Z",
     "iopub.status.idle": "2025-04-02T15:33:32.180188Z",
     "shell.execute_reply": "2025-04-02T15:33:32.179461Z",
     "shell.execute_reply.started": "2025-04-02T15:33:28.787399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: 21508  => After augmentation: 24819\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  3. (OPTIONAL) DATA AUGMENTATION (Synonym Replacement)\n",
    "#      - We'll replace 1 random word in claim/evidence with a WordNet synonym\n",
    "#      - For demonstration, there's a 15% chance per example to create an\n",
    "#        augmented copy.\n",
    "# \n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    \"\"\"\n",
    "    Replace 'n' words in 'sentence' with synonyms from WordNet, if possible.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "\n",
    "    indices_to_replace = random.sample(range(len(words)), k=min(n, len(words)))\n",
    "    new_words = words[:]\n",
    "    for i in indices_to_replace:\n",
    "        word = words[i]\n",
    "        syns = wordnet.synsets(word)\n",
    "        if not syns:\n",
    "            continue\n",
    "        # For simplicity, pick from the first synset's lemmas\n",
    "        lemmas = syns[0].lemma_names()\n",
    "        # Filter out lemmas that are the same as the original\n",
    "        lemmas = [l for l in lemmas if l.lower() != word.lower()]\n",
    "        if len(lemmas) == 0:\n",
    "            continue\n",
    "        new_words[i] = random.choice(lemmas)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "def augment_dataframe(df, alpha=0.15):\n",
    "    \"\"\"\n",
    "    For each row, with probability alpha, create an augmented copy.\n",
    "    Return a new DataFrame with both original and augmented samples.\n",
    "    \"\"\"\n",
    "    augmented_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Original row\n",
    "        augmented_rows.append(row.to_dict())\n",
    "        \n",
    "        if random.random() < alpha:\n",
    "            new_row = row.copy()\n",
    "            # Randomly augment claim or evidence\n",
    "            if random.random() < 0.5:\n",
    "                new_row[\"claim\"] = synonym_replacement(row[\"claim\"], n=1)\n",
    "            else:\n",
    "                new_row[\"evidence\"] = synonym_replacement(row[\"evidence\"], n=1)\n",
    "            augmented_rows.append(new_row.to_dict())\n",
    "    return pd.DataFrame(augmented_rows)\n",
    "\n",
    "# Let's do a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# AUGMENT the training set (remove if undesired)\n",
    "augmented_train_df = augment_dataframe(train_df, alpha=0.15)\n",
    "print(\"Original train size:\", len(train_df), \n",
    "      \" => After augmentation:\", len(augmented_train_df))\n",
    "\n",
    "train_df = augmented_train_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:32.181249Z",
     "iopub.status.busy": "2025-04-02T15:33:32.180990Z",
     "iopub.status.idle": "2025-04-02T15:33:32.302244Z",
     "shell.execute_reply": "2025-04-02T15:33:32.301381Z",
     "shell.execute_reply.started": "2025-04-02T15:33:32.181207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'evidence', 'label'],\n",
       "        num_rows: 24819\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['claim', 'evidence', 'label'],\n",
       "        num_rows: 5926\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'evidence'],\n",
       "        num_rows: 4688\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  4. CREATE HUGGING FACE DATASETS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset   = Dataset.from_pandas(dev_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"dev\":   dev_dataset,\n",
    "    \"test\":  test_dataset\n",
    "})\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:32.303394Z",
     "iopub.status.busy": "2025-04-02T15:33:32.303159Z",
     "iopub.status.idle": "2025-04-02T15:33:43.991558Z",
     "shell.execute_reply": "2025-04-02T15:33:43.990633Z",
     "shell.execute_reply.started": "2025-04-02T15:33:32.303373Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f13fcdb6a804e13be6b0f6e191e60e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d862627fb6cf4c7aa6fa7074a49bba3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3b7d433d4146f2baacc40e91491416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  5. TOKENIZATION\n",
    "#     We use a powerful model: DeBERTa v3 (microsoft/deberta-v3-base)\n",
    "#     which is known to outperform standard BERT on many tasks.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"claim\"],\n",
    "        examples[\"evidence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "encoded_dataset = dataset_dict.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:43.992612Z",
     "iopub.status.busy": "2025-04-02T15:33:43.992398Z",
     "iopub.status.idle": "2025-04-02T15:33:44.005917Z",
     "shell.execute_reply": "2025-04-02T15:33:44.005123Z",
     "shell.execute_reply.started": "2025-04-02T15:33:43.992593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Handling the labels in the dataset\n",
    "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].rename_column(\"label\", \"labels\")\n",
    "encoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].rename_column(\"label\", \"labels\")\n",
    "\n",
    "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].remove_columns([\"claim\", \"evidence\"])\n",
    "encoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].remove_columns([\"claim\", \"evidence\"])\n",
    "\n",
    "encoded_dataset[\"test\"]  = encoded_dataset[\"test\"].remove_columns([\"claim\", \"evidence\"])\n",
    "\n",
    "encoded_dataset[\"train\"].set_format(\"torch\")\n",
    "encoded_dataset[\"dev\"].set_format(\"torch\")\n",
    "encoded_dataset[\"test\"].set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:44.007174Z",
     "iopub.status.busy": "2025-04-02T15:33:44.006911Z",
     "iopub.status.idle": "2025-04-02T15:33:44.016571Z",
     "shell.execute_reply": "2025-04-02T15:33:44.015597Z",
     "shell.execute_reply.started": "2025-04-02T15:33:44.007153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  6. CUSTOM MODEL: Focal Loss or Label Smoothing\n",
    "#     overridomg forward() to allow advanced loss functions.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDebertaModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=2, use_focal_loss=False, gamma=2.0, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        # Load the pre-trained DeBERTa classification model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        # DeBERTa forward pass (omit internal CE)\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None,\n",
    "            **kwargs\n",
    "        )\n",
    "        logits = outputs.logits  # shape: (batch_size, num_labels)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.use_focal_loss:\n",
    "                loss = self.focal_loss(logits, labels, self.gamma)\n",
    "            else:\n",
    "                loss = self.label_smoothing_loss(logits, labels, self.label_smoothing)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def focal_loss(self, logits, targets, gamma=2.0):\n",
    "        # Focal Loss\n",
    "        ce = nn.CrossEntropyLoss(reduction='none')(logits, targets)\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = (1 - pt)**gamma * ce\n",
    "        return focal.mean()\n",
    "\n",
    "    def label_smoothing_loss(self, logits, targets, smoothing=0.0):\n",
    "        if smoothing == 0.0:\n",
    "            return nn.CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "        log_probs = nn.LogSoftmax(dim=-1)(logits)\n",
    "        n_class = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(smoothing / (n_class - 1))\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:44.018971Z",
     "iopub.status.busy": "2025-04-02T15:33:44.018733Z",
     "iopub.status.idle": "2025-04-02T20:01:51.244494Z",
     "shell.execute_reply": "2025-04-02T20:01:51.243746Z",
     "shell.execute_reply.started": "2025-04-02T15:33:44.018952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 12:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>0.887010</td>\n",
       "      <td>0.884576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.015070</td>\n",
       "      <td>0.888316</td>\n",
       "      <td>0.885926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.5, 'label_smoothing': 0.12159883445297202, 'learning_rate': 1.403352626309778e-05, 'use_focal_loss': True} => F1=0.8883\n",
      "  3%|▎         | 1/30 [13:31<6:32:00, 811.05s/trial, best loss: -0.8883157289201107]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 36:41, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.338183</td>\n",
       "      <td>0.884733</td>\n",
       "      <td>0.882889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.368012</td>\n",
       "      <td>0.884334</td>\n",
       "      <td>0.881876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.500047</td>\n",
       "      <td>0.873865</td>\n",
       "      <td>0.869389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.475074</td>\n",
       "      <td>0.889748</td>\n",
       "      <td>0.887445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 4, 'gamma': 2.5, 'label_smoothing': 0.017174488963293922, 'learning_rate': 1.804114416744061e-05, 'use_focal_loss': False} => F1=0.8897\n",
      "  7%|▋         | 2/30 [50:43<12:48:42, 1647.22s/trial, best loss: -0.8897480309880822]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 36:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.590224</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.587900</td>\n",
       "      <td>0.590458</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.587300</td>\n",
       "      <td>0.590370</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>0.590262</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 4, 'gamma': 4.5, 'label_smoothing': 0.0008609566740792607, 'learning_rate': 0.0003437195506520273, 'use_focal_loss': False} => F1=0.6071\n",
      " 10%|█         | 3/30 [1:28:09<14:24:13, 1920.49s/trial, best loss: -0.8897480309880822]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 36:16, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.053659</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.053710</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.051040</td>\n",
       "      <td>0.671278</td>\n",
       "      <td>0.752109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.044210</td>\n",
       "      <td>0.750358</td>\n",
       "      <td>0.761897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 4, 'gamma': 3.5, 'label_smoothing': 0.13308782780412667, 'learning_rate': 7.109681895877819e-05, 'use_focal_loss': True} => F1=0.7504\n",
      " 13%|█▎        | 4/30 [2:04:55<14:41:10, 2033.47s/trial, best loss: -0.8897480309880822]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 12:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.633100</td>\n",
       "      <td>0.632260</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.632890</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 3.5, 'label_smoothing': 0.1131348005430801, 'learning_rate': 0.0001601445807601044, 'use_focal_loss': False} => F1=0.6071\n",
      " 17%|█▋        | 5/30 [2:17:54<10:58:39, 1580.79s/trial, best loss: -0.8897480309880822]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 35:26, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.524318</td>\n",
       "      <td>0.887981</td>\n",
       "      <td>0.886601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.534548</td>\n",
       "      <td>0.887337</td>\n",
       "      <td>0.885083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.473600</td>\n",
       "      <td>0.554314</td>\n",
       "      <td>0.880816</td>\n",
       "      <td>0.876983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.456400</td>\n",
       "      <td>0.549849</td>\n",
       "      <td>0.889528</td>\n",
       "      <td>0.887276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 4, 'gamma': 4.5, 'label_smoothing': 0.1552095854820624, 'learning_rate': 1.3105059712462589e-05, 'use_focal_loss': False} => F1=0.8895\n",
      " 20%|██        | 6/30 [2:53:51<11:50:47, 1776.99s/trial, best loss: -0.8897480309880822]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 47:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.650600</td>\n",
       "      <td>0.648645</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.647800</td>\n",
       "      <td>0.648656</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.647400</td>\n",
       "      <td>0.649094</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 2.5, 'label_smoothing': 0.1683797151233103, 'learning_rate': 0.0001528471392864525, 'use_focal_loss': False} => F1=0.6071\n",
      " 23%|██▎       | 7/30 [3:41:48<13:39:00, 2136.56s/trial, best loss: -0.8897480309880822]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 12:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>0.107524</td>\n",
       "      <td>0.877162</td>\n",
       "      <td>0.873439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.118573</td>\n",
       "      <td>0.891459</td>\n",
       "      <td>0.889639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 1.5, 'label_smoothing': 0.165068225725836, 'learning_rate': 2.084358534961521e-05, 'use_focal_loss': True} => F1=0.8915\n",
      " 27%|██▋       | 8/30 [3:54:42<10:24:20, 1702.75s/trial, best loss: -0.8914590908738702]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12410' max='12410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12410/12410 31:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.150061</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.150258</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 2, 'gamma': 2.0, 'label_smoothing': 0.1228314311873115, 'learning_rate': 0.00022976060375887023, 'use_focal_loss': True} => F1=0.6071\n",
      " 30%|███       | 9/30 [4:26:56<10:21:15, 1775.02s/trial, best loss: -0.8914590908738702]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 17:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625100</td>\n",
       "      <td>0.624345</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.620800</td>\n",
       "      <td>0.618916</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 2.5, 'label_smoothing': 0.08792093552471075, 'learning_rate': 0.00014921331904601946, 'use_focal_loss': False} => F1=0.6071\n",
      " 33%|███▎      | 10/30 [4:44:48<8:39:21, 1558.07s/trial, best loss: -0.8914590908738702]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 17:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.035875</td>\n",
       "      <td>0.887565</td>\n",
       "      <td>0.885758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.050691</td>\n",
       "      <td>0.892011</td>\n",
       "      <td>0.889639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 3.0, 'label_smoothing': 0.05736481208349751, 'learning_rate': 1.0495183700025125e-05, 'use_focal_loss': True} => F1=0.8920\n",
      " 37%|███▋      | 11/30 [5:02:36<7:25:53, 1408.08s/trial, best loss: -0.892011039832924] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 48:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.211329</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>0.211273</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.211493</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 1.5, 'label_smoothing': 0.18935308979340293, 'learning_rate': 0.00022060936202683698, 'use_focal_loss': True} => F1=0.6071\n",
      " 40%|████      | 12/30 [5:51:50<9:23:28, 1878.23s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12410' max='12410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12410/12410 33:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.613900</td>\n",
       "      <td>0.613915</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.615828</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 2, 'gamma': 5.0, 'label_smoothing': 0.05654789517538768, 'learning_rate': 8.412609132075768e-05, 'use_focal_loss': False} => F1=0.6071\n",
      " 43%|████▎     | 13/30 [6:25:46<9:05:43, 1926.07s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4656' max='4656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4656/4656 19:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.019059</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.019037</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.682661</td>\n",
       "      <td>0.733547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 3, 'gamma': 5.0, 'label_smoothing': 0.18398608845659972, 'learning_rate': 0.00012816468457413227, 'use_focal_loss': True} => F1=0.6827\n",
      " 47%|████▋     | 14/30 [6:45:51<7:35:31, 1708.22s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 26:28, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>0.027950</td>\n",
       "      <td>0.876324</td>\n",
       "      <td>0.872427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.027338</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.888964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.054854</td>\n",
       "      <td>0.884370</td>\n",
       "      <td>0.881708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.087415</td>\n",
       "      <td>0.891595</td>\n",
       "      <td>0.890145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 3.5, 'label_smoothing': 0.14481766643999597, 'learning_rate': 1.5629540193973018e-05, 'use_focal_loss': True} => F1=0.8916\n",
      " 50%|█████     | 15/30 [7:12:51<7:00:24, 1681.63s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 13:18, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.212600</td>\n",
       "      <td>0.211189</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.210500</td>\n",
       "      <td>0.211520</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 1.5, 'label_smoothing': 0.09001359678431926, 'learning_rate': 0.0002415762344714122, 'use_focal_loss': True} => F1=0.6071\n",
      " 53%|█████▎    | 16/30 [7:26:40<5:32:28, 1424.90s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24820/24820 1:08:18, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.603000</td>\n",
       "      <td>0.598805</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.599100</td>\n",
       "      <td>0.599181</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.597900</td>\n",
       "      <td>0.600134</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.597100</td>\n",
       "      <td>0.600049</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 3.0, 'label_smoothing': 0.02124180043594457, 'learning_rate': 0.00024079132520729045, 'use_focal_loss': False} => F1=0.6071\n",
      " 57%|█████▋    | 17/30 [8:35:30<8:05:00, 2238.47s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 50:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.606200</td>\n",
       "      <td>0.601562</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0.601579</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.600100</td>\n",
       "      <td>0.604901</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 1.0, 'label_smoothing': 0.028101331397877828, 'learning_rate': 0.00031678710574485977, 'use_focal_loss': False} => F1=0.6071\n",
      " 60%|██████    | 18/30 [9:26:45<8:17:56, 2489.73s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 50:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.656417</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.656364</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.655400</td>\n",
       "      <td>0.656937</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 4.5, 'label_smoothing': 0.19806543660787856, 'learning_rate': 9.296619262909997e-05, 'use_focal_loss': False} => F1=0.6071\n",
      " 63%|██████▎   | 19/30 [10:17:50<8:08:07, 2662.50s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 49:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.612100</td>\n",
       "      <td>0.623946</td>\n",
       "      <td>0.813240</td>\n",
       "      <td>0.810834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.576800</td>\n",
       "      <td>0.591409</td>\n",
       "      <td>0.822775</td>\n",
       "      <td>0.816740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.536900</td>\n",
       "      <td>0.560643</td>\n",
       "      <td>0.857246</td>\n",
       "      <td>0.856564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 1.5, 'label_smoothing': 0.15589210799291897, 'learning_rate': 3.4469853421405605e-05, 'use_focal_loss': False} => F1=0.8572\n",
      " 67%|██████▋   | 20/30 [11:08:03<7:41:17, 2767.79s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 38:12, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.027983</td>\n",
       "      <td>0.870079</td>\n",
       "      <td>0.866352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.026660</td>\n",
       "      <td>0.882600</td>\n",
       "      <td>0.880695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.062120</td>\n",
       "      <td>0.882917</td>\n",
       "      <td>0.881370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.095838</td>\n",
       "      <td>0.885691</td>\n",
       "      <td>0.884070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 4, 'gamma': 3.5, 'label_smoothing': 0.06652181199212014, 'learning_rate': 3.999504592504038e-05, 'use_focal_loss': True} => F1=0.8857\n",
      " 70%|███████   | 21/30 [11:46:47<6:35:12, 2634.68s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 25:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.019342</td>\n",
       "      <td>0.879236</td>\n",
       "      <td>0.875802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.020928</td>\n",
       "      <td>0.878882</td>\n",
       "      <td>0.874958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.029247</td>\n",
       "      <td>0.880384</td>\n",
       "      <td>0.877152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.042514</td>\n",
       "      <td>0.885840</td>\n",
       "      <td>0.883564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 4.0, 'label_smoothing': 0.051267275213648905, 'learning_rate': 1.0398989510547704e-05, 'use_focal_loss': True} => F1=0.8858\n",
      " 73%|███████▎  | 22/30 [12:13:14<5:09:21, 2320.23s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 38:19, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.882860</td>\n",
       "      <td>0.881033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.042338</td>\n",
       "      <td>0.887868</td>\n",
       "      <td>0.886939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.083416</td>\n",
       "      <td>0.888895</td>\n",
       "      <td>0.887445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.135628</td>\n",
       "      <td>0.888505</td>\n",
       "      <td>0.886601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 4, 'gamma': 3.0, 'label_smoothing': 0.07708953726284096, 'learning_rate': 2.9729599618629024e-05, 'use_focal_loss': True} => F1=0.8889\n",
      " 77%|███████▋  | 23/30 [12:52:12<4:31:18, 2325.44s/trial, best loss: -0.892011039832924]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 13:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.018936</td>\n",
       "      <td>0.888721</td>\n",
       "      <td>0.887108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.020919</td>\n",
       "      <td>0.892705</td>\n",
       "      <td>0.890989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.0, 'label_smoothing': 0.1414757766530963, 'learning_rate': 4.798455916219888e-05, 'use_focal_loss': True} => F1=0.8927\n",
      " 80%|████████  | 24/30 [13:05:47<3:07:14, 1872.33s/trial, best loss: -0.8927049346409967]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 19:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.037953</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.038139</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 4.0, 'label_smoothing': 0.10390261914513857, 'learning_rate': 5.3424417668195164e-05, 'use_focal_loss': True} => F1=0.6071\n",
      " 83%|████████▎ | 25/30 [13:25:33<2:18:51, 1666.35s/trial, best loss: -0.8927049346409967]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 13:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.018048</td>\n",
       "      <td>0.885784</td>\n",
       "      <td>0.883226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.023999</td>\n",
       "      <td>0.889322</td>\n",
       "      <td>0.887108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.0, 'label_smoothing': 0.03554493012139208, 'learning_rate': 2.414437389287209e-05, 'use_focal_loss': True} => F1=0.8893\n",
      " 87%|████████▋ | 26/30 [13:39:25<1:34:24, 1416.05s/trial, best loss: -0.8927049346409967]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 13:59, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.038195</td>\n",
       "      <td>0.878527</td>\n",
       "      <td>0.874958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.043816</td>\n",
       "      <td>0.889242</td>\n",
       "      <td>0.887108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 3.0, 'label_smoothing': 0.04145571816519898, 'learning_rate': 5.279633310022454e-05, 'use_focal_loss': True} => F1=0.8892\n",
      " 90%|█████████ | 27/30 [13:54:00<1:02:41, 1253.73s/trial, best loss: -0.8927049346409967]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 17:59, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.018853</td>\n",
       "      <td>0.877801</td>\n",
       "      <td>0.874452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.023372</td>\n",
       "      <td>0.887859</td>\n",
       "      <td>0.885420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 4.0, 'label_smoothing': 0.07214507156523497, 'learning_rate': 1.0302550249527089e-05, 'use_focal_loss': True} => F1=0.8879\n",
      " 93%|█████████▎| 28/30 [14:12:34<40:23, 1211.76s/trial, best loss: -0.8927049346409967]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 14:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>0.150067</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.150219</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 2.0, 'label_smoothing': 0.12099940322990729, 'learning_rate': 0.0004715680943388656, 'use_focal_loss': True} => F1=0.6071\n",
      " 97%|█████████▋| 29/30 [14:27:38<18:39, 1119.60s/trial, best loss: -0.8927049346409967]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 17:18, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>0.885956</td>\n",
       "      <td>0.884914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.882968</td>\n",
       "      <td>0.880695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 5.0, 'label_smoothing': 0.13984736905248035, 'learning_rate': 4.293097221060447e-05, 'use_focal_loss': True} => F1=0.8860\n",
      "100%|██████████| 30/30 [14:45:26<00:00, 1770.89s/trial, best loss: -0.8927049346409967]\n",
      "\n",
      "Hyperopt best param indices: {'batch_size': np.int64(2), 'epochs': np.int64(0), 'gamma': np.float64(4.0), 'label_smoothing': np.float64(0.1414757766530963), 'learning_rate': np.float64(4.798455916219888e-05), 'use_focal_loss': np.int64(1)}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  7. HYPERPARAMETER SEARCH WITH HYPEROPT\n",
    "#     We'll define:\n",
    "#       - learning_rate\n",
    "#       - epochs\n",
    "#       - batch_size\n",
    "#       - use_focal_loss\n",
    "#       - gamma (for focal loss)\n",
    "#       - label_smoothing\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "    \"\"\"\n",
    "    Hyperopt Objective:\n",
    "      1) Create a CustomDebertaModel with the candidate hyperparams\n",
    "      2) Train on train_dataset, evaluate on dev_dataset\n",
    "      3) Return negative F1 (since Hyperopt minimizes)\n",
    "    \"\"\"\n",
    "    learning_rate = space[\"learning_rate\"]\n",
    "    epochs        = int(space[\"epochs\"])\n",
    "    batch_size    = int(space[\"batch_size\"])\n",
    "    use_focal_loss = space[\"use_focal_loss\"]\n",
    "    gamma          = space[\"gamma\"]\n",
    "    label_smoothing = space[\"label_smoothing\"]\n",
    "\n",
    "    # Build the model\n",
    "    model = CustomDebertaModel(\n",
    "        model_name=model_name,\n",
    "        num_labels=2,\n",
    "        use_focal_loss=use_focal_loss,\n",
    "        gamma=gamma,\n",
    "        label_smoothing=label_smoothing\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./sota-ed-checkpoints\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=8,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\", \n",
    "        logging_steps=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"dev\"],\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate(encoded_dataset[\"dev\"])\n",
    "    f1 = metrics[\"eval_f1\"]\n",
    "\n",
    "    print(f\"[Hyperopt] params={space} => F1={f1:.4f}\")\n",
    "    return {\"loss\": -f1, \"status\": STATUS_OK}\n",
    "\n",
    "\n",
    "# Define search space\n",
    "search_space = {\n",
    "    \"learning_rate\":   hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(5e-4)),\n",
    "    \"epochs\":          hp.choice(\"epochs\", [2, 3, 4]),\n",
    "    \"batch_size\":      hp.choice(\"batch_size\", [4, 8, 16]),\n",
    "    \"use_focal_loss\":  hp.choice(\"use_focal_loss\", [False, True]),\n",
    "    \"gamma\":           hp.quniform(\"gamma\", 1.0, 5.0, 0.5),    \n",
    "    \"label_smoothing\": hp.uniform(\"label_smoothing\", 0.0, 0.2)\n",
    "}\n",
    "\n",
    "max_evals = 30 \n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=max_evals,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"\\nHyperopt best param indices:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:09:10.010278Z",
     "iopub.status.busy": "2025-04-02T22:09:10.009906Z",
     "iopub.status.idle": "2025-04-02T22:09:10.016588Z",
     "shell.execute_reply": "2025-04-02T22:09:10.015799Z",
     "shell.execute_reply.started": "2025-04-02T22:09:10.010247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreted best hyperparams:\n",
      " {'learning_rate': np.float64(4.798455916219888e-05), 'epochs': 2, 'batch_size': 16, 'use_focal_loss': True, 'gamma': np.float64(4.0), 'label_smoothing': np.float64(0.1414757766530963)}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  7.1 Interpret best param indices from Hyperopt\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "epochs_options = [2, 3, 4]\n",
    "batch_options  = [4, 8, 16]\n",
    "use_focal_options = [False, True]\n",
    "\n",
    "final_params = {\n",
    "    \"learning_rate\":    best[\"learning_rate\"],\n",
    "    \"epochs\":           epochs_options[best[\"epochs\"]],\n",
    "    \"batch_size\":       batch_options[ best[\"batch_size\"] ],\n",
    "    \"use_focal_loss\":   use_focal_options[ best[\"use_focal_loss\"] ],\n",
    "    \"gamma\":            best[\"gamma\"],\n",
    "    \"label_smoothing\":  best[\"label_smoothing\"]\n",
    "}\n",
    "\n",
    "print(\"Interpreted best hyperparams:\\n\", final_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:09:12.614271Z",
     "iopub.status.busy": "2025-04-02T22:09:12.613986Z",
     "iopub.status.idle": "2025-04-02T22:25:32.967972Z",
     "shell.execute_reply": "2025-04-02T22:25:32.967039Z",
     "shell.execute_reply.started": "2025-04-02T22:09:12.614249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_6292\\4257955412.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 12:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.019016</td>\n",
       "      <td>0.878786</td>\n",
       "      <td>0.875802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.020458</td>\n",
       "      <td>0.890355</td>\n",
       "      <td>0.888458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dev Results: {'eval_loss': 0.02045840583741665, 'eval_f1': 0.8903553648924198, 'eval_accuracy': 0.8884576442794465, 'eval_runtime': 26.1667, 'eval_samples_per_second': 226.471, 'eval_steps_per_second': 28.318, 'epoch': 2.0}\n",
      "\n",
      "Detailed Classification Report (Dev):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9439    0.8992    0.9210      4286\n",
      "           1     0.7656    0.8604    0.8102      1640\n",
      "\n",
      "    accuracy                         0.8885      5926\n",
      "   macro avg     0.8548    0.8798    0.8656      5926\n",
      "weighted avg     0.8946    0.8885    0.8904      5926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  8. TRAIN A FINAL MODEL USING THE BEST HYPERPARAMS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "best_model = CustomDebertaModel(\n",
    "    model_name=model_name,\n",
    "    num_labels=2,\n",
    "    use_focal_loss=final_params[\"use_focal_loss\"],\n",
    "    gamma=final_params[\"gamma\"],\n",
    "    label_smoothing=final_params[\"label_smoothing\"]\n",
    ")\n",
    "best_model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final-sota-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=final_params[\"learning_rate\"],\n",
    "    num_train_epochs=final_params[\"epochs\"],\n",
    "    per_device_train_batch_size=final_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=8,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",  \n",
    "    logging_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results_dev = trainer.evaluate(encoded_dataset[\"dev\"])\n",
    "print(\"Final Dev Results:\", results_dev)\n",
    "\n",
    "# Classification report\n",
    "preds_output = trainer.predict(encoded_dataset[\"dev\"])\n",
    "dev_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "dev_labels = preds_output.label_ids\n",
    "print(\"\\nDetailed Classification Report (Dev):\")\n",
    "print(classification_report(dev_labels, dev_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T20:18:11.819913Z",
     "iopub.status.busy": "2025-04-02T20:18:11.819568Z",
     "iopub.status.idle": "2025-04-02T20:20:16.796849Z",
     "shell.execute_reply": "2025-04-02T20:20:16.796163Z",
     "shell.execute_reply.started": "2025-04-02T20:18:11.819882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved to data\\taskC\\best_deberta_model.pt\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 8.1 SAVE THE BEST MODEL\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "hf_config_dict = trainer.model.model.config.to_dict()\n",
    "\n",
    "# 2) Store everything in one dictionary\n",
    "save_dict = {\n",
    "    \"model_state_dict\": trainer.model.state_dict(),\n",
    "    \n",
    "    # The base Hugging Face model config \n",
    "    \"hf_config\": hf_config_dict,\n",
    "    \n",
    "    # Custom hyperparameters for re-initializing the custom wrapper\n",
    "    \"hyperparams\": {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_labels\": trainer.model.num_labels,\n",
    "        \"use_focal_loss\": trainer.model.use_focal_loss,\n",
    "        \"gamma\": float(trainer.model.gamma),\n",
    "        \"label_smoothing\": float(trainer.model.label_smoothing)\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(save_dict, BEST_MODEL_PATH)\n",
    "print(f\"Model checkpoint saved to {BEST_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev predictions saved to dev_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 9.1 INFERENCE ON THE DEV SET (codebench debugging)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DEV_OUTPUT_PATH = \"dev_predictions.csv\"\n",
    "\n",
    "# Create a DataLoader for the dev set\n",
    "dev_loader = DataLoader(encoded_dataset[\"dev\"], batch_size=8)\n",
    "trainer.model.eval()\n",
    "all_dev_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = trainer.model(**batch)\n",
    "        logits = outputs[\"logits\"]\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_dev_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "dev_pred_df = pd.DataFrame({\"prediction\": all_dev_preds})\n",
    "dev_pred_df.to_csv(DEV_OUTPUT_PATH, index=False)\n",
    "print(f\"Dev predictions saved to {DEV_OUTPUT_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to data\\taskC\\predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 9.2 INFERENCE ON THE TEST SET \n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_loader = DataLoader(encoded_dataset[\"test\"], batch_size=8)\n",
    "trainer.model.eval()\n",
    "all_test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = trainer.model(**batch)\n",
    "        logits = outputs[\"logits\"]\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_test_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "test_pred_df = pd.DataFrame({\"prediction\": all_test_preds})\n",
    "test_pred_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Test predictions saved to {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6952367,
     "sourceId": 11144868,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
