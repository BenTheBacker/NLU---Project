{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71917d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m BEST_MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtaskB\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mED_B_Model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 118\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBEST_MODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Extract hyperparams & embedding matrix from the checkpoint\u001b[39;00m\n\u001b[0;32m    121\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1464\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1468\u001b[0m                 )\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# load_model.py\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "########################################\n",
    "# 1. Define or import your model classes\n",
    "########################################\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
    "        self.v = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_outputs, mask=None):\n",
    "        # lstm_outputs: (B, L, 2H)\n",
    "        score = torch.tanh(self.W(lstm_outputs))    # (B, L, 2H)\n",
    "        score = self.v(score).squeeze(-1)          # (B, L)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = F.softmax(score, dim=-1)    # (B, L)\n",
    "        attn_weights = attn_weights.unsqueeze(1)   # (B, 1, L)\n",
    "        context = torch.bmm(attn_weights, lstm_outputs)  # (B, 1, 2H)\n",
    "        context = context.squeeze(1)               # (B, 2H)\n",
    "        return context\n",
    "\n",
    "\n",
    "class CustomBiLSTMModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 embed_dim=300,\n",
    "                 hidden_dim=256, \n",
    "                 num_labels=2,\n",
    "                 num_layers=2, \n",
    "                 dropout=0.3, \n",
    "                 use_attention=True,\n",
    "                 use_focal_loss=False, \n",
    "                 gamma=2.0, \n",
    "                 label_smoothing=0.0,\n",
    "                 embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            with torch.no_grad():\n",
    "                self.embedding.weight.copy_(embedding_matrix)\n",
    "\n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Optional attention\n",
    "        if self.use_attention:\n",
    "            self.attn = SimpleAttention(self.hidden_dim)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(2 * self.hidden_dim, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        # Embeddings\n",
    "        embeds = self.embedding(input_ids)\n",
    "        if attention_mask is not None:\n",
    "            expand_mask = attention_mask.unsqueeze(-1).float()\n",
    "            embeds = embeds * expand_mask\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_outputs, (h, c) = self.lstm(embeds)  # (B, L, 2H), h shape: (2*num_layers, B, H)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            context = self.attn(lstm_outputs, mask=attention_mask) \n",
    "        else:\n",
    "            # Take the last forward and backward states from the top LSTM layer\n",
    "            h_forward = h[-2]  # last layer's forward state\n",
    "            h_backward = h[-1] # last layer's backward state\n",
    "            context = torch.cat((h_forward, h_backward), dim=-1)\n",
    "\n",
    "        logits = self.classifier(context)\n",
    "\n",
    "        # We typically return a dict { \"loss\": ..., \"logits\": ... } for HF Trainer\n",
    "        # but for simple inference, we just return logits\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "########################################\n",
    "# 2. Load the Trained Model Checkpoint\n",
    "########################################\n",
    "\n",
    "BEST_MODEL_PATH = \"data\\\\taskB\\\\ED_B_Model.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    BEST_MODEL_PATH, \n",
    "    map_location=device, \n",
    "    weights_only=False\n",
    ")\n",
    "\n",
    "# Extract hyperparams & embedding matrix from the checkpoint\n",
    "hyperparams = checkpoint[\"hyperparams\"]\n",
    "embedding_matrix_tensor = checkpoint[\"embedding_matrix\"].to(device)\n",
    "\n",
    "# Re-instantiate the model\n",
    "loaded_model = CustomBiLSTMModel(\n",
    "    vocab_size      = hyperparams[\"vocab_size\"],\n",
    "    embed_dim       = hyperparams[\"embed_dim\"],\n",
    "    hidden_dim      = hyperparams[\"hidden_dim\"],\n",
    "    num_labels      = hyperparams[\"num_labels\"],\n",
    "    num_layers      = hyperparams[\"num_layers\"],\n",
    "    dropout         = hyperparams[\"dropout\"],\n",
    "    use_attention   = hyperparams[\"use_attention\"],\n",
    "    use_focal_loss  = hyperparams[\"use_focal_loss\"],\n",
    "    gamma           = hyperparams[\"gamma\"],\n",
    "    label_smoothing = hyperparams[\"label_smoothing\"],\n",
    "    embedding_matrix=embedding_matrix_tensor\n",
    ")\n",
    "\n",
    "loaded_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model and embedding matrix loaded successfully!\")\n",
    "\n",
    "########################################\n",
    "# 3. Load and Preprocess Test Data\n",
    "########################################\n",
    "TEST_PATH = \"data\\\\test.csv\"  # Adapt if needed\n",
    "OUTPUT_PATH = \"data\\\\predictionsDemo.csv\"\n",
    "\n",
    "# Load test CSV with pandas\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# If your CSV columns are something else, rename them to \"claim\" and \"evidence\"\n",
    "test_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "\n",
    "# If there's no label column in your test set, that's okay; we won't use it\n",
    "# Just make sure the only textual columns used for tokenization are named \"claim\" and \"evidence\".\n",
    "\n",
    "# Convert to a Hugging Face Dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Initialize the same tokenizer that was used during training\n",
    "TOKENIZER_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"claim\"],\n",
    "        examples[\"evidence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenize\n",
    "encoded_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original text columns (only keep input_ids, attention_mask, etc.)\n",
    "encoded_test = encoded_test.remove_columns([\"claim\", \"evidence\"])\n",
    "# If you have other columns that are not relevant, remove them as well.\n",
    "\n",
    "# Set to PyTorch format\n",
    "encoded_test.set_format(\"torch\")\n",
    "\n",
    "########################################\n",
    "# 4. Run Inference on the Test Set\n",
    "########################################\n",
    "test_loader = DataLoader(encoded_test, batch_size=8)\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move each tensor in the batch to the same device as the model\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = loaded_model(**batch)   # forward pass\n",
    "        logits = outputs[\"logits\"]        # shape: (B, num_labels)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "########################################\n",
    "# 5. Save Predictions\n",
    "########################################\n",
    "# Create a DataFrame with the predicted labels\n",
    "test_pred_df = pd.DataFrame({\"prediction\": all_preds})\n",
    "\n",
    "# Save to CSV\n",
    "test_pred_df.to_csv(OUTPUT_PATH, index=False, header=True)\n",
    "print(f\"Predictions saved to: {OUTPUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
