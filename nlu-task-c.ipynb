{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:12.810376Z",
     "iopub.status.busy": "2025-04-02T15:33:12.810000Z",
     "iopub.status.idle": "2025-04-02T15:33:28.494143Z",
     "shell.execute_reply": "2025-04-02T15:33:28.493379Z",
     "shell.execute_reply.started": "2025-04-02T15:33:12.810339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Download necessary NLTK data (WordNet for synonyms)\n",
    "nltk_data_dir = \"/kaggle/working/nltk_data\"\n",
    "nltk.download(\"wordnet\", download_dir=nltk_data_dir)\n",
    "nltk.download(\"omw-1.4\", download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "#  1.1 Check GPU availability\n",
    "# ----------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:28.495800Z",
     "iopub.status.busy": "2025-04-02T15:33:28.495135Z",
     "iopub.status.idle": "2025-04-02T15:33:28.498982Z",
     "shell.execute_reply": "2025-04-02T15:33:28.498179Z",
     "shell.execute_reply.started": "2025-04-02T15:33:28.495772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora/\n",
    "#!unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:28.501064Z",
     "iopub.status.busy": "2025-04-02T15:33:28.500827Z",
     "iopub.status.idle": "2025-04-02T15:33:28.785416Z",
     "shell.execute_reply": "2025-04-02T15:33:28.784539Z",
     "shell.execute_reply.started": "2025-04-02T15:33:28.501033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 21508\n",
      "Dev samples: 5926\n",
      "Test samples: 21508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We should introduce school vouchers</td>\n",
       "      <td>Among the many educational reform efforts, suc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We should legalize insider trading</td>\n",
       "      <td>The U.S. Securities and Exchange Commission wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We should subsidize investigative journalism</td>\n",
       "      <td>The film won an Emmy Award (1980), George Polk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          claim  \\\n",
       "0           We should introduce school vouchers   \n",
       "1            We should legalize insider trading   \n",
       "2  We should subsidize investigative journalism   \n",
       "\n",
       "                                            evidence  label  \n",
       "0  Among the many educational reform efforts, suc...      0  \n",
       "1  The U.S. Securities and Exchange Commission wa...      0  \n",
       "2  The film won an Emmy Award (1980), George Polk...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  2. LOAD YOUR DATA\n",
    "#     Adjust these file paths to your environment. \n",
    "#     The CSVs must contain columns:\n",
    "#       train.csv: claim, evidence, label\n",
    "#       dev.csv:   claim, evidence, label\n",
    "#       test.csv:  claim, evidence, (no label)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "TRAIN_PATH = \"/kaggle/input/nlu-ed-task/train.csv\"\n",
    "DEV_PATH   = \"/kaggle/input/nlu-ed-task/dev.csv\"\n",
    "TEST_PATH  = \"/kaggle/input/nlu-ed-task/train.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df   = pd.read_csv(DEV_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "train_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "dev_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "test_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "\n",
    "print(\"Train samples:\", len(train_df))\n",
    "print(\"Dev samples:\", len(dev_df))\n",
    "print(\"Test samples:\", len(test_df))\n",
    "\n",
    "# If labels are strings, map them to integer {0,1} or {0,1,2,...}.\n",
    "# For ED, assume 2 classes: 0 = not evidence, 1 = relevant evidence\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "dev_df[\"label\"]   = dev_df[\"label\"].astype(int)\n",
    "\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:28.787430Z",
     "iopub.status.busy": "2025-04-02T15:33:28.787111Z",
     "iopub.status.idle": "2025-04-02T15:33:32.180188Z",
     "shell.execute_reply": "2025-04-02T15:33:32.179461Z",
     "shell.execute_reply.started": "2025-04-02T15:33:28.787399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: 21508  => After augmentation: 24819\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  3. (OPTIONAL) DATA AUGMENTATION (Synonym Replacement)\n",
    "#      - We'll replace 1 random word in claim/evidence with a WordNet synonym\n",
    "#      - For demonstration, there's a 15% chance per example to create an\n",
    "#        augmented copy.\n",
    "#      - This can help if your data is small or you want more variety.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    \"\"\"\n",
    "    Replace 'n' words in 'sentence' with synonyms from WordNet, if possible.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "\n",
    "    indices_to_replace = random.sample(range(len(words)), k=min(n, len(words)))\n",
    "    new_words = words[:]\n",
    "    for i in indices_to_replace:\n",
    "        word = words[i]\n",
    "        syns = wordnet.synsets(word)\n",
    "        if not syns:\n",
    "            continue\n",
    "        # For simplicity, pick from the first synset's lemmas\n",
    "        lemmas = syns[0].lemma_names()\n",
    "        # Filter out lemmas that are the same as the original\n",
    "        lemmas = [l for l in lemmas if l.lower() != word.lower()]\n",
    "        if len(lemmas) == 0:\n",
    "            continue\n",
    "        new_words[i] = random.choice(lemmas)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "def augment_dataframe(df, alpha=0.15):\n",
    "    \"\"\"\n",
    "    For each row, with probability alpha, create an augmented copy.\n",
    "    Return a new DataFrame with both original and augmented samples.\n",
    "    \"\"\"\n",
    "    augmented_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Original row\n",
    "        augmented_rows.append(row.to_dict())\n",
    "        \n",
    "        if random.random() < alpha:\n",
    "            new_row = row.copy()\n",
    "            # Randomly augment claim or evidence\n",
    "            if random.random() < 0.5:\n",
    "                new_row[\"claim\"] = synonym_replacement(row[\"claim\"], n=1)\n",
    "            else:\n",
    "                new_row[\"evidence\"] = synonym_replacement(row[\"evidence\"], n=1)\n",
    "            augmented_rows.append(new_row.to_dict())\n",
    "    return pd.DataFrame(augmented_rows)\n",
    "\n",
    "# Let's do a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# AUGMENT the training set (remove if undesired)\n",
    "augmented_train_df = augment_dataframe(train_df, alpha=0.15)\n",
    "print(\"Original train size:\", len(train_df), \n",
    "      \" => After augmentation:\", len(augmented_train_df))\n",
    "\n",
    "train_df = augmented_train_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:32.181249Z",
     "iopub.status.busy": "2025-04-02T15:33:32.180990Z",
     "iopub.status.idle": "2025-04-02T15:33:32.302244Z",
     "shell.execute_reply": "2025-04-02T15:33:32.301381Z",
     "shell.execute_reply.started": "2025-04-02T15:33:32.181207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'evidence', 'label'],\n",
       "        num_rows: 24819\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['claim', 'evidence', 'label'],\n",
       "        num_rows: 5926\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'evidence', 'label'],\n",
       "        num_rows: 21508\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  4. CREATE HUGGING FACE DATASETS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset   = Dataset.from_pandas(dev_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"dev\":   dev_dataset,\n",
    "    \"test\":  test_dataset\n",
    "})\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:32.303394Z",
     "iopub.status.busy": "2025-04-02T15:33:32.303159Z",
     "iopub.status.idle": "2025-04-02T15:33:43.991558Z",
     "shell.execute_reply": "2025-04-02T15:33:43.990633Z",
     "shell.execute_reply.started": "2025-04-02T15:33:32.303373Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ded11ed58b4606823ceac443d35175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20de1c12db1d49be9d1ec7d2d253795c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47de13b2b4eb4b6aaa8293632eca3ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f68f828ab0a4c66b54f2b09712dc240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3b35bb2dc84b739173d27138b3c7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb3623830ab434da72471520623ae7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  5. TOKENIZATION\n",
    "#     We'll use a powerful model: DeBERTa v3 (microsoft/deberta-v3-base)\n",
    "#     which is known to outperform standard BERT on many tasks.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"claim\"],\n",
    "        examples[\"evidence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "encoded_dataset = dataset_dict.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:43.992612Z",
     "iopub.status.busy": "2025-04-02T15:33:43.992398Z",
     "iopub.status.idle": "2025-04-02T15:33:44.005917Z",
     "shell.execute_reply": "2025-04-02T15:33:44.005123Z",
     "shell.execute_reply.started": "2025-04-02T15:33:43.992593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].rename_column(\"label\", \"labels\")\n",
    "encoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].rename_column(\"label\", \"labels\")\n",
    "# test split doesn't have label, so skip rename\n",
    "\n",
    "# Now you can safely remove columns or set the format\n",
    "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].remove_columns([\"claim\", \"evidence\"])\n",
    "encoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].remove_columns([\"claim\", \"evidence\"])\n",
    "\n",
    "# If your test set still has \"claim\" & \"evidence\", remove them too:\n",
    "encoded_dataset[\"test\"]  = encoded_dataset[\"test\"].remove_columns([\"claim\", \"evidence\"])\n",
    "\n",
    "# Then set format for PyTorch\n",
    "encoded_dataset[\"train\"].set_format(\"torch\")\n",
    "encoded_dataset[\"dev\"].set_format(\"torch\")\n",
    "encoded_dataset[\"test\"].set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:44.007174Z",
     "iopub.status.busy": "2025-04-02T15:33:44.006911Z",
     "iopub.status.idle": "2025-04-02T15:33:44.016571Z",
     "shell.execute_reply": "2025-04-02T15:33:44.015597Z",
     "shell.execute_reply.started": "2025-04-02T15:33:44.007153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  6. CUSTOM MODEL: Focal Loss or Label Smoothing\n",
    "#     We'll override forward() to allow advanced loss functions.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDebertaModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=2, use_focal_loss=False, gamma=2.0, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        # Load the pre-trained DeBERTa classification model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        # DeBERTa forward pass (omit internal CE)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None,\n",
    "            **kwargs\n",
    "        )\n",
    "        logits = outputs.logits  # shape: (batch_size, num_labels)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.use_focal_loss:\n",
    "                loss = self.focal_loss(logits, labels, self.gamma)\n",
    "            else:\n",
    "                loss = self.label_smoothing_loss(logits, labels, self.label_smoothing)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def focal_loss(self, logits, targets, gamma=2.0):\n",
    "        # Focal Loss\n",
    "        ce = nn.CrossEntropyLoss(reduction='none')(logits, targets)\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = (1 - pt)**gamma * ce\n",
    "        return focal.mean()\n",
    "\n",
    "    def label_smoothing_loss(self, logits, targets, smoothing=0.0):\n",
    "        if smoothing == 0.0:\n",
    "            return nn.CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "        log_probs = nn.LogSoftmax(dim=-1)(logits)\n",
    "        n_class = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(smoothing / (n_class - 1))\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:44.018971Z",
     "iopub.status.busy": "2025-04-02T15:33:44.018733Z",
     "iopub.status.idle": "2025-04-02T20:01:51.244494Z",
     "shell.execute_reply": "2025-04-02T20:01:51.243746Z",
     "shell.execute_reply.started": "2025-04-02T15:33:44.018952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1f94bd206848e7981fff7ff3f88c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 36:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.626700</td>\n",
       "      <td>0.626217</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.625700</td>\n",
       "      <td>0.626089</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.624800</td>\n",
       "      <td>0.628621</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 4.5, 'label_smoothing': 0.09456020674407445, 'learning_rate': 4.650364054074947e-05, 'use_focal_loss': False} => F1=0.6071\n",
      " 10%|█         | 1/10 [36:55<5:32:19, 2215.53s/trial, best loss: -0.607102296650199]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 15:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>0.304784</td>\n",
       "      <td>0.877063</td>\n",
       "      <td>0.873102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.325829</td>\n",
       "      <td>0.892632</td>\n",
       "      <td>0.890651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.5, 'label_smoothing': 0.00011108738704290744, 'learning_rate': 2.379886141068789e-05, 'use_focal_loss': False} => F1=0.8926\n",
      " 20%|██        | 2/10 [52:39<3:15:41, 1467.73s/trial, best loss: -0.8926320009887566]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 18:23, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>0.368823</td>\n",
       "      <td>0.878019</td>\n",
       "      <td>0.874620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.406623</td>\n",
       "      <td>0.889338</td>\n",
       "      <td>0.886939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 1.5, 'label_smoothing': 0.025140874030732153, 'learning_rate': 1.257211962841882e-05, 'use_focal_loss': False} => F1=0.8893\n",
      " 30%|███       | 3/10 [1:11:40<2:33:47, 1318.27s/trial, best loss: -0.8926320009887566]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 36:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.037970</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.038010</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.037949</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 4.0, 'label_smoothing': 0.14953429278875716, 'learning_rate': 0.0002599290684811281, 'use_focal_loss': True} => F1=0.6071\n",
      " 40%|████      | 4/10 [1:48:43<2:47:33, 1675.57s/trial, best loss: -0.8926320009887566]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 27:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.297070</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.296300</td>\n",
       "      <td>0.296987</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.275470</td>\n",
       "      <td>0.707435</td>\n",
       "      <td>0.738610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 1.0, 'label_smoothing': 0.005405331684439041, 'learning_rate': 0.00010436157084074034, 'use_focal_loss': True} => F1=0.7074\n",
      " 50%|█████     | 5/10 [2:16:51<2:20:00, 1680.15s/trial, best loss: -0.8926320009887566]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 18:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>0.297114</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.298429</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 1.0, 'label_smoothing': 0.042321244377780225, 'learning_rate': 8.501860704557743e-05, 'use_focal_loss': True} => F1=0.6071\n",
      " 60%|██████    | 6/10 [2:35:50<1:39:43, 1495.95s/trial, best loss: -0.8926320009887566]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 27:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.049547</td>\n",
       "      <td>0.851188</td>\n",
       "      <td>0.845764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.044236</td>\n",
       "      <td>0.877285</td>\n",
       "      <td>0.873945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.063105</td>\n",
       "      <td>0.882228</td>\n",
       "      <td>0.879683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 3.0, 'label_smoothing': 0.06689669298932018, 'learning_rate': 2.681460711622723e-05, 'use_focal_loss': True} => F1=0.8822\n",
      " 70%|███████   | 7/10 [3:03:57<1:17:55, 1558.44s/trial, best loss: -0.8926320009887566]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 18:18, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.053597</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 3.5, 'label_smoothing': 0.09892829430991, 'learning_rate': 0.00026964269961960493, 'use_focal_loss': True} => F1=0.6071\n",
      " 80%|████████  | 8/10 [3:22:52<47:27, 1423.77s/trial, best loss: -0.8926320009887566]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 36:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.106574</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.106598</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.106527</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 2.5, 'label_smoothing': 0.11402312832557485, 'learning_rate': 0.0002704088096216985, 'use_focal_loss': True} => F1=0.6071\n",
      " 90%|█████████ | 9/10 [3:59:55<27:53, 1673.53s/trial, best loss: -0.8926320009887566]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 27:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.398398</td>\n",
       "      <td>0.864380</td>\n",
       "      <td>0.864664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.288600</td>\n",
       "      <td>0.441032</td>\n",
       "      <td>0.865231</td>\n",
       "      <td>0.860952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.193900</td>\n",
       "      <td>0.482011</td>\n",
       "      <td>0.875046</td>\n",
       "      <td>0.872427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 2.0, 'label_smoothing': 0.022622711387814333, 'learning_rate': 5.689927283324451e-05, 'use_focal_loss': False} => F1=0.8750\n",
      "100%|██████████| 10/10 [4:28:07<00:00, 1608.72s/trial, best loss: -0.8926320009887566]\n",
      "\n",
      "Hyperopt best param indices: {'batch_size': 2, 'epochs': 0, 'gamma': 4.5, 'label_smoothing': 0.00011108738704290744, 'learning_rate': 2.379886141068789e-05, 'use_focal_loss': 0}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  7. HYPERPARAMETER SEARCH WITH HYPEROPT\n",
    "#     We'll define:\n",
    "#       - learning_rate\n",
    "#       - epochs\n",
    "#       - batch_size\n",
    "#       - use_focal_loss\n",
    "#       - gamma (for focal loss)\n",
    "#       - label_smoothing\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "    \"\"\"\n",
    "    Hyperopt Objective:\n",
    "      1) Create a CustomDebertaModel with the candidate hyperparams\n",
    "      2) Train on train_dataset, evaluate on dev_dataset\n",
    "      3) Return negative F1 (since Hyperopt minimizes)\n",
    "    \"\"\"\n",
    "    learning_rate = space[\"learning_rate\"]\n",
    "    epochs        = int(space[\"epochs\"])\n",
    "    batch_size    = int(space[\"batch_size\"])\n",
    "    use_focal_loss = space[\"use_focal_loss\"]\n",
    "    gamma          = space[\"gamma\"]\n",
    "    label_smoothing = space[\"label_smoothing\"]\n",
    "\n",
    "    # Build the model\n",
    "    model = CustomDebertaModel(\n",
    "        model_name=model_name,\n",
    "        num_labels=2,\n",
    "        use_focal_loss=use_focal_loss,\n",
    "        gamma=gamma,\n",
    "        label_smoothing=label_smoothing\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./sota-ed-checkpoints\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=8,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",  # Turn off W&B or any other tracking\n",
    "        logging_steps=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"dev\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate(encoded_dataset[\"dev\"])\n",
    "    f1 = metrics[\"eval_f1\"]\n",
    "\n",
    "    print(f\"[Hyperopt] params={space} => F1={f1:.4f}\")\n",
    "    return {\"loss\": -f1, \"status\": STATUS_OK}\n",
    "\n",
    "\n",
    "# Define search space\n",
    "search_space = {\n",
    "    \"learning_rate\":   hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(5e-4)),\n",
    "    \"epochs\":          hp.choice(\"epochs\", [2, 3, 4]),\n",
    "    \"batch_size\":      hp.choice(\"batch_size\", [4, 8, 16]),\n",
    "    \"use_focal_loss\":  hp.choice(\"use_focal_loss\", [False, True]),\n",
    "    \"gamma\":           hp.quniform(\"gamma\", 1.0, 5.0, 0.5),    # relevant if focal_loss=True\n",
    "    \"label_smoothing\": hp.uniform(\"label_smoothing\", 0.0, 0.2)\n",
    "}\n",
    "\n",
    "max_evals = 10  # For demonstration; increase for better search\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=max_evals,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"\\nHyperopt best param indices:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:09:10.010278Z",
     "iopub.status.busy": "2025-04-02T22:09:10.009906Z",
     "iopub.status.idle": "2025-04-02T22:09:10.016588Z",
     "shell.execute_reply": "2025-04-02T22:09:10.015799Z",
     "shell.execute_reply.started": "2025-04-02T22:09:10.010247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreted best hyperparams:\n",
      " {'learning_rate': 2.379886141068789e-05, 'epochs': 2, 'batch_size': 16, 'use_focal_loss': False, 'gamma': 4.5, 'label_smoothing': 0.00011108738704290744}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  7.1 Interpret best param indices from Hyperopt\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "epochs_options = [2, 3, 4]\n",
    "batch_options  = [4, 8, 16]\n",
    "use_focal_options = [False, True]\n",
    "\n",
    "final_params = {\n",
    "    \"learning_rate\":    best[\"learning_rate\"],\n",
    "    \"epochs\":           epochs_options[best[\"epochs\"]],\n",
    "    \"batch_size\":       batch_options[ best[\"batch_size\"] ],\n",
    "    \"use_focal_loss\":   use_focal_options[ best[\"use_focal_loss\"] ],\n",
    "    \"gamma\":            best[\"gamma\"],\n",
    "    \"label_smoothing\":  best[\"label_smoothing\"]\n",
    "}\n",
    "\n",
    "print(\"Interpreted best hyperparams:\\n\", final_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:09:12.614271Z",
     "iopub.status.busy": "2025-04-02T22:09:12.613986Z",
     "iopub.status.idle": "2025-04-02T22:25:32.967972Z",
     "shell.execute_reply": "2025-04-02T22:25:32.967039Z",
     "shell.execute_reply.started": "2025-04-02T22:09:12.614249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-21-ab1f2220c315>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 15:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>0.314324</td>\n",
       "      <td>0.871924</td>\n",
       "      <td>0.867195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.346780</td>\n",
       "      <td>0.889496</td>\n",
       "      <td>0.887108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dev Results: {'eval_loss': 0.3467804491519928, 'eval_f1': 0.889495550249558, 'eval_accuracy': 0.8871076611542356, 'eval_runtime': 34.26, 'eval_samples_per_second': 172.972, 'eval_steps_per_second': 21.629, 'epoch': 2.0}\n",
      "\n",
      "Detailed Classification Report (Dev):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9493    0.8915    0.9195      4286\n",
      "           1     0.7554    0.8756    0.8111      1640\n",
      "\n",
      "    accuracy                         0.8871      5926\n",
      "   macro avg     0.8524    0.8836    0.8653      5926\n",
      "weighted avg     0.8956    0.8871    0.8895      5926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  8. TRAIN A FINAL MODEL USING THE BEST HYPERPARAMS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "best_model = CustomDebertaModel(\n",
    "    model_name=model_name,\n",
    "    num_labels=2,\n",
    "    use_focal_loss=final_params[\"use_focal_loss\"],\n",
    "    gamma=final_params[\"gamma\"],\n",
    "    label_smoothing=final_params[\"label_smoothing\"]\n",
    ")\n",
    "best_model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final-sota-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=final_params[\"learning_rate\"],\n",
    "    num_train_epochs=final_params[\"epochs\"],\n",
    "    per_device_train_batch_size=final_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=8,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",  # Turn off W&B or any other tracking\n",
    "    logging_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results_dev = trainer.evaluate(encoded_dataset[\"dev\"])\n",
    "print(\"Final Dev Results:\", results_dev)\n",
    "\n",
    "# Optional: classification report\n",
    "preds_output = trainer.predict(encoded_dataset[\"dev\"])\n",
    "dev_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "dev_labels = preds_output.label_ids\n",
    "print(\"\\nDetailed Classification Report (Dev):\")\n",
    "print(classification_report(dev_labels, dev_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T20:18:11.819913Z",
     "iopub.status.busy": "2025-04-02T20:18:11.819568Z",
     "iopub.status.idle": "2025-04-02T20:20:16.796849Z",
     "shell.execute_reply": "2025-04-02T20:20:16.796163Z",
     "shell.execute_reply.started": "2025-04-02T20:18:11.819882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We should introduce school vouchers</td>\n",
       "      <td>Among the many educational reform efforts, suc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We should legalize insider trading</td>\n",
       "      <td>The U.S. Securities and Exchange Commission wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We should subsidize investigative journalism</td>\n",
       "      <td>The film won an Emmy Award (1980), George Polk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We should further exploit nuclear power</td>\n",
       "      <td>a 2001 survey by the European Commission found...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We should ban whaling</td>\n",
       "      <td>The US and several other nations are whaling u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          claim  \\\n",
       "0           We should introduce school vouchers   \n",
       "1            We should legalize insider trading   \n",
       "2  We should subsidize investigative journalism   \n",
       "3       We should further exploit nuclear power   \n",
       "4                         We should ban whaling   \n",
       "\n",
       "                                            evidence  label  \n",
       "0  Among the many educational reform efforts, suc...      0  \n",
       "1  The U.S. Securities and Exchange Commission wa...      0  \n",
       "2  The film won an Emmy Award (1980), George Polk...      0  \n",
       "3  a 2001 survey by the European Commission found...      1  \n",
       "4  The US and several other nations are whaling u...      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  9. INFERENCE ON TEST SET\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "test_predictions = trainer.predict(encoded_dataset[\"test\"])\n",
    "test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "# Add predictions to the test_df\n",
    "test_df[\"label\"] = test_preds\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T20:20:16.798054Z",
     "iopub.status.busy": "2025-04-02T20:20:16.797737Z",
     "iopub.status.idle": "2025-04-02T20:20:16.922228Z",
     "shell.execute_reply": "2025-04-02T20:20:16.921503Z",
     "shell.execute_reply.started": "2025-04-02T20:20:16.798023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to: test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  9.1 SAVE PREDICTIONS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "OUTPUT_PATH = \"test_predictions.csv\"\n",
    "test_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Test predictions saved to: {OUTPUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6952367,
     "sourceId": 11144868,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
