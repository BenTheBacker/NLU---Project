{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:12.810376Z",
     "iopub.status.busy": "2025-04-02T15:33:12.810000Z",
     "iopub.status.idle": "2025-04-02T15:33:28.494143Z",
     "shell.execute_reply": "2025-04-02T15:33:28.493379Z",
     "shell.execute_reply.started": "2025-04-02T15:33:12.810339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Backe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Backe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Download necessary NLTK data (WordNet for synonyms)\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "#  1.1 Check GPU availability\n",
    "# ----------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:28.495800Z",
     "iopub.status.busy": "2025-04-02T15:33:28.495135Z",
     "iopub.status.idle": "2025-04-02T15:33:28.498982Z",
     "shell.execute_reply": "2025-04-02T15:33:28.498179Z",
     "shell.execute_reply.started": "2025-04-02T15:33:28.495772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora/\n",
    "#!unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:28.501064Z",
     "iopub.status.busy": "2025-04-02T15:33:28.500827Z",
     "iopub.status.idle": "2025-04-02T15:33:28.785416Z",
     "shell.execute_reply": "2025-04-02T15:33:28.784539Z",
     "shell.execute_reply.started": "2025-04-02T15:33:28.501033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 21508\n",
      "Dev samples: 5926\n",
      "Test samples: 4688\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "claim",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "evidence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b3ff0ea4-69d0-487b-bbf9-cf4a99307ad3",
       "rows": [
        [
         "0",
         "We should introduce school vouchers",
         "Among the many educational reform efforts, such as charter schools, school vouchers, magnet schools, and alternative schools, the full-service community school model is one of many educational reform efforts that are intended to increase student achievement, but the full-service community school model specifically focuses on the development of the community as a whole.",
         "0"
        ],
        [
         "1",
         "We should legalize insider trading",
         "The U.S. Securities and Exchange Commission was established the following year, which helped combat insider trading and reducing transaction risk.  ",
         "0"
        ],
        [
         "2",
         "We should subsidize investigative journalism",
         "The film won an Emmy Award (1980), George Polk Award[REF] for investigative journalism on TV, Hugh M. Hefner First Amendment Award,[REF] and Best Documentary at the Mannheim Film Festival [REF].",
         "0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We should introduce school vouchers</td>\n",
       "      <td>Among the many educational reform efforts, suc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We should legalize insider trading</td>\n",
       "      <td>The U.S. Securities and Exchange Commission wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We should subsidize investigative journalism</td>\n",
       "      <td>The film won an Emmy Award (1980), George Polk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          claim  \\\n",
       "0           We should introduce school vouchers   \n",
       "1            We should legalize insider trading   \n",
       "2  We should subsidize investigative journalism   \n",
       "\n",
       "                                            evidence  label  \n",
       "0  Among the many educational reform efforts, suc...      0  \n",
       "1  The U.S. Securities and Exchange Commission wa...      0  \n",
       "2  The film won an Emmy Award (1980), George Polk...      0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  2. LOAD YOUR DATA\n",
    "#     Adjust these file paths to your environment. \n",
    "#     The CSVs must contain columns:\n",
    "#       train.csv: claim, evidence, label\n",
    "#       dev.csv:   claim, evidence, label\n",
    "#       test.csv:  claim, evidence, (no label)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "TRAIN_PATH = \"data\\\\train.csv\"\n",
    "DEV_PATH   = \"data\\\\dev.csv\"\n",
    "TEST_PATH  = \"data\\\\test.csv\"\n",
    "\n",
    "BEST_MODEL_PATH = \"data\\\\taskC\\\\best_deberta_model.pt\"\n",
    "OUTPUT_PATH = \"data\\\\taskC\\\\predictions.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df   = pd.read_csv(DEV_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "train_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "dev_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "test_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "\n",
    "print(\"Train samples:\", len(train_df))\n",
    "print(\"Dev samples:\", len(dev_df))\n",
    "print(\"Test samples:\", len(test_df))\n",
    "\n",
    "# If labels are strings, map them to integer {0,1} or {0,1,2,...}.\n",
    "# For ED, assume 2 classes: 0 = not evidence, 1 = relevant evidence\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "dev_df[\"label\"]   = dev_df[\"label\"].astype(int)\n",
    "\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:28.787430Z",
     "iopub.status.busy": "2025-04-02T15:33:28.787111Z",
     "iopub.status.idle": "2025-04-02T15:33:32.180188Z",
     "shell.execute_reply": "2025-04-02T15:33:32.179461Z",
     "shell.execute_reply.started": "2025-04-02T15:33:28.787399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: 21508  => After augmentation: 24819\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  3. (OPTIONAL) DATA AUGMENTATION (Synonym Replacement)\n",
    "#      - We'll replace 1 random word in claim/evidence with a WordNet synonym\n",
    "#      - For demonstration, there's a 15% chance per example to create an\n",
    "#        augmented copy.\n",
    "# \n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    \"\"\"\n",
    "    Replace 'n' words in 'sentence' with synonyms from WordNet, if possible.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "\n",
    "    indices_to_replace = random.sample(range(len(words)), k=min(n, len(words)))\n",
    "    new_words = words[:]\n",
    "    for i in indices_to_replace:\n",
    "        word = words[i]\n",
    "        syns = wordnet.synsets(word)\n",
    "        if not syns:\n",
    "            continue\n",
    "        # For simplicity, pick from the first synset's lemmas\n",
    "        lemmas = syns[0].lemma_names()\n",
    "        # Filter out lemmas that are the same as the original\n",
    "        lemmas = [l for l in lemmas if l.lower() != word.lower()]\n",
    "        if len(lemmas) == 0:\n",
    "            continue\n",
    "        new_words[i] = random.choice(lemmas)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "def augment_dataframe(df, alpha=0.15):\n",
    "    \"\"\"\n",
    "    For each row, with probability alpha, create an augmented copy.\n",
    "    Return a new DataFrame with both original and augmented samples.\n",
    "    \"\"\"\n",
    "    augmented_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Original row\n",
    "        augmented_rows.append(row.to_dict())\n",
    "        \n",
    "        if random.random() < alpha:\n",
    "            new_row = row.copy()\n",
    "            # Randomly augment claim or evidence\n",
    "            if random.random() < 0.5:\n",
    "                new_row[\"claim\"] = synonym_replacement(row[\"claim\"], n=1)\n",
    "            else:\n",
    "                new_row[\"evidence\"] = synonym_replacement(row[\"evidence\"], n=1)\n",
    "            augmented_rows.append(new_row.to_dict())\n",
    "    return pd.DataFrame(augmented_rows)\n",
    "\n",
    "# Let's do a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# AUGMENT the training set (remove if undesired)\n",
    "augmented_train_df = augment_dataframe(train_df, alpha=0.15)\n",
    "print(\"Original train size:\", len(train_df), \n",
    "      \" => After augmentation:\", len(augmented_train_df))\n",
    "\n",
    "train_df = augmented_train_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:32.181249Z",
     "iopub.status.busy": "2025-04-02T15:33:32.180990Z",
     "iopub.status.idle": "2025-04-02T15:33:32.302244Z",
     "shell.execute_reply": "2025-04-02T15:33:32.301381Z",
     "shell.execute_reply.started": "2025-04-02T15:33:32.181207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'evidence', 'label'],\n",
       "        num_rows: 24819\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['claim', 'evidence', 'label'],\n",
       "        num_rows: 5926\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['claim', 'evidence'],\n",
       "        num_rows: 4688\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  4. CREATE HUGGING FACE DATASETS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset   = Dataset.from_pandas(dev_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"dev\":   dev_dataset,\n",
    "    \"test\":  test_dataset\n",
    "})\n",
    "dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:32.303394Z",
     "iopub.status.busy": "2025-04-02T15:33:32.303159Z",
     "iopub.status.idle": "2025-04-02T15:33:43.991558Z",
     "shell.execute_reply": "2025-04-02T15:33:43.990633Z",
     "shell.execute_reply.started": "2025-04-02T15:33:32.303373Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7205b24c4071439da14274f51e013098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462ce94a23c340b8a45d79c9f287b7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e61a9e8b8648608eefc12acb41c801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  5. TOKENIZATION\n",
    "#     We use a powerful model: DeBERTa v3 (microsoft/deberta-v3-base)\n",
    "#     which is known to outperform standard BERT on many tasks.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"claim\"],\n",
    "        examples[\"evidence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "encoded_dataset = dataset_dict.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:43.992612Z",
     "iopub.status.busy": "2025-04-02T15:33:43.992398Z",
     "iopub.status.idle": "2025-04-02T15:33:44.005917Z",
     "shell.execute_reply": "2025-04-02T15:33:44.005123Z",
     "shell.execute_reply.started": "2025-04-02T15:33:43.992593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Handling the labels in the dataset\n",
    "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].rename_column(\"label\", \"labels\")\n",
    "encoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].rename_column(\"label\", \"labels\")\n",
    "\n",
    "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].remove_columns([\"claim\", \"evidence\"])\n",
    "encoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].remove_columns([\"claim\", \"evidence\"])\n",
    "\n",
    "encoded_dataset[\"test\"]  = encoded_dataset[\"test\"].remove_columns([\"claim\", \"evidence\"])\n",
    "\n",
    "encoded_dataset[\"train\"].set_format(\"torch\")\n",
    "encoded_dataset[\"dev\"].set_format(\"torch\")\n",
    "encoded_dataset[\"test\"].set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:44.007174Z",
     "iopub.status.busy": "2025-04-02T15:33:44.006911Z",
     "iopub.status.idle": "2025-04-02T15:33:44.016571Z",
     "shell.execute_reply": "2025-04-02T15:33:44.015597Z",
     "shell.execute_reply.started": "2025-04-02T15:33:44.007153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  6. CUSTOM MODEL: Focal Loss or Label Smoothing\n",
    "#     overridomg forward() to allow advanced loss functions.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDebertaModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=2, use_focal_loss=False, gamma=2.0, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        # Load the pre-trained DeBERTa classification model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        # DeBERTa forward pass (omit internal CE)\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None,\n",
    "            **kwargs\n",
    "        )\n",
    "        logits = outputs.logits  # shape: (batch_size, num_labels)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.use_focal_loss:\n",
    "                loss = self.focal_loss(logits, labels, self.gamma)\n",
    "            else:\n",
    "                loss = self.label_smoothing_loss(logits, labels, self.label_smoothing)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def focal_loss(self, logits, targets, gamma=2.0):\n",
    "        # Focal Loss\n",
    "        ce = nn.CrossEntropyLoss(reduction='none')(logits, targets)\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = (1 - pt)**gamma * ce\n",
    "        return focal.mean()\n",
    "\n",
    "    def label_smoothing_loss(self, logits, targets, smoothing=0.0):\n",
    "        if smoothing == 0.0:\n",
    "            return nn.CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "        log_probs = nn.LogSoftmax(dim=-1)(logits)\n",
    "        n_class = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(smoothing / (n_class - 1))\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T15:33:44.018971Z",
     "iopub.status.busy": "2025-04-02T15:33:44.018733Z",
     "iopub.status.idle": "2025-04-02T20:01:51.244494Z",
     "shell.execute_reply": "2025-04-02T20:01:51.243746Z",
     "shell.execute_reply.started": "2025-04-02T15:33:44.018952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12410' max='12410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12410/12410 31:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.300400</td>\n",
       "      <td>0.297859</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.299362</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 2, 'gamma': 1.0, 'label_smoothing': 0.08672050344959666, 'learning_rate': 0.00012634788355162484, 'use_focal_loss': True} => F1=0.6071\n",
      "  3%|▎         | 1/30 [31:31<15:14:12, 1891.46s/trial, best loss: -0.607102296650199]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 28:29, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.037999</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.037954</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.037944</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.037944</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 4.0, 'label_smoothing': 0.10773162807203462, 'learning_rate': 9.964250922861787e-05, 'use_focal_loss': True} => F1=0.6071\n",
      "  7%|▋         | 2/30 [1:00:33<14:01:32, 1803.31s/trial, best loss: -0.607102296650199]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 43:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.630700</td>\n",
       "      <td>0.627734</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.626900</td>\n",
       "      <td>0.627803</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.625800</td>\n",
       "      <td>0.628440</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 3.0, 'label_smoothing': 0.0993708290133283, 'learning_rate': 0.00027118448777552456, 'use_focal_loss': False} => F1=0.6071\n",
      " 10%|█         | 3/30 [1:44:06<16:17:52, 2173.04s/trial, best loss: -0.607102296650199]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 33:37, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>0.869561</td>\n",
       "      <td>0.866183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.016152</td>\n",
       "      <td>0.856087</td>\n",
       "      <td>0.850152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.024856</td>\n",
       "      <td>0.881629</td>\n",
       "      <td>0.880526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.049745</td>\n",
       "      <td>0.880927</td>\n",
       "      <td>0.878839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 4, 'gamma': 4.5, 'label_smoothing': 0.18027208373731465, 'learning_rate': 4.5080800504417184e-05, 'use_focal_loss': True} => F1=0.8816\n",
      " 13%|█▎        | 4/30 [2:18:15<15:20:28, 2124.19s/trial, best loss: -0.8816287159700587]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 37:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.037987</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.037948</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.037954</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.037944</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 4.0, 'label_smoothing': 0.011327595605151087, 'learning_rate': 0.00021970189070193293, 'use_focal_loss': True} => F1=0.6071\n",
      " 17%|█▋        | 5/30 [2:56:27<15:10:18, 2184.73s/trial, best loss: -0.8816287159700587]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 16:53, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.405200</td>\n",
       "      <td>0.355265</td>\n",
       "      <td>0.887670</td>\n",
       "      <td>0.886095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.288400</td>\n",
       "      <td>0.405907</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.891158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 1.5, 'label_smoothing': 0.031926145594475486, 'learning_rate': 1.879441696809551e-05, 'use_focal_loss': False} => F1=0.8934\n",
      " 20%|██        | 6/30 [3:13:50<11:58:37, 1796.56s/trial, best loss: -0.893369349629818] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12410' max='12410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12410/12410 10:35:40, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.043226</td>\n",
       "      <td>0.892043</td>\n",
       "      <td>0.891158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>0.056921</td>\n",
       "      <td>0.889431</td>\n",
       "      <td>0.886939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 2, 'gamma': 3.0, 'label_smoothing': 0.05051277325799206, 'learning_rate': 1.5929841121371466e-05, 'use_focal_loss': True} => F1=0.8920\n",
      " 23%|██▎       | 7/30 [13:50:03<87:27:20, 13688.71s/trial, best loss: -0.893369349629818]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 28:10, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>0.110107</td>\n",
       "      <td>0.869650</td>\n",
       "      <td>0.867364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.108732</td>\n",
       "      <td>0.883664</td>\n",
       "      <td>0.881370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.206960</td>\n",
       "      <td>0.876313</td>\n",
       "      <td>0.873102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.228995</td>\n",
       "      <td>0.880680</td>\n",
       "      <td>0.878670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 1.5, 'label_smoothing': 0.16069659461521935, 'learning_rate': 6.336198076397712e-05, 'use_focal_loss': True} => F1=0.8837\n",
      " 27%|██▋       | 8/30 [14:18:49<60:22:50, 9880.48s/trial, best loss: -0.893369349629818] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24820/24820 1:05:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.628200</td>\n",
       "      <td>0.626398</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.625895</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.625300</td>\n",
       "      <td>0.626997</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.624700</td>\n",
       "      <td>0.627043</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 1.0, 'label_smoothing': 0.09395626219382402, 'learning_rate': 0.0001097628027946798, 'use_focal_loss': False} => F1=0.6071\n",
      " 30%|███       | 9/30 [15:24:28<46:48:03, 8023.03s/trial, best loss: -0.893369349629818]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 13:04, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>0.404272</td>\n",
       "      <td>0.855033</td>\n",
       "      <td>0.848802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.365201</td>\n",
       "      <td>0.884653</td>\n",
       "      <td>0.882045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.5, 'label_smoothing': 0.024482437807113323, 'learning_rate': 1.1850862886557e-05, 'use_focal_loss': False} => F1=0.8847\n",
      " 33%|███▎      | 10/30 [15:38:03<32:12:39, 5798.00s/trial, best loss: -0.893369349629818]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 43:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.037994</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.038001</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.037954</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 4.0, 'label_smoothing': 0.06070184050785832, 'learning_rate': 0.00018683738197035072, 'use_focal_loss': True} => F1=0.6071\n",
      " 37%|███▋      | 11/30 [16:21:37<25:27:23, 4823.36s/trial, best loss: -0.893369349629818]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24820/24820 59:01, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.026912</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.026954</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.026877</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.026877</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 4.5, 'label_smoothing': 0.12992113965102156, 'learning_rate': 0.00012137691764950135, 'use_focal_loss': True} => F1=0.6071\n",
      " 40%|████      | 12/30 [17:21:08<22:12:42, 4442.38s/trial, best loss: -0.893369349629818]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 26:01, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.053580</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.053557</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.053553</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.053550</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 3.5, 'label_smoothing': 0.04895674939753514, 'learning_rate': 8.814181397671885e-05, 'use_focal_loss': True} => F1=0.6071\n",
      " 43%|████▎     | 13/30 [17:47:43<16:54:15, 3579.74s/trial, best loss: -0.893369349629818]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 27:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>0.484265</td>\n",
       "      <td>0.881844</td>\n",
       "      <td>0.878839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.484767</td>\n",
       "      <td>0.892397</td>\n",
       "      <td>0.890820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.399900</td>\n",
       "      <td>0.503069</td>\n",
       "      <td>0.893377</td>\n",
       "      <td>0.890989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 3.0, 'label_smoothing': 0.11205288449302564, 'learning_rate': 1.8158929545468875e-05, 'use_focal_loss': False} => F1=0.8934\n",
      " 47%|████▋     | 14/30 [18:15:21<13:19:51, 2999.47s/trial, best loss: -0.8933765059050405]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12410' max='12410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12410/12410 33:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.044836</td>\n",
       "      <td>0.889247</td>\n",
       "      <td>0.889133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.061949</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.889976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 2, 'gamma': 3.0, 'label_smoothing': 0.05304744261829811, 'learning_rate': 1.2955055291914709e-05, 'use_focal_loss': True} => F1=0.8918\n",
      " 50%|█████     | 15/30 [18:49:39<11:18:52, 2715.51s/trial, best loss: -0.8933765059050405]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 17:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.102923</td>\n",
       "      <td>0.885283</td>\n",
       "      <td>0.885589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.143638</td>\n",
       "      <td>0.885038</td>\n",
       "      <td>0.882720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 1.5, 'label_smoothing': 0.183157384568861, 'learning_rate': 4.393573366421746e-05, 'use_focal_loss': True} => F1=0.8853\n",
      " 53%|█████▎    | 16/30 [19:08:07<8:40:42, 2231.63s/trial, best loss: -0.8933765059050405] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 12:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>0.569749</td>\n",
       "      <td>0.883758</td>\n",
       "      <td>0.881033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.546200</td>\n",
       "      <td>0.572334</td>\n",
       "      <td>0.887388</td>\n",
       "      <td>0.884914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 3.5, 'label_smoothing': 0.19984721687915863, 'learning_rate': 1.4139342385561376e-05, 'use_focal_loss': False} => F1=0.8874\n",
      " 57%|█████▋    | 17/30 [19:20:59<6:28:27, 1792.86s/trial, best loss: -0.8933765059050405]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 35:59, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.536400</td>\n",
       "      <td>0.517599</td>\n",
       "      <td>0.881972</td>\n",
       "      <td>0.880020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.476500</td>\n",
       "      <td>0.545029</td>\n",
       "      <td>0.870022</td>\n",
       "      <td>0.865339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.532794</td>\n",
       "      <td>0.887634</td>\n",
       "      <td>0.884914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.423600</td>\n",
       "      <td>0.536544</td>\n",
       "      <td>0.891133</td>\n",
       "      <td>0.889133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 4, 'gamma': 2.0, 'label_smoothing': 0.13967662389764965, 'learning_rate': 2.7204750836743727e-05, 'use_focal_loss': False} => F1=0.8911\n",
      " 60%|██████    | 18/30 [19:57:34<6:22:42, 1913.58s/trial, best loss: -0.8933765059050405]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 24:28, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.050648</td>\n",
       "      <td>0.884913</td>\n",
       "      <td>0.882551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.057256</td>\n",
       "      <td>0.894367</td>\n",
       "      <td>0.892339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.101759</td>\n",
       "      <td>0.884469</td>\n",
       "      <td>0.881370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.143188</td>\n",
       "      <td>0.892096</td>\n",
       "      <td>0.889976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 2.5, 'label_smoothing': 0.06252359010244561, 'learning_rate': 1.785454033277208e-05, 'use_focal_loss': True} => F1=0.8944\n",
      " 63%|██████▎   | 19/30 [20:22:36<5:28:08, 1789.90s/trial, best loss: -0.8943666045652502]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6206/6206 17:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.610744</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>0.611502</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 5.0, 'label_smoothing': 0.04806870998017668, 'learning_rate': 0.00012525585317361445, 'use_focal_loss': False} => F1=0.6071\n",
      " 67%|██████▋   | 20/30 [20:41:01<4:24:03, 1584.34s/trial, best loss: -0.8943666045652502]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 26:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>0.430107</td>\n",
       "      <td>0.887714</td>\n",
       "      <td>0.886433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.383100</td>\n",
       "      <td>0.465321</td>\n",
       "      <td>0.889007</td>\n",
       "      <td>0.886770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.330300</td>\n",
       "      <td>0.480223</td>\n",
       "      <td>0.885971</td>\n",
       "      <td>0.883226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 2.5, 'label_smoothing': 0.08024144968797006, 'learning_rate': 2.5696553188284025e-05, 'use_focal_loss': False} => F1=0.8890\n",
      " 70%|███████   | 21/30 [21:08:22<4:00:12, 1601.37s/trial, best loss: -0.8943666045652502]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 26:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>0.493611</td>\n",
       "      <td>0.884505</td>\n",
       "      <td>0.882383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.452400</td>\n",
       "      <td>0.499779</td>\n",
       "      <td>0.888457</td>\n",
       "      <td>0.886095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.422200</td>\n",
       "      <td>0.512575</td>\n",
       "      <td>0.886555</td>\n",
       "      <td>0.883564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 2.5, 'label_smoothing': 0.11612128420639324, 'learning_rate': 1.0065107974453222e-05, 'use_focal_loss': False} => F1=0.8885\n",
      " 73%|███████▎  | 22/30 [21:35:46<3:35:12, 1614.11s/trial, best loss: -0.8943666045652502]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4656' max='4656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4656/4656 18:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.424796</td>\n",
       "      <td>0.881389</td>\n",
       "      <td>0.878502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.452577</td>\n",
       "      <td>0.879420</td>\n",
       "      <td>0.875464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.321500</td>\n",
       "      <td>0.459614</td>\n",
       "      <td>0.888278</td>\n",
       "      <td>0.885589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 3, 'gamma': 2.5, 'label_smoothing': 0.07385518682696005, 'learning_rate': 2.372892642546355e-05, 'use_focal_loss': False} => F1=0.8883\n",
      " 77%|███████▋  | 23/30 [21:54:29<2:51:08, 1466.89s/trial, best loss: -0.8943666045652502]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4656' max='4656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4656/4656 18:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>0.514711</td>\n",
       "      <td>0.879428</td>\n",
       "      <td>0.876477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.477600</td>\n",
       "      <td>0.525495</td>\n",
       "      <td>0.886994</td>\n",
       "      <td>0.884576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.442300</td>\n",
       "      <td>0.530840</td>\n",
       "      <td>0.889181</td>\n",
       "      <td>0.886770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 3, 'gamma': 2.0, 'label_smoothing': 0.14355111866704404, 'learning_rate': 3.789493852966346e-05, 'use_focal_loss': False} => F1=0.8892\n",
      " 80%|████████  | 24/30 [22:13:12<2:16:22, 1363.76s/trial, best loss: -0.8943666045652502]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 26:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.635700</td>\n",
       "      <td>0.634540</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.634610</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>0.634664</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.723253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 3.5, 'label_smoothing': 0.12036839329901747, 'learning_rate': 0.0004561999311834353, 'use_focal_loss': False} => F1=0.6071\n",
      " 83%|████████▎ | 25/30 [22:40:32<2:00:32, 1446.57s/trial, best loss: -0.8943666045652502]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4656' max='4656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4656/4656 18:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092500</td>\n",
       "      <td>0.077817</td>\n",
       "      <td>0.875211</td>\n",
       "      <td>0.873777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.081563</td>\n",
       "      <td>0.890812</td>\n",
       "      <td>0.890145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.130365</td>\n",
       "      <td>0.882864</td>\n",
       "      <td>0.880864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 3, 'gamma': 2.0, 'label_smoothing': 0.06784922911079358, 'learning_rate': 5.987226405908773e-05, 'use_focal_loss': True} => F1=0.8908\n",
      " 87%|████████▋ | 26/30 [22:59:16<1:29:58, 1349.60s/trial, best loss: -0.8943666045652502]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9309/9309 26:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.054065</td>\n",
       "      <td>0.886545</td>\n",
       "      <td>0.886095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.073362</td>\n",
       "      <td>0.889980</td>\n",
       "      <td>0.888289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.118762</td>\n",
       "      <td>0.891779</td>\n",
       "      <td>0.889808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 2.5, 'label_smoothing': 0.0009259536821840803, 'learning_rate': 3.136651912519059e-05, 'use_focal_loss': True} => F1=0.8918\n",
      " 90%|█████████ | 27/30 [23:26:35<1:11:50, 1436.69s/trial, best loss: -0.8943666045652502]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12412' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12412/12412 35:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.551000</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.874205</td>\n",
       "      <td>0.870233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.498800</td>\n",
       "      <td>0.538673</td>\n",
       "      <td>0.884148</td>\n",
       "      <td>0.881201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>0.544811</td>\n",
       "      <td>0.887429</td>\n",
       "      <td>0.885251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.548516</td>\n",
       "      <td>0.892464</td>\n",
       "      <td>0.890820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 8, 'epochs': 4, 'gamma': 3.5, 'label_smoothing': 0.1566917294437482, 'learning_rate': 2.0084866507935567e-05, 'use_focal_loss': False} => F1=0.8925\n",
      " 93%|█████████▎| 28/30 [24:02:45<55:13, 1656.69s/trial, best loss: -0.8943666045652502]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 24:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.173400</td>\n",
       "      <td>0.174015</td>\n",
       "      <td>0.867277</td>\n",
       "      <td>0.862302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.161441</td>\n",
       "      <td>0.888519</td>\n",
       "      <td>0.886264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.077400</td>\n",
       "      <td>0.205045</td>\n",
       "      <td>0.882880</td>\n",
       "      <td>0.880020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.248037</td>\n",
       "      <td>0.882640</td>\n",
       "      <td>0.880020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 1.0, 'label_smoothing': 0.08299773877656061, 'learning_rate': 1.0303387461831919e-05, 'use_focal_loss': True} => F1=0.8885\n",
      " 97%|█████████▋| 29/30 [24:27:26<26:43, 1603.91s/trial, best loss: -0.8943666045652502]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4656' max='4656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4656/4656 18:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.383500</td>\n",
       "      <td>0.329326</td>\n",
       "      <td>0.885684</td>\n",
       "      <td>0.884239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.379126</td>\n",
       "      <td>0.883801</td>\n",
       "      <td>0.880526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.401091</td>\n",
       "      <td>0.885612</td>\n",
       "      <td>0.882551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 3, 'gamma': 2.0, 'label_smoothing': 0.02416071096173772, 'learning_rate': 1.7804767303328657e-05, 'use_focal_loss': False} => F1=0.8857\n",
      "100%|██████████| 30/30 [24:46:10<00:00, 2972.35s/trial, best loss: -0.8943666045652502]\n",
      "\n",
      "Hyperopt best param indices: {'batch_size': np.int64(2), 'epochs': np.int64(2), 'gamma': np.float64(2.5), 'label_smoothing': np.float64(0.06252359010244561), 'learning_rate': np.float64(1.785454033277208e-05), 'use_focal_loss': np.int64(1)}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  7. HYPERPARAMETER SEARCH WITH HYPEROPT\n",
    "#     We'll define:\n",
    "#       - learning_rate\n",
    "#       - epochs\n",
    "#       - batch_size\n",
    "#       - use_focal_loss\n",
    "#       - gamma (for focal loss)\n",
    "#       - label_smoothing\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "    \"\"\"\n",
    "    Hyperopt Objective:\n",
    "      1) Create a CustomDebertaModel with the candidate hyperparams\n",
    "      2) Train on train_dataset, evaluate on dev_dataset\n",
    "      3) Return negative F1 (since Hyperopt minimizes)\n",
    "    \"\"\"\n",
    "    learning_rate = space[\"learning_rate\"]\n",
    "    epochs        = int(space[\"epochs\"])\n",
    "    batch_size    = int(space[\"batch_size\"])\n",
    "    use_focal_loss = space[\"use_focal_loss\"]\n",
    "    gamma          = space[\"gamma\"]\n",
    "    label_smoothing = space[\"label_smoothing\"]\n",
    "\n",
    "    # Build the model\n",
    "    model = CustomDebertaModel(\n",
    "        model_name=model_name,\n",
    "        num_labels=2,\n",
    "        use_focal_loss=use_focal_loss,\n",
    "        gamma=gamma,\n",
    "        label_smoothing=label_smoothing\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./sota-ed-checkpoints\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=8,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\", \n",
    "        logging_steps=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"dev\"],\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate(encoded_dataset[\"dev\"])\n",
    "    f1 = metrics[\"eval_f1\"]\n",
    "\n",
    "    print(f\"[Hyperopt] params={space} => F1={f1:.4f}\")\n",
    "    return {\"loss\": -f1, \"status\": STATUS_OK}\n",
    "\n",
    "\n",
    "# Define search space\n",
    "search_space = {\n",
    "    \"learning_rate\":   hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(5e-4)),\n",
    "    \"epochs\":          hp.choice(\"epochs\", [2, 3, 4]),\n",
    "    \"batch_size\":      hp.choice(\"batch_size\", [4, 8, 16]),\n",
    "    \"use_focal_loss\":  hp.choice(\"use_focal_loss\", [False, True]),\n",
    "    \"gamma\":           hp.quniform(\"gamma\", 1.0, 5.0, 0.5),    \n",
    "    \"label_smoothing\": hp.uniform(\"label_smoothing\", 0.0, 0.2)\n",
    "}\n",
    "\n",
    "max_evals = 30 \n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=max_evals,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"\\nHyperopt best param indices:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:09:10.010278Z",
     "iopub.status.busy": "2025-04-02T22:09:10.009906Z",
     "iopub.status.idle": "2025-04-02T22:09:10.016588Z",
     "shell.execute_reply": "2025-04-02T22:09:10.015799Z",
     "shell.execute_reply.started": "2025-04-02T22:09:10.010247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreted best hyperparams:\n",
      " {'learning_rate': np.float64(1.785454033277208e-05), 'epochs': 4, 'batch_size': 16, 'use_focal_loss': True, 'gamma': np.float64(2.5), 'label_smoothing': np.float64(0.06252359010244561)}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  7.1 Interpret best param indices from Hyperopt\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "epochs_options = [2, 3, 4]\n",
    "batch_options  = [4, 8, 16]\n",
    "use_focal_options = [False, True]\n",
    "\n",
    "final_params = {\n",
    "    \"learning_rate\":    best[\"learning_rate\"],\n",
    "    \"epochs\":           epochs_options[best[\"epochs\"]],\n",
    "    \"batch_size\":       batch_options[ best[\"batch_size\"] ],\n",
    "    \"use_focal_loss\":   use_focal_options[ best[\"use_focal_loss\"] ],\n",
    "    \"gamma\":            best[\"gamma\"],\n",
    "    \"label_smoothing\":  best[\"label_smoothing\"]\n",
    "}\n",
    "\n",
    "print(\"Interpreted best hyperparams:\\n\", final_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T22:09:12.614271Z",
     "iopub.status.busy": "2025-04-02T22:09:12.613986Z",
     "iopub.status.idle": "2025-04-02T22:25:32.967972Z",
     "shell.execute_reply": "2025-04-02T22:25:32.967039Z",
     "shell.execute_reply.started": "2025-04-02T22:09:12.614249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_9560\\3855427350.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 28:50, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.049908</td>\n",
       "      <td>0.882377</td>\n",
       "      <td>0.879514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.053875</td>\n",
       "      <td>0.887228</td>\n",
       "      <td>0.884576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.084432</td>\n",
       "      <td>0.888672</td>\n",
       "      <td>0.886601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.114946</td>\n",
       "      <td>0.889400</td>\n",
       "      <td>0.887276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dev Results: {'eval_loss': 0.11494570225477219, 'eval_f1': 0.8893995170056225, 'eval_accuracy': 0.8872764090448869, 'eval_runtime': 29.9141, 'eval_samples_per_second': 198.101, 'eval_steps_per_second': 24.771, 'epoch': 4.0}\n",
      "\n",
      "Detailed Classification Report (Dev):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9458    0.8955    0.9199      4286\n",
      "           1     0.7602    0.8659    0.8096      1640\n",
      "\n",
      "    accuracy                         0.8873      5926\n",
      "   macro avg     0.8530    0.8807    0.8648      5926\n",
      "weighted avg     0.8944    0.8873    0.8894      5926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "#  8. TRAIN A FINAL MODEL USING THE BEST HYPERPARAMS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "best_model = CustomDebertaModel(\n",
    "    model_name=model_name,\n",
    "    num_labels=2,\n",
    "    use_focal_loss=final_params[\"use_focal_loss\"],\n",
    "    gamma=final_params[\"gamma\"],\n",
    "    label_smoothing=final_params[\"label_smoothing\"]\n",
    ")\n",
    "best_model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final-sota-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=final_params[\"learning_rate\"],\n",
    "    num_train_epochs=final_params[\"epochs\"],\n",
    "    per_device_train_batch_size=final_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=8,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",  \n",
    "    logging_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"dev\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results_dev = trainer.evaluate(encoded_dataset[\"dev\"])\n",
    "print(\"Final Dev Results:\", results_dev)\n",
    "\n",
    "# Classification report\n",
    "preds_output = trainer.predict(encoded_dataset[\"dev\"])\n",
    "dev_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "dev_labels = preds_output.label_ids\n",
    "print(\"\\nDetailed Classification Report (Dev):\")\n",
    "print(classification_report(dev_labels, dev_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T20:18:11.819913Z",
     "iopub.status.busy": "2025-04-02T20:18:11.819568Z",
     "iopub.status.idle": "2025-04-02T20:20:16.796849Z",
     "shell.execute_reply": "2025-04-02T20:20:16.796163Z",
     "shell.execute_reply.started": "2025-04-02T20:18:11.819882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to data\\taskC\\best_deberta_model.pt\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 8.1 SAVE THE BEST MODEL\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "torch.save(trainer.model.state_dict(), BEST_MODEL_PATH)\n",
    "print(f\"Best model saved to {BEST_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T20:20:16.798054Z",
     "iopub.status.busy": "2025-04-02T20:20:16.797737Z",
     "iopub.status.idle": "2025-04-02T20:20:16.922228Z",
     "shell.execute_reply": "2025-04-02T20:20:16.921503Z",
     "shell.execute_reply.started": "2025-04-02T20:20:16.798023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded state_dict into loaded_model.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 8.2 LOAD THE BEST MODEL\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Re-initialize the same architecture\n",
    "loaded_model = CustomDebertaModel(\n",
    "    model_name=model_name,\n",
    "    num_labels=2,\n",
    "    use_focal_loss=final_params[\"use_focal_loss\"],\n",
    "    gamma=final_params[\"gamma\"],\n",
    "    label_smoothing=final_params[\"label_smoothing\"]\n",
    ")\n",
    "\n",
    "# Load the saved weights\n",
    "loaded_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Successfully loaded state_dict into loaded_model.\")\n",
    "\n",
    "# Assign the loaded model to the Trainer\n",
    "trainer.model = loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev predictions saved to dev_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 9.1 INFERENCE ON THE DEV SET (codebench debugging)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DEV_OUTPUT_PATH = \"dev_predictions.csv\"\n",
    "\n",
    "# Create a DataLoader for the dev set\n",
    "dev_loader = DataLoader(encoded_dataset[\"dev\"], batch_size=8)\n",
    "\n",
    "loaded_model.eval()\n",
    "all_dev_preds = []\n",
    "\n",
    "for batch in dev_loader:\n",
    "    for key in batch:\n",
    "        batch[key] = batch[key].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**batch)  \n",
    "    \n",
    "    logits = outputs[\"logits\"]                  \n",
    "    preds = torch.argmax(logits, dim=1)          \n",
    "    all_dev_preds.extend(preds.cpu().tolist())   \n",
    "\n",
    "# Create a single-column DataFrame for the dev predictions\n",
    "dev_pred_df = pd.DataFrame({\"prediction\": all_dev_preds})\n",
    "dev_pred_df.to_csv(DEV_OUTPUT_PATH, index=False)\n",
    "print(f\"Dev predictions saved to {DEV_OUTPUT_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported types (<class 'NoneType'>) passed to `_pad_across_processes`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m encoded_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 2) Predict\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(test_predictions\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 3) Save test predictions to a single-column CSV\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\transformers\\trainer.py:4232\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4229\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4231\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4232\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[0;32m   4234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4235\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\transformers\\trainer.py:4370\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4368\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m   4369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4370\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4372\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\accelerate\\accelerator.py:2813\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[1;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[0;32m   2780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   2781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2782\u001b[0m \u001b[38;5;124;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[0;32m   2783\u001b[0m \u001b[38;5;124;03m    they can safely be gathered.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   2812\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:403\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m         operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:673\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[1;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[0;32m    670\u001b[0m     new_tensor[indices] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[1;32m--> 673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:107\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[1;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03mRecursively apply a function on a data structure that is a nested list/tuple/dictionary of a given base type.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    The same data structure as `data` with `func` applied to every object of type `main_type`.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhonor_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_on_other_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[0;32m    118\u001b[0m         {\n\u001b[0;32m    119\u001b[0m             k: recursively_apply(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m         }\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:81\u001b[0m, in \u001b[0;36mhonor_type\u001b[1;34m(obj, generator)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(generator))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:110\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03mRecursively apply a function on a data structure that is a nested list/tuple/dictionary of a given base type.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    The same data structure as `data` with `func` applied to every object of type `main_type`.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m honor_type(\n\u001b[0;32m    108\u001b[0m         data,\n\u001b[0;32m    109\u001b[0m         (\n\u001b[1;32m--> 110\u001b[0m             recursively_apply(\n\u001b[0;32m    111\u001b[0m                 func, o, \u001b[38;5;241m*\u001b[39margs, test_type\u001b[38;5;241m=\u001b[39mtest_type, error_on_other_type\u001b[38;5;241m=\u001b[39merror_on_other_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    112\u001b[0m             )\n\u001b[0;32m    113\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[0;32m    114\u001b[0m         ),\n\u001b[0;32m    115\u001b[0m     )\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[0;32m    118\u001b[0m         {\n\u001b[0;32m    119\u001b[0m             k: recursively_apply(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m         }\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:128\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[1;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m     )\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mTypeError\u001b[0m: Unsupported types (<class 'NoneType'>) passed to `_pad_across_processes`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed."
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 9.2 INFERENCE ON THE TEST SET \n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_loader = DataLoader(encoded_dataset[\"test\"], batch_size=8)\n",
    "\n",
    "loaded_model.eval()\n",
    "all_test_preds = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    # Move inputs to GPU/CPU\n",
    "    for key in batch:\n",
    "        batch[key] = batch[key].to(device)\n",
    "    \n",
    "    # No gradient calculation needed\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**batch)  # outputs contains \"logits\"\n",
    "    \n",
    "    logits = outputs[\"logits\"]                   # shape (B, num_labels)\n",
    "    preds = torch.argmax(logits, dim=1)          # shape (B,)\n",
    "    all_test_preds.extend(preds.cpu().tolist())  # Collect predictions on CPU\n",
    "\n",
    "# Create a single-column DataFrame for the test predictions\n",
    "test_pred_df = pd.DataFrame({\"prediction\": all_test_preds})\n",
    "test_pred_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Test predictions saved to {OUTPUT_PATH}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6952367,
     "sourceId": 11144868,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
