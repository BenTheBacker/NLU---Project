{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11144868,"sourceType":"datasetVersion","datasetId":6952367}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport random\nimport nltk\n\nfrom tqdm import tqdm\nfrom nltk.corpus import wordnet\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset, DatasetDict\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\n\n# Download necessary NLTK data (WordNet for synonyms)\nnltk_data_dir = \"/kaggle/working/nltk_data\"\nnltk.download(\"wordnet\", download_dir=nltk_data_dir)\nnltk.download(\"omw-1.4\", download_dir=nltk_data_dir)\nnltk.data.path.append(nltk_data_dir)\n\n# ----------------------------------------------------------------------------\n#  1.1 Check GPU availability\n# ----------------------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:33:12.810000Z","iopub.execute_input":"2025-04-02T15:33:12.810376Z","iopub.status.idle":"2025-04-02T15:33:28.494143Z","shell.execute_reply.started":"2025-04-02T15:33:12.810339Z","shell.execute_reply":"2025-04-02T15:33:28.493379Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\nUsing device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#!unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora/\n#!unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:33:28.495135Z","iopub.execute_input":"2025-04-02T15:33:28.495800Z","iopub.status.idle":"2025-04-02T15:33:28.498982Z","shell.execute_reply.started":"2025-04-02T15:33:28.495772Z","shell.execute_reply":"2025-04-02T15:33:28.498179Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n#  2. LOAD YOUR DATA\n#     Adjust these file paths to your environment. \n#     The CSVs must contain columns:\n#       train.csv: claim, evidence, label\n#       dev.csv:   claim, evidence, label\n#       test.csv:  claim, evidence       (no label)\n# ----------------------------------------------------------------------------\n\nTRAIN_PATH = \"/kaggle/input/nlu-ed-task/train.csv\"\nDEV_PATH   = \"/kaggle/input/nlu-ed-task/dev.csv\"\nTEST_PATH  = \"/kaggle/input/nlu-ed-task/train.csv\"\n\ntrain_df = pd.read_csv(TRAIN_PATH)\ndev_df   = pd.read_csv(DEV_PATH)\ntest_df  = pd.read_csv(TEST_PATH)\n\ntrain_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\ndev_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\ntest_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n\nprint(\"Train samples:\", len(train_df))\nprint(\"Dev samples:\", len(dev_df))\nprint(\"Test samples:\", len(test_df))\n\n# If labels are strings, map them to integer {0,1} or {0,1,2,...}.\n# For ED, assume 2 classes: 0 = not evidence, 1 = relevant evidence\ntrain_df[\"label\"] = train_df[\"label\"].astype(int)\ndev_df[\"label\"]   = dev_df[\"label\"].astype(int)\n\ntrain_df.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:33:28.500827Z","iopub.execute_input":"2025-04-02T15:33:28.501064Z","iopub.status.idle":"2025-04-02T15:33:28.785416Z","shell.execute_reply.started":"2025-04-02T15:33:28.501033Z","shell.execute_reply":"2025-04-02T15:33:28.784539Z"}},"outputs":[{"name":"stdout","text":"Train samples: 21508\nDev samples: 5926\nTest samples: 21508\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                          claim  \\\n0           We should introduce school vouchers   \n1            We should legalize insider trading   \n2  We should subsidize investigative journalism   \n\n                                            evidence  label  \n0  Among the many educational reform efforts, suc...      0  \n1  The U.S. Securities and Exchange Commission wa...      0  \n2  The film won an Emmy Award (1980), George Polk...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>claim</th>\n      <th>evidence</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>We should introduce school vouchers</td>\n      <td>Among the many educational reform efforts, suc...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>We should legalize insider trading</td>\n      <td>The U.S. Securities and Exchange Commission wa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>We should subsidize investigative journalism</td>\n      <td>The film won an Emmy Award (1980), George Polk...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n#  3. (OPTIONAL) DATA AUGMENTATION (Synonym Replacement)\n#      - We'll replace 1 random word in claim/evidence with a WordNet synonym\n#      - For demonstration, there's a 15% chance per example to create an\n#        augmented copy.\n#      - This can help if your data is small or you want more variety.\n# ----------------------------------------------------------------------------\n\ndef synonym_replacement(sentence, n=1):\n    \"\"\"\n    Replace 'n' words in 'sentence' with synonyms from WordNet, if possible.\n    \"\"\"\n    words = sentence.split()\n    if len(words) < 2:\n        return sentence\n\n    indices_to_replace = random.sample(range(len(words)), k=min(n, len(words)))\n    new_words = words[:]\n    for i in indices_to_replace:\n        word = words[i]\n        syns = wordnet.synsets(word)\n        if not syns:\n            continue\n        # For simplicity, pick from the first synset's lemmas\n        lemmas = syns[0].lemma_names()\n        # Filter out lemmas that are the same as the original\n        lemmas = [l for l in lemmas if l.lower() != word.lower()]\n        if len(lemmas) == 0:\n            continue\n        new_words[i] = random.choice(lemmas)\n    return \" \".join(new_words)\n\n\ndef augment_dataframe(df, alpha=0.15):\n    \"\"\"\n    For each row, with probability alpha, create an augmented copy.\n    Return a new DataFrame with both original and augmented samples.\n    \"\"\"\n    augmented_rows = []\n    for _, row in df.iterrows():\n        # Original row\n        augmented_rows.append(row.to_dict())\n        \n        if random.random() < alpha:\n            new_row = row.copy()\n            # Randomly augment claim or evidence\n            if random.random() < 0.5:\n                new_row[\"claim\"] = synonym_replacement(row[\"claim\"], n=1)\n            else:\n                new_row[\"evidence\"] = synonym_replacement(row[\"evidence\"], n=1)\n            augmented_rows.append(new_row.to_dict())\n    return pd.DataFrame(augmented_rows)\n\n# Let's do a random seed for reproducibility\nrandom.seed(42)\n\n# AUGMENT the training set (remove if undesired)\naugmented_train_df = augment_dataframe(train_df, alpha=0.15)\nprint(\"Original train size:\", len(train_df), \n      \" => After augmentation:\", len(augmented_train_df))\n\ntrain_df = augmented_train_df.reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:33:28.787111Z","iopub.execute_input":"2025-04-02T15:33:28.787430Z","iopub.status.idle":"2025-04-02T15:33:32.180188Z","shell.execute_reply.started":"2025-04-02T15:33:28.787399Z","shell.execute_reply":"2025-04-02T15:33:32.179461Z"}},"outputs":[{"name":"stdout","text":"Original train size: 21508  => After augmentation: 24819\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n#  4. CREATE HUGGING FACE DATASETS\n# ----------------------------------------------------------------------------\n\ntrain_dataset = Dataset.from_pandas(train_df)\ndev_dataset   = Dataset.from_pandas(dev_df)\ntest_dataset  = Dataset.from_pandas(test_df)\n\ndataset_dict = DatasetDict({\n    \"train\": train_dataset,\n    \"dev\":   dev_dataset,\n    \"test\":  test_dataset\n})\ndataset_dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:33:32.180990Z","iopub.execute_input":"2025-04-02T15:33:32.181249Z","iopub.status.idle":"2025-04-02T15:33:32.302244Z","shell.execute_reply.started":"2025-04-02T15:33:32.181207Z","shell.execute_reply":"2025-04-02T15:33:32.301381Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['claim', 'evidence', 'label'],\n        num_rows: 24819\n    })\n    dev: Dataset({\n        features: ['claim', 'evidence', 'label'],\n        num_rows: 5926\n    })\n    test: Dataset({\n        features: ['claim', 'evidence', 'label'],\n        num_rows: 21508\n    })\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n#  5. TOKENIZATION\n#     We'll use a powerful model: DeBERTa v3 (microsoft/deberta-v3-base)\n#     which is known to outperform standard BERT on many tasks.\n# ----------------------------------------------------------------------------\n\nmodel_name = \"microsoft/deberta-v3-base\"\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"claim\"],\n        examples[\"evidence\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128\n    )\n\nencoded_dataset = dataset_dict.map(tokenize_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:33:32.303159Z","iopub.execute_input":"2025-04-02T15:33:32.303394Z","iopub.status.idle":"2025-04-02T15:33:43.991558Z","shell.execute_reply.started":"2025-04-02T15:33:32.303373Z","shell.execute_reply":"2025-04-02T15:33:43.990633Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ded11ed58b4606823ceac443d35175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20de1c12db1d49be9d1ec7d2d253795c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47de13b2b4eb4b6aaa8293632eca3ebe"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f68f828ab0a4c66b54f2b09712dc240"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5926 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3b35bb2dc84b739173d27138b3c7fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcb3623830ab434da72471520623ae7e"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"encoded_dataset[\"train\"] = encoded_dataset[\"train\"].rename_column(\"label\", \"labels\")\nencoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].rename_column(\"label\", \"labels\")\n# test split doesn't have label, so skip rename\n\n# Now you can safely remove columns or set the format\nencoded_dataset[\"train\"] = encoded_dataset[\"train\"].remove_columns([\"claim\", \"evidence\"])\nencoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].remove_columns([\"claim\", \"evidence\"])\n\n# If your test set still has \"claim\" & \"evidence\", remove them too:\nencoded_dataset[\"test\"]  = encoded_dataset[\"test\"].remove_columns([\"claim\", \"evidence\"])\n\n# Then set format for PyTorch\nencoded_dataset[\"train\"].set_format(\"torch\")\nencoded_dataset[\"dev\"].set_format(\"torch\")\nencoded_dataset[\"test\"].set_format(\"torch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:33:43.992398Z","iopub.execute_input":"2025-04-02T15:33:43.992612Z","iopub.status.idle":"2025-04-02T15:33:44.005917Z","shell.execute_reply.started":"2025-04-02T15:33:43.992593Z","shell.execute_reply":"2025-04-02T15:33:44.005123Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n#  6. CUSTOM MODEL: Focal Loss or Label Smoothing\n#     We'll override forward() to allow advanced loss functions.\n# ----------------------------------------------------------------------------\n\nimport torch.nn as nn\n\nclass CustomDebertaModel(nn.Module):\n    def __init__(self, model_name, num_labels=2, use_focal_loss=False, gamma=2.0, label_smoothing=0.0):\n        super().__init__()\n        self.num_labels = num_labels\n        self.use_focal_loss = use_focal_loss\n        self.gamma = gamma\n        self.label_smoothing = label_smoothing\n        \n        # Load the pre-trained DeBERTa classification model\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=num_labels\n        )\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n        # DeBERTa forward pass (omit internal CE)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=None,\n            **kwargs\n        )\n        logits = outputs.logits  # shape: (batch_size, num_labels)\n        \n        loss = None\n        if labels is not None:\n            if self.use_focal_loss:\n                loss = self.focal_loss(logits, labels, self.gamma)\n            else:\n                loss = self.label_smoothing_loss(logits, labels, self.label_smoothing)\n        \n        return {\"loss\": loss, \"logits\": logits}\n\n    def focal_loss(self, logits, targets, gamma=2.0):\n        # Focal Loss\n        ce = nn.CrossEntropyLoss(reduction='none')(logits, targets)\n        pt = torch.exp(-ce)\n        focal = (1 - pt)**gamma * ce\n        return focal.mean()\n\n    def label_smoothing_loss(self, logits, targets, smoothing=0.0):\n        if smoothing == 0.0:\n            return nn.CrossEntropyLoss()(logits, targets)\n\n        log_probs = nn.LogSoftmax(dim=-1)(logits)\n        n_class = logits.size(1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(smoothing / (n_class - 1))\n            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:33:44.006911Z","iopub.execute_input":"2025-04-02T15:33:44.007174Z","iopub.status.idle":"2025-04-02T15:33:44.016571Z","shell.execute_reply.started":"2025-04-02T15:33:44.007153Z","shell.execute_reply":"2025-04-02T15:33:44.015597Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n#  7. HYPERPARAMETER SEARCH WITH HYPEROPT\n#     We'll define:\n#       - learning_rate\n#       - epochs\n#       - batch_size\n#       - use_focal_loss\n#       - gamma (for focal loss)\n#       - label_smoothing\n# ----------------------------------------------------------------------------\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"f1\": f1, \"accuracy\": acc}\n\n\ndef objective(space):\n    \"\"\"\n    Hyperopt Objective:\n      1) Create a CustomDebertaModel with the candidate hyperparams\n      2) Train on train_dataset, evaluate on dev_dataset\n      3) Return negative F1 (since Hyperopt minimizes)\n    \"\"\"\n    learning_rate = space[\"learning_rate\"]\n    epochs        = int(space[\"epochs\"])\n    batch_size    = int(space[\"batch_size\"])\n    use_focal_loss = space[\"use_focal_loss\"]\n    gamma          = space[\"gamma\"]\n    label_smoothing = space[\"label_smoothing\"]\n\n    # Build the model\n    model = CustomDebertaModel(\n        model_name=model_name,\n        num_labels=2,\n        use_focal_loss=use_focal_loss,\n        gamma=gamma,\n        label_smoothing=label_smoothing\n    )\n    model.to(device)\n\n    training_args = TrainingArguments(\n        output_dir=\"./sota-ed-checkpoints\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=8,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n        save_total_limit=1,\n        report_to=\"none\",  # Turn off W&B or any other tracking\n        logging_steps=1\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=encoded_dataset[\"train\"],\n        eval_dataset=encoded_dataset[\"dev\"],\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    metrics = trainer.evaluate(encoded_dataset[\"dev\"])\n    f1 = metrics[\"eval_f1\"]\n\n    print(f\"[Hyperopt] params={space} => F1={f1:.4f}\")\n    return {\"loss\": -f1, \"status\": STATUS_OK}\n\n\n# Define search space\nsearch_space = {\n    \"learning_rate\":   hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(5e-4)),\n    \"epochs\":          hp.choice(\"epochs\", [2, 3, 4]),\n    \"batch_size\":      hp.choice(\"batch_size\", [4, 8, 16]),\n    \"use_focal_loss\":  hp.choice(\"use_focal_loss\", [False, True]),\n    \"gamma\":           hp.quniform(\"gamma\", 1.0, 5.0, 0.5),    # relevant if focal_loss=True\n    \"label_smoothing\": hp.uniform(\"label_smoothing\", 0.0, 0.2)\n}\n\nmax_evals = 10  # For demonstration; increase for better search\ntrials = Trials()\n\nbest = fmin(\n    fn=objective,\n    space=search_space,\n    algo=tpe.suggest,\n    max_evals=max_evals,\n    trials=trials\n)\n\nprint(\"\\nHyperopt best param indices:\", best)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:33:44.018733Z","iopub.execute_input":"2025-04-02T15:33:44.018971Z","iopub.status.idle":"2025-04-02T20:01:51.244494Z","shell.execute_reply.started":"2025-04-02T15:33:44.018952Z","shell.execute_reply":"2025-04-02T20:01:51.243746Z"}},"outputs":[{"name":"stdout","text":"  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff1f94bd206848e7981fff7ff3f88c54"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18615/18615 36:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.626700</td>\n      <td>0.626217</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.625700</td>\n      <td>0.626089</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.624800</td>\n      <td>0.628621</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 4.5, 'label_smoothing': 0.09456020674407445, 'learning_rate': 4.650364054074947e-05, 'use_focal_loss': False} => F1=0.6071\n 10%|â–ˆ         | 1/10 [36:55<5:32:19, 2215.53s/trial, best loss: -0.607102296650199]","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3104/3104 15:07, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.338100</td>\n      <td>0.304784</td>\n      <td>0.877063</td>\n      <td>0.873102</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.188000</td>\n      <td>0.325829</td>\n      <td>0.892632</td>\n      <td>0.890651</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.5, 'label_smoothing': 0.00011108738704290744, 'learning_rate': 2.379886141068789e-05, 'use_focal_loss': False} => F1=0.8926\n 20%|â–ˆâ–ˆ        | 2/10 [52:39<3:15:41, 1467.73s/trial, best loss: -0.8926320009887566]","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6206/6206 18:23, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.389500</td>\n      <td>0.368823</td>\n      <td>0.878019</td>\n      <td>0.874620</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.281400</td>\n      <td>0.406623</td>\n      <td>0.889338</td>\n      <td>0.886939</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 1.5, 'label_smoothing': 0.025140874030732153, 'learning_rate': 1.257211962841882e-05, 'use_focal_loss': False} => F1=0.8893\n 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [1:11:40<2:33:47, 1318.27s/trial, best loss: -0.8926320009887566]","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18615/18615 36:26, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.038700</td>\n      <td>0.037970</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.038000</td>\n      <td>0.038010</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.037800</td>\n      <td>0.037949</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 4.0, 'label_smoothing': 0.14953429278875716, 'learning_rate': 0.0002599290684811281, 'use_focal_loss': True} => F1=0.6071\n 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [1:48:43<2:47:33, 1675.57s/trial, best loss: -0.8926320009887566]","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9309/9309 27:31, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.298400</td>\n      <td>0.297070</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.296300</td>\n      <td>0.296987</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.293300</td>\n      <td>0.275470</td>\n      <td>0.707435</td>\n      <td>0.738610</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 1.0, 'label_smoothing': 0.005405331684439041, 'learning_rate': 0.00010436157084074034, 'use_focal_loss': True} => F1=0.7074\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [2:16:51<2:20:00, 1680.15s/trial, best loss: -0.8926320009887566]","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6206/6206 18:21, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.297600</td>\n      <td>0.297114</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.296000</td>\n      <td>0.298429</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 1.0, 'label_smoothing': 0.042321244377780225, 'learning_rate': 8.501860704557743e-05, 'use_focal_loss': True} => F1=0.6071\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [2:35:50<1:39:43, 1495.95s/trial, best loss: -0.8926320009887566]","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9309/9309 27:30, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.052500</td>\n      <td>0.049547</td>\n      <td>0.851188</td>\n      <td>0.845764</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.032600</td>\n      <td>0.044236</td>\n      <td>0.877285</td>\n      <td>0.873945</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.018400</td>\n      <td>0.063105</td>\n      <td>0.882228</td>\n      <td>0.879683</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 3.0, 'label_smoothing': 0.06689669298932018, 'learning_rate': 2.681460711622723e-05, 'use_focal_loss': True} => F1=0.8822\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [3:03:57<1:17:55, 1558.44s/trial, best loss: -0.8926320009887566]","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6206/6206 18:18, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.054300</td>\n      <td>0.053571</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.053400</td>\n      <td>0.053597</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 3.5, 'label_smoothing': 0.09892829430991, 'learning_rate': 0.00026964269961960493, 'use_focal_loss': True} => F1=0.6071\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [3:22:52<47:27, 1423.77s/trial, best loss: -0.8926320009887566]  ","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18615/18615 36:25, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.108000</td>\n      <td>0.106574</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.106400</td>\n      <td>0.106598</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.106100</td>\n      <td>0.106527</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 2.5, 'label_smoothing': 0.11402312832557485, 'learning_rate': 0.0002704088096216985, 'use_focal_loss': True} => F1=0.6071\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [3:59:55<27:53, 1673.53s/trial, best loss: -0.8926320009887566]","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-7825857fcce0>:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9309/9309 27:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.422100</td>\n      <td>0.398398</td>\n      <td>0.864380</td>\n      <td>0.864664</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.288600</td>\n      <td>0.441032</td>\n      <td>0.865231</td>\n      <td>0.860952</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.193900</td>\n      <td>0.482011</td>\n      <td>0.875046</td>\n      <td>0.872427</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 2.0, 'label_smoothing': 0.022622711387814333, 'learning_rate': 5.689927283324451e-05, 'use_focal_loss': False} => F1=0.8750\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [4:28:07<00:00, 1608.72s/trial, best loss: -0.8926320009887566]\n\nHyperopt best param indices: {'batch_size': 2, 'epochs': 0, 'gamma': 4.5, 'label_smoothing': 0.00011108738704290744, 'learning_rate': 2.379886141068789e-05, 'use_focal_loss': 0}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n#  7.1 Interpret best param indices from Hyperopt\n# ----------------------------------------------------------------------------\n\nepochs_options = [2, 3, 4]\nbatch_options  = [4, 8, 16]\nuse_focal_options = [False, True]\n\nfinal_params = {\n    \"learning_rate\":    best[\"learning_rate\"],\n    \"epochs\":           epochs_options[best[\"epochs\"]],\n    \"batch_size\":       batch_options[ best[\"batch_size\"] ],\n    \"use_focal_loss\":   use_focal_options[ best[\"use_focal_loss\"] ],\n    \"gamma\":            best[\"gamma\"],\n    \"label_smoothing\":  best[\"label_smoothing\"]\n}\n\nprint(\"Interpreted best hyperparams:\\n\", final_params)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T22:09:10.009906Z","iopub.execute_input":"2025-04-02T22:09:10.010278Z","iopub.status.idle":"2025-04-02T22:09:10.016588Z","shell.execute_reply.started":"2025-04-02T22:09:10.010247Z","shell.execute_reply":"2025-04-02T22:09:10.015799Z"}},"outputs":[{"name":"stdout","text":"Interpreted best hyperparams:\n {'learning_rate': 2.379886141068789e-05, 'epochs': 2, 'batch_size': 16, 'use_focal_loss': False, 'gamma': 4.5, 'label_smoothing': 0.00011108738704290744}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n#  8. TRAIN A FINAL MODEL USING THE BEST HYPERPARAMS\n#     (Optionally, you could merge train + dev to get more data, \n#      if the shared task rules allow it.)\n# ----------------------------------------------------------------------------\n\nbest_model = CustomDebertaModel(\n    model_name=model_name,\n    num_labels=2,\n    use_focal_loss=final_params[\"use_focal_loss\"],\n    gamma=final_params[\"gamma\"],\n    label_smoothing=final_params[\"label_smoothing\"]\n)\nbest_model.to(device)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./final-sota-model\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    learning_rate=final_params[\"learning_rate\"],\n    num_train_epochs=final_params[\"epochs\"],\n    per_device_train_batch_size=final_params[\"batch_size\"],\n    per_device_eval_batch_size=8,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    save_total_limit=1,\n    report_to=\"none\",  # Turn off W&B or any other tracking\n    logging_steps=1\n)\n\ntrainer = Trainer(\n    model=best_model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"dev\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\nresults_dev = trainer.evaluate(encoded_dataset[\"dev\"])\nprint(\"Final Dev Results:\", results_dev)\n\n# Optional: classification report\npreds_output = trainer.predict(encoded_dataset[\"dev\"])\ndev_preds = np.argmax(preds_output.predictions, axis=1)\ndev_labels = preds_output.label_ids\nprint(\"\\nDetailed Classification Report (Dev):\")\nprint(classification_report(dev_labels, dev_preds, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T22:09:12.613986Z","iopub.execute_input":"2025-04-02T22:09:12.614271Z","iopub.status.idle":"2025-04-02T22:25:32.967972Z","shell.execute_reply.started":"2025-04-02T22:09:12.614249Z","shell.execute_reply":"2025-04-02T22:25:32.967039Z"}},"outputs":[{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-21-ab1f2220c315>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3104/3104 15:08, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.331800</td>\n      <td>0.314324</td>\n      <td>0.871924</td>\n      <td>0.867195</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.181800</td>\n      <td>0.346780</td>\n      <td>0.889496</td>\n      <td>0.887108</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Final Dev Results: {'eval_loss': 0.3467804491519928, 'eval_f1': 0.889495550249558, 'eval_accuracy': 0.8871076611542356, 'eval_runtime': 34.26, 'eval_samples_per_second': 172.972, 'eval_steps_per_second': 21.629, 'epoch': 2.0}\n\nDetailed Classification Report (Dev):\n              precision    recall  f1-score   support\n\n           0     0.9493    0.8915    0.9195      4286\n           1     0.7554    0.8756    0.8111      1640\n\n    accuracy                         0.8871      5926\n   macro avg     0.8524    0.8836    0.8653      5926\nweighted avg     0.8956    0.8871    0.8895      5926\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n#  9. INFERENCE ON TEST SET\n# ----------------------------------------------------------------------------\n\ntest_predictions = trainer.predict(encoded_dataset[\"test\"])\ntest_preds = np.argmax(test_predictions.predictions, axis=1)\n\n# Add predictions to the test_df\ntest_df[\"label\"] = test_preds\ntest_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:18:11.819568Z","iopub.execute_input":"2025-04-02T20:18:11.819913Z","iopub.status.idle":"2025-04-02T20:20:16.796849Z","shell.execute_reply.started":"2025-04-02T20:18:11.819882Z","shell.execute_reply":"2025-04-02T20:20:16.796163Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                          claim  \\\n0           We should introduce school vouchers   \n1            We should legalize insider trading   \n2  We should subsidize investigative journalism   \n3       We should further exploit nuclear power   \n4                         We should ban whaling   \n\n                                            evidence  label  \n0  Among the many educational reform efforts, suc...      0  \n1  The U.S. Securities and Exchange Commission wa...      0  \n2  The film won an Emmy Award (1980), George Polk...      0  \n3  a 2001 survey by the European Commission found...      1  \n4  The US and several other nations are whaling u...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>claim</th>\n      <th>evidence</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>We should introduce school vouchers</td>\n      <td>Among the many educational reform efforts, suc...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>We should legalize insider trading</td>\n      <td>The U.S. Securities and Exchange Commission wa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>We should subsidize investigative journalism</td>\n      <td>The film won an Emmy Award (1980), George Polk...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>We should further exploit nuclear power</td>\n      <td>a 2001 survey by the European Commission found...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>We should ban whaling</td>\n      <td>The US and several other nations are whaling u...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n#  9.1 SAVE PREDICTIONS\n# ----------------------------------------------------------------------------\n\nOUTPUT_PATH = \"test_predictions.csv\"\ntest_df.to_csv(OUTPUT_PATH, index=False)\nprint(f\"Test predictions saved to: {OUTPUT_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T20:20:16.797737Z","iopub.execute_input":"2025-04-02T20:20:16.798054Z","iopub.status.idle":"2025-04-02T20:20:16.922228Z","shell.execute_reply.started":"2025-04-02T20:20:16.798023Z","shell.execute_reply":"2025-04-02T20:20:16.921503Z"}},"outputs":[{"name":"stdout","text":"Test predictions saved to: test_predictions.csv\n","output_type":"stream"}],"execution_count":15}]}