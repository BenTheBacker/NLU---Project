{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3y5__yaUKyb1"
   },
   "source": [
    "The model card template makes use of Jinja, hence we need to install the necessary package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4350,
     "status": "ok",
     "timestamp": 1739734646291,
     "user": {
      "displayName": "rbbatista",
      "userId": "16466300301400709217"
     },
     "user_tz": 0
    },
    "id": "kjSmXdzczFoh",
    "outputId": "27891584-dbd5-4970-fcd4-a49e682827fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Jinja2 in c:\\users\\backe\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\backe\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from Jinja2) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install Jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Puto8-5ILO2s"
   },
   "source": [
    "Required import statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1091,
     "status": "ok",
     "timestamp": 1739734647380,
     "user": {
      "displayName": "rbbatista",
      "userId": "16466300301400709217"
     },
     "user_tz": 0
    },
    "id": "inUOAq0Yy_O5"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard, ModelCardData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX5nkStaLTRC"
   },
   "source": [
    "Before running the cell below, upload the model card template (`COMP34812_modelcard_template.md`) provided to you using the Colab file browser (on the left-hand side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739734647381,
     "user": {
      "displayName": "rbbatista",
      "userId": "16466300301400709217"
     },
     "user_tz": 0
    },
    "id": "pg4o6fuPbl5X",
    "outputId": "2a50a3a5-c07e-4777-bf8c-3e555af6518c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model card generated: C Model Card.md\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 1. Define ModelCardData\n",
    "# ----------------------------------------------------------------------------\n",
    "card_data = ModelCardData(\n",
    "    language='en',\n",
    "    license='cc-by-4.0',\n",
    "    tags=['text-classification'],\n",
    "    repo=\"https://github.com/BenTheBacker/NLU---Project\",\n",
    "    ignore_metadata_errors=True\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. Create Model Card from Template\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Optionally, add a new string to summarize best hyperparams:\n",
    "best_hyperparams_str = \"\"\"\n",
    "**Best Hyperparameters**:\n",
    "- learning_rate: 2.379886141068789e-05\n",
    "- epochs: 2\n",
    "- batch_size: 16\n",
    "- use_focal_loss: False\n",
    "- gamma: 4.5\n",
    "- label_smoothing: 0.00011108738704290744\n",
    "\"\"\"\n",
    "\n",
    "card = ModelCard.from_template(\n",
    "    card_data=card_data,\n",
    "    template_path='COMP34812_modelcard_template.md',\n",
    "    \n",
    "    # Model ID: incorporate the usernames & track\n",
    "    model_id='m81976bb-v36373bb-ED-TaskC',\n",
    "\n",
    "    # Provide a brief model summary\n",
    "    model_summary='''This classification model was built for \"Task C\", \n",
    "      where the system determines whether a given claim is supported by \n",
    "      a piece of evidence (0) or not (1).''',\n",
    "\n",
    "    # Describe your model architecture and training approach\n",
    "    model_description='''Our approach uses **microsoft/deberta-v3-base** as the base model, \n",
    "      fine-tuned on a dataset of claim-evidence pairs. We performed data augmentation \n",
    "      via synonym replacement, and used Hyperopt (TPE) to explore hyperparameters \n",
    "      (focal loss vs. label smoothing). This helps address potential data imbalance \n",
    "      and improves generalization.''',\n",
    "\n",
    "    developers='Ben Baker and Ben Barrow',\n",
    "    base_model_repo='https://huggingface.co/microsoft/deberta-v3-base',\n",
    "    base_model_paper='https://arxiv.org/abs/2111.09543',\n",
    "    model_type='Supervised',\n",
    "    model_architecture='DeBERTa-v3 Base, fine-tuned with optional focal loss.',\n",
    "    language='English',\n",
    "    base_model='microsoft/deberta-v3-base',\n",
    "\n",
    "    # Data references\n",
    "    training_data='This model was trained on approximately 24.8k claim-evidence pairs, plus augmented samples.',\n",
    "\n",
    "    hyperparameters=''' \n",
    "      - learning_rate: Hyperopt (log-uniform from 1e-5 to 5e-4)\n",
    "      - epochs: 2, 3, or 4\n",
    "      - batch_size: 4, 8, or 16\n",
    "      - focal_loss gamma: 1.0 to 5.0\n",
    "      - label_smoothing: 0.0 to 0.2\n",
    "    ''',\n",
    "\n",
    "    speeds_sizes_times=f'''\n",
    "      - Total training time across all Hyperopt trials: ~4h 28m 07s\n",
    "      - Single final run training time: ~15 minutes (2 epochs) on a Kaggle P100 GPU\n",
    "      - Model size: ~400MB (including DeBERTa weights)\n",
    "    ''',\n",
    "\n",
    "     testing_data='We used the official ED dev set (~6k samples) for evaluation.',\n",
    "    testing_metrics='''\n",
    "      - F1 (weighted)\n",
    "      - Accuracy\n",
    "      - Precision / Recall\n",
    "    ''',\n",
    "\n",
    "    # Incorporate final results and a short classification report summary\n",
    "    results=f'''\n",
    "\n",
    "**Final Model Results** (Dev Set):\n",
    "- **eval_loss**: 0.3468\n",
    "- **F1 (weighted)**: 0.8895\n",
    "- **Accuracy**: 0.8871\n",
    "\n",
    "**Epoch-by-Epoch Performance**:\n",
    "| Epoch | Training Loss | Validation Loss | F1      | Accuracy |\n",
    "|-------|--------------:|----------------:|--------:|---------:|\n",
    "|   1   | 0.331800      | 0.314324        | 0.871924| 0.867195 |\n",
    "|   2   | 0.181800      | 0.346780        | 0.889496| 0.887108 |\n",
    "\n",
    "**Classification Report**:\n",
    "- **Class 0** => precision=0.9493, recall=0.8915, f1=0.9195\n",
    "- **Class 1** => precision=0.7554, recall=0.8756, f1=0.8111\n",
    "- **Weighted Avg** => f1=0.8895, accuracy=0.8871\n",
    "\n",
    "{best_hyperparams_str}\n",
    "''',\n",
    "\n",
    "    hardware_requirements=''' \n",
    "      - GPU recommended (trained on a Kaggle P100)\n",
    "      - ~2GB storage for data & model artifacts\n",
    "      - ~16GB RAM\n",
    "    ''',\n",
    "\n",
    "    software='''\n",
    "      - Transformers 4.x\n",
    "      - PyTorch 1.x\n",
    "      - NLTK for synonym replacement\n",
    "      - Hyperopt for hyperparameter tuning\n",
    "    ''',\n",
    "\n",
    "    bias_risks_limitations='''Data augmentation may introduce synonyms that alter sentence context. \n",
    "      The model performance may degrade on domain-specific language \n",
    "      or out-of-vocabulary terms. Mitigation strategies may involve \n",
    "      domain adaptation or further data collection.''',\n",
    "\n",
    "    additional_information='''All hyperparameters were chosen via TPE (Tree-structured Parzen Estimator) \n",
    "      with a maximum of 30 evaluations.'''\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. Write Model Card to Markdown File\n",
    "# ----------------------------------------------------------------------------\n",
    "with open('C Model Card.md', 'w') as model_card_file:\n",
    "    model_card_file.write(card.content)\n",
    "\n",
    "print(\"Model card generated: C Model Card.md\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPzvps9Dhvz36rnNhpWjrbn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
