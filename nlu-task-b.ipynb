{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11144868,"sourceType":"datasetVersion","datasetId":6952367}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport random\nimport nltk\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\nfrom nltk.corpus import wordnet\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset, DatasetDict\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\nfrom transformers import Trainer, TrainingArguments\n\n# ----------------------------------------------------------------------------\n# Global Variables and Flags\n# ----------------------------------------------------------------------------\nTESTING_FLAG = True  # If True, print debug info\nDOWNLOAD_FLAG = True # If True, handle NLTK data\n\nNLTK_DATA_DIR = \"/kaggle/working/nltk_data\"\nTRAIN_PATH = \"/kaggle/input/nlu-ed-task/train.csv\"\nDEV_PATH   = \"/kaggle/input/nlu-ed-task/dev.csv\"\nTEST_PATH  = \"/kaggle/input/nlu-ed-task/train.csv\"  # Temporary for demonstration\nOUTPUT_PATH = \"test_predictions.csv\"\n\nAUGMENTED_COPY_CHANCE = 0.15\nEPOCH_OPTIONS = [2, 3, 4]\nBATCH_OPTIONS  = [4, 8, 16]\nUSE_FOCAL_OPTIONS = [False, True]\n\nSEARCH_SPACE = {\n    \"learning_rate\":   hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(5e-4)),\n    \"epochs\":          hp.choice(\"epochs\", EPOCH_OPTIONS),\n    \"batch_size\":      hp.choice(\"batch_size\", BATCH_OPTIONS),\n    \"use_focal_loss\":  hp.choice(\"use_focal_loss\", USE_FOCAL_OPTIONS),\n    \"gamma\":           hp.quniform(\"gamma\", 1.0, 5.0, 0.5),\n    \"label_smoothing\": hp.uniform(\"label_smoothing\", 0.0, 0.2)\n}\n\nMAX_EVALS = 30  \nEVAL_BATCH_SIZE = 8\nBEST_MODEL_METRIC = \"f1\"\n\nGLOVE_PATH = \"/kaggle/input/glove6b300d/glove.6B.300d.txt\"  # Adjust path if needed\nEMBED_DIM = 300         \nHIDDEN_DIM = 256\nNUM_LAYERS = 2\nDROPOUT = 0.3           \nUSE_ATTENTION = True    # If True, apply a simple attention layer\n\n# Check GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif TESTING_FLAG:\n    print(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:44:42.457314Z","iopub.execute_input":"2025-04-01T17:44:42.457578Z","iopub.status.idle":"2025-04-01T17:45:03.004922Z","shell.execute_reply.started":"2025-04-01T17:44:42.457550Z","shell.execute_reply":"2025-04-01T17:45:03.004134Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 1. Fetch NLTK data\n# ----------------------------------------------------------------------------\nif DOWNLOAD_FLAG:\n    nltk.data.path.append(NLTK_DATA_DIR)\n    nltk.download(\"wordnet\", download_dir=NLTK_DATA_DIR)\n    nltk.download(\"omw-1.4\", download_dir=NLTK_DATA_DIR)\n    !unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora/\n    !unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora/\n    print(\"Downloaded NLTK data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:45:03.005758Z","iopub.execute_input":"2025-04-01T17:45:03.006576Z","iopub.status.idle":"2025-04-01T18:37:20.034394Z","shell.execute_reply.started":"2025-04-01T17:45:03.006551Z","shell.execute_reply":"2025-04-01T18:37:20.033499Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\nArchive:  /kaggle/working/nltk_data/corpora/omw-1.4.zip\nreplace /kaggle/working/nltk_data/corpora/omw-1.4/fin/LICENSE? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\nArchive:  /kaggle/working/nltk_data/corpora/wordnet.zip\nreplace /kaggle/working/nltk_data/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\nDownloaded NLTK data\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 2. Load Data\n# ----------------------------------------------------------------------------\ntrain_df = pd.read_csv(TRAIN_PATH)\ndev_df   = pd.read_csv(DEV_PATH)\ntest_df  = pd.read_csv(TEST_PATH)\n\n# rename columns if needed\ntrain_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\ndev_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\ntest_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n\nif TESTING_FLAG:\n    print(\"Train samples:\", len(train_df))\n    print(\"Dev samples:\", len(dev_df))\n    print(\"Test samples:\", len(test_df))\n\ntrain_df[\"label\"] = train_df[\"label\"].astype(int)\ndev_df[\"label\"]   = dev_df[\"label\"].astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:37:20.035498Z","iopub.execute_input":"2025-04-01T18:37:20.035832Z","iopub.status.idle":"2025-04-01T18:37:20.336769Z","shell.execute_reply.started":"2025-04-01T18:37:20.035806Z","shell.execute_reply":"2025-04-01T18:37:20.335907Z"}},"outputs":[{"name":"stdout","text":"Train samples: 21508\nDev samples: 5926\nTest samples: 21508\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 3. Data Augmentation (Synonym Replacement)\n# ----------------------------------------------------------------------------\nrandom.seed(42)\n\ndef synonym_replacement(sentence, n=1):\n    words = sentence.split()\n    if len(words) < 2:\n        return sentence\n    indices_to_replace = random.sample(range(len(words)), k=min(n, len(words)))\n    new_words = words[:]\n    for i in indices_to_replace:\n        word = words[i]\n        syns = wordnet.synsets(word)\n        if not syns:\n            continue\n        lemmas = syns[0].lemma_names()\n        lemmas = [l for l in lemmas if l.lower() != word.lower()]\n        if len(lemmas) == 0:\n            continue\n        new_words[i] = random.choice(lemmas)\n    return \" \".join(new_words)\n\ndef augment_dataframe(df):\n    augmented_rows = []\n    for _, row in df.iterrows():\n        augmented_rows.append(row.to_dict())\n        if random.random() < AUGMENTED_COPY_CHANCE:\n            new_row = row.copy()\n            if random.random() < 0.5:\n                new_row[\"claim\"] = synonym_replacement(row[\"claim\"], n=1)\n            else:\n                new_row[\"evidence\"] = synonym_replacement(row[\"evidence\"], n=1)\n            augmented_rows.append(new_row.to_dict())\n    return pd.DataFrame(augmented_rows)\n\naugmented_train_df = augment_dataframe(train_df)\nif TESTING_FLAG:\n    print(\"Original train size:\", len(train_df),\n          \"=> After augmentation:\", len(augmented_train_df))\n\ntrain_df = augmented_train_df.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:37:20.338987Z","iopub.execute_input":"2025-04-01T18:37:20.339215Z","iopub.status.idle":"2025-04-01T18:37:23.600658Z","shell.execute_reply.started":"2025-04-01T18:37:20.339194Z","shell.execute_reply":"2025-04-01T18:37:23.599880Z"}},"outputs":[{"name":"stdout","text":"Original train size: 21508 => After augmentation: 24819\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 4. Create Hugging Face Datasets\n# ----------------------------------------------------------------------------\ntrain_dataset = Dataset.from_pandas(train_df)\ndev_dataset   = Dataset.from_pandas(dev_df)\ntest_dataset  = Dataset.from_pandas(test_df)\n\ndataset_dict = DatasetDict({\n    \"train\": train_dataset,\n    \"dev\":   dev_dataset,\n    \"test\":  test_dataset\n})\n\nif TESTING_FLAG:\n    print(dataset_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:37:23.602190Z","iopub.execute_input":"2025-04-01T18:37:23.602490Z","iopub.status.idle":"2025-04-01T18:37:23.723215Z","shell.execute_reply.started":"2025-04-01T18:37:23.602466Z","shell.execute_reply":"2025-04-01T18:37:23.722511Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['claim', 'evidence', 'label'],\n        num_rows: 24819\n    })\n    dev: Dataset({\n        features: ['claim', 'evidence', 'label'],\n        num_rows: 5926\n    })\n    test: Dataset({\n        features: ['claim', 'evidence', 'label'],\n        num_rows: 21508\n    })\n})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 5. Tokenization\n# ----------------------------------------------------------------------------\n# We'll use a BERT tokenizer just to produce consistent token IDs & attention mask\nTOKENIZER_NAME = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"claim\"],\n        examples[\"evidence\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128\n    )\n\nencoded_dataset = dataset_dict.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:37:23.724033Z","iopub.execute_input":"2025-04-01T18:37:23.724235Z","iopub.status.idle":"2025-04-01T18:37:33.660871Z","shell.execute_reply.started":"2025-04-01T18:37:23.724217Z","shell.execute_reply":"2025-04-01T18:37:33.659994Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e4ef4bb068940848d717a32872ec171"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5c150989a2a4fc696fc050374f9edab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"310d62abdd134a558b1116545ac876da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"292169cc2b9c4b908441d3c1c972501d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0149fe6451454e69852c63733af75bf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5926 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e398bf1a0824c3e8c2bdd3a05ed7ea0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7db5d5bd72c445fd8c5879a68d371c5e"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 5.1. Format for PyTorch\n# ----------------------------------------------------------------------------\nencoded_dataset[\"train\"] = encoded_dataset[\"train\"].rename_column(\"label\", \"labels\")\nencoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].rename_column(\"label\", \"labels\")\n\nencoded_dataset[\"train\"] = encoded_dataset[\"train\"].remove_columns([\"claim\", \"evidence\"])\nencoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].remove_columns([\"claim\", \"evidence\"])\nencoded_dataset[\"test\"]  = encoded_dataset[\"test\"].remove_columns([\"claim\", \"evidence\"])\n\nencoded_dataset[\"train\"].set_format(\"torch\")\nencoded_dataset[\"dev\"].set_format(\"torch\")\nencoded_dataset[\"test\"].set_format(\"torch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:37:33.661785Z","iopub.execute_input":"2025-04-01T18:37:33.662050Z","iopub.status.idle":"2025-04-01T18:37:33.674074Z","shell.execute_reply.started":"2025-04-01T18:37:33.662016Z","shell.execute_reply":"2025-04-01T18:37:33.673443Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 6. Load GloVe embeddings & Build an Embedding Matrix\n# ----------------------------------------------------------------------------\ndef load_glove_embeddings(glove_file, vocab, embedding_dim=300):\n    \"\"\"\n    Attempt to load GloVe 300d vectors and align them with the given vocab.\n    vocab: a dict {token_string: token_index}\n    Returns: a numpy array [vocab_size, embedding_dim]\n    \"\"\"\n    embedding_matrix = np.random.normal(\n        scale=0.1, \n        size=(len(vocab), embedding_dim)\n    ).astype(np.float32)\n    found = 0\n\n    if not os.path.isfile(glove_file):\n        print(f\"GloVe file not found at {glove_file}, using random init.\")\n        return embedding_matrix, found\n\n    print(f\"Loading GloVe from {glove_file}...\")\n    glove_dict = {}\n    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            values = line.split()\n            if len(values) != embedding_dim + 1:\n                continue\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype=\"float32\")\n            glove_dict[word] = coefs\n\n    # For each token in the BERT-based vocab, see if it matches a GloVe word\n    for token, idx in vocab.items():\n        # BERT tokens can have wordpiece prefixes like ##ing\n        # We'll do a simple check: if the token starts with \"##\", remove it\n        # Also try all-lowercase for matching.\n        normalized = token.replace(\"##\", \"\").lower()\n        if normalized in glove_dict:\n            embedding_matrix[idx] = glove_dict[normalized]\n            found += 1\n\n    print(f\"Initialized embedding_matrix with {found} GloVe tokens matched out of {len(vocab)}\")\n    return embedding_matrix, found\n\nvocab_dict = tokenizer.get_vocab()  # {token_str: token_id}\nembedding_matrix_np, glove_found = load_glove_embeddings(GLOVE_PATH, vocab_dict, EMBED_DIM)\nembedding_matrix_tensor = torch.tensor(embedding_matrix_np)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:37:33.674972Z","iopub.execute_input":"2025-04-01T18:37:33.675178Z","iopub.status.idle":"2025-04-01T18:37:34.016712Z","shell.execute_reply.started":"2025-04-01T18:37:33.675160Z","shell.execute_reply":"2025-04-01T18:37:34.015818Z"}},"outputs":[{"name":"stdout","text":"GloVe file not found at /kaggle/input/glove6b300d/glove.6B.300d.txt, using random init.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 7. Custom BiLSTM with optional attention\n# ----------------------------------------------------------------------------\nclass SimpleAttention(nn.Module):\n    \"\"\"\n    A simple additive attention: \n    score = tanh(W1*H + W2*h_context), \n    then softmax over time steps, \n    output = sum of weighted hidden states\n    \"\"\"\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        # Because LSTM is bidirectional, total hidden is 2 * hidden_dim\n        self.W = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n        self.v = nn.Linear(2 * hidden_dim, 1, bias=False)\n\n    def forward(self, lstm_outputs, mask=None):\n        \"\"\"\n        lstm_outputs: (B, L, 2*hidden_dim)\n        mask: (B, L) if needed (1 for real tokens, 0 for pad)\n        Returns: (B, 2*hidden_dim) - the weighted sum\n        \"\"\"\n        # Score calculation\n        score = torch.tanh(self.W(lstm_outputs))  # (B, L, 2H)\n        score = self.v(score).squeeze(-1)         # (B, L)\n        \n        # Optional mask\n        if mask is not None:\n            # mask=0 => shouldn't contribute, so set score to -inf\n            score = score.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(score, dim=-1)   # (B, L)\n        # Weighted sum\n        attn_weights = attn_weights.unsqueeze(1)  # (B, 1, L)\n        context = torch.bmm(attn_weights, lstm_outputs)  # (B, 1, 2H)\n        context = context.squeeze(1)              # (B, 2H)\n\n        return context\n\n\nclass CustomBiLSTMModel(nn.Module):\n    def __init__(self, \n                 vocab_size, \n                 embed_dim=300, \n                 hidden_dim=256, \n                 num_labels=2, \n                 num_layers=2,\n                 dropout=0.3,\n                 use_attention=True,\n                 use_focal_loss=False, \n                 gamma=2.0, \n                 label_smoothing=0.0,\n                 embedding_matrix=None):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.num_labels = num_labels\n        \n        self.use_attention = use_attention\n        self.use_focal_loss = use_focal_loss\n        self.gamma = gamma\n        self.label_smoothing = label_smoothing\n\n        # Embedding layer\n        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=0)\n        if embedding_matrix is not None:\n            with torch.no_grad():\n                self.embedding.weight.copy_(embedding_matrix)\n\n        # BiLSTM with multiple layers & dropout\n        self.lstm = nn.LSTM(\n            input_size=self.embed_dim,\n            hidden_size=self.hidden_dim,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n            batch_first=True,\n            bidirectional=True\n        )\n\n        # Optional attention\n        if self.use_attention:\n            self.attn = SimpleAttention(self.hidden_dim)\n\n        # Classification head\n        # If we have 2 directions => 2H\n        self.classifier = nn.Linear(2 * self.hidden_dim, self.num_labels)\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n        # Embeddings\n        embeds = self.embedding(input_ids)\n        # zero out padding\n        if attention_mask is not None:\n            expand_mask = attention_mask.unsqueeze(-1).float()\n            embeds = embeds * expand_mask\n        \n        # LSTM\n        lstm_outputs, (h, c) = self.lstm(embeds)\n        # shape of lstm_outputs: (B, L, 2H)\n\n        if self.use_attention:\n            # Weighted sum of outputs\n            context = self.attn(lstm_outputs, mask=attention_mask)\n        else:\n            # We'll just take final hidden states from both directions\n            # h shape: (num_layers*2, B, H)\n            h_forward = h[-2]  # last layer's forward state\n            h_backward = h[-1] # last layer's backward state\n            context = torch.cat((h_forward, h_backward), dim=-1)  # (B, 2H)\n\n        logits = self.classifier(context)\n\n        # Loss\n        loss = None\n        if labels is not None:\n            if self.use_focal_loss:\n                loss = self.focal_loss(logits, labels, self.gamma)\n            else:\n                loss = self.label_smoothing_loss(logits, labels, self.label_smoothing)\n\n        return {\"loss\": loss, \"logits\": logits}\n\n    def focal_loss(self, logits, targets, gamma=2.0):\n        ce = nn.CrossEntropyLoss(reduction='none')(logits, targets)\n        pt = torch.exp(-ce)\n        focal = (1 - pt)**gamma * ce\n        return focal.mean()\n\n    def label_smoothing_loss(self, logits, targets, smoothing=0.0):\n        if smoothing == 0.0:\n            return nn.CrossEntropyLoss()(logits, targets)\n        log_probs = F.log_softmax(logits, dim=-1)\n        n_class = logits.size(1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(smoothing / (n_class - 1))\n            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:37:34.017647Z","iopub.execute_input":"2025-04-01T18:37:34.017986Z","iopub.status.idle":"2025-04-01T18:37:34.030585Z","shell.execute_reply.started":"2025-04-01T18:37:34.017954Z","shell.execute_reply":"2025-04-01T18:37:34.029921Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 8. Build Trainer & Hyperopt\n# ----------------------------------------------------------------------------\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"f1\": f1, \"accuracy\": acc}\n\ndef make_trainer(model, train_ds, dev_ds, space):\n    learning_rate = space[\"learning_rate\"]\n    epochs        = int(space[\"epochs\"])\n    batch_size    = int(space[\"batch_size\"])\n\n    training_args = TrainingArguments(\n        output_dir=\"./enhanced-bilstm-ed-checkpoints\",\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        learning_rate=learning_rate,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n        load_best_model_at_end=True,\n        metric_for_best_model=BEST_MODEL_METRIC,\n        greater_is_better=True,\n        save_total_limit=1,\n        report_to=\"none\",\n        logging_steps=1\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=dev_ds,\n        processing_class=tokenizer,\n        compute_metrics=compute_metrics\n    )\n    return trainer\n\ndef objective(space):\n    use_focal_loss = space[\"use_focal_loss\"]\n    gamma = space[\"gamma\"]\n    label_smoothing = space[\"label_smoothing\"]\n\n    # Build model\n    model = CustomBiLSTMModel(\n        vocab_size=len(tokenizer.get_vocab()),\n        embed_dim=EMBED_DIM,\n        hidden_dim=HIDDEN_DIM,\n        num_labels=2,\n        num_layers=NUM_LAYERS,\n        dropout=DROPOUT,\n        use_attention=USE_ATTENTION,\n        use_focal_loss=use_focal_loss,\n        gamma=gamma,\n        label_smoothing=label_smoothing,\n        embedding_matrix=embedding_matrix_tensor\n    )\n    model.to(device)\n\n    trainer = make_trainer(model, encoded_dataset[\"train\"], encoded_dataset[\"dev\"], space)\n    trainer.train()\n    metrics = trainer.evaluate(encoded_dataset[\"dev\"])\n    f1 = metrics[\"eval_f1\"]\n\n    if TESTING_FLAG:\n        print(f\"[Hyperopt] params={space} => F1={f1:.4f}\")\n    return {\"loss\": -f1, \"status\": STATUS_OK}\n\ntrials = Trials()\nbest = fmin(\n    fn=objective,\n    space=SEARCH_SPACE,\n    algo=tpe.suggest,\n    max_evals=MAX_EVALS,\n    trials=trials\n)\n\nif TESTING_FLAG:\n    print(\"\\nHyperopt best param indices:\", best)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:37:34.031361Z","iopub.execute_input":"2025-04-01T18:37:34.031558Z","iopub.status.idle":"2025-04-01T21:12:52.343743Z","shell.execute_reply.started":"2025-04-01T18:37:34.031541Z","shell.execute_reply":"2025-04-01T21:12:52.343084Z"}},"outputs":[{"name":"stdout","text":"  0%|          | 0/30 [00:00<?, ?trial/s, best loss=?]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3104/3104 01:37, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.049400</td>\n      <td>0.044498</td>\n      <td>0.722082</td>\n      <td>0.763247</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.040900</td>\n      <td>0.042593</td>\n      <td>0.759018</td>\n      <td>0.776747</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 3.5, 'label_smoothing': 0.15599873886629706, 'learning_rate': 3.170526971699426e-05, 'use_focal_loss': True} => F1=0.7590\n  3%|▎         | 1/30 [01:46<51:21, 106.26s/trial, best loss: -0.7590180872514256]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6206/6206 02:29, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.105400</td>\n      <td>0.104673</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.101200</td>\n      <td>0.100414</td>\n      <td>0.607499</td>\n      <td>0.723422</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 2.5, 'label_smoothing': 0.028835345786337997, 'learning_rate': 1.0304624272436615e-05, 'use_focal_loss': True} => F1=0.6075\n  7%|▋         | 2/30 [04:22<1:03:22, 135.81s/trial, best loss: -0.7590180872514256]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:31, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.032500</td>\n      <td>0.029251</td>\n      <td>0.782434</td>\n      <td>0.789234</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.026200</td>\n      <td>0.028527</td>\n      <td>0.797402</td>\n      <td>0.802227</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.024300</td>\n      <td>0.034640</td>\n      <td>0.798839</td>\n      <td>0.803746</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.023600</td>\n      <td>0.038624</td>\n      <td>0.800356</td>\n      <td>0.804927</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 4.0, 'label_smoothing': 0.048510621605982275, 'learning_rate': 2.90498654240857e-05, 'use_focal_loss': True} => F1=0.8004\n 10%|█         | 3/30 [13:00<2:19:40, 310.41s/trial, best loss: -0.800355651832787] ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6206/6206 02:35, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.488600</td>\n      <td>0.454297</td>\n      <td>0.800204</td>\n      <td>0.801384</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.389900</td>\n      <td>0.475443</td>\n      <td>0.797960</td>\n      <td>0.804421</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 2.0, 'label_smoothing': 0.021342332085049834, 'learning_rate': 0.0001245387711327438, 'use_focal_loss': False} => F1=0.8002\n 13%|█▎        | 4/30 [15:43<1:49:13, 252.06s/trial, best loss: -0.800355651832787]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3104/3104 01:40, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.544100</td>\n      <td>0.503527</td>\n      <td>0.753410</td>\n      <td>0.774722</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.453000</td>\n      <td>0.474724</td>\n      <td>0.776240</td>\n      <td>0.782990</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 1.5, 'label_smoothing': 0.007807440513335706, 'learning_rate': 4.11167894582814e-05, 'use_focal_loss': False} => F1=0.7762\n 17%|█▋        | 5/30 [17:31<1:23:26, 200.25s/trial, best loss: -0.800355651832787]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.561700</td>\n      <td>0.560445</td>\n      <td>0.793685</td>\n      <td>0.797165</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.498400</td>\n      <td>0.558794</td>\n      <td>0.801718</td>\n      <td>0.803409</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.456300</td>\n      <td>0.570823</td>\n      <td>0.798446</td>\n      <td>0.803409</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.425300</td>\n      <td>0.595695</td>\n      <td>0.797752</td>\n      <td>0.801721</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 1.5, 'label_smoothing': 0.10998744379026944, 'learning_rate': 0.0002398018217274467, 'use_focal_loss': False} => F1=0.8017\n 20%|██        | 6/30 [26:20<2:04:46, 311.93s/trial, best loss: -0.8017180465085869]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3104/3104 01:37, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.037700</td>\n      <td>0.037586</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.037100</td>\n      <td>0.037238</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.0, 'label_smoothing': 0.07094317542816368, 'learning_rate': 1.0230798732370338e-05, 'use_focal_loss': True} => F1=0.6071\n 23%|██▎       | 7/30 [28:05<1:33:36, 244.20s/trial, best loss: -0.8017180465085869]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6206/6206 02:28, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.095800</td>\n      <td>0.085287</td>\n      <td>0.754181</td>\n      <td>0.775228</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.079400</td>\n      <td>0.085415</td>\n      <td>0.772823</td>\n      <td>0.785015</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 2.5, 'label_smoothing': 0.022277476101585748, 'learning_rate': 2.70583025787652e-05, 'use_focal_loss': True} => F1=0.7728\n 27%|██▋       | 8/30 [30:40<1:19:10, 215.92s/trial, best loss: -0.8017180465085869]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9309/9309 03:49, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.618600</td>\n      <td>0.591460</td>\n      <td>0.770560</td>\n      <td>0.786196</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.576300</td>\n      <td>0.584999</td>\n      <td>0.784611</td>\n      <td>0.795815</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.565500</td>\n      <td>0.586227</td>\n      <td>0.785324</td>\n      <td>0.793959</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 4.5, 'label_smoothing': 0.15631430529930673, 'learning_rate': 3.098579348728949e-05, 'use_focal_loss': False} => F1=0.7853\n 30%|███       | 9/30 [34:37<1:17:52, 222.49s/trial, best loss: -0.8017180465085869]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4656' max='4656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4656/4656 02:27, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.641300</td>\n      <td>0.627767</td>\n      <td>0.607499</td>\n      <td>0.723422</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.603100</td>\n      <td>0.599908</td>\n      <td>0.754691</td>\n      <td>0.774215</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.586400</td>\n      <td>0.596667</td>\n      <td>0.766883</td>\n      <td>0.781303</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 3, 'gamma': 3.0, 'label_smoothing': 0.1584789861040354, 'learning_rate': 2.1212588253369e-05, 'use_focal_loss': False} => F1=0.7669\n 33%|███▎      | 10/30 [37:12<1:07:11, 201.57s/trial, best loss: -0.8017180465085869]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3104/3104 01:38, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.029600</td>\n      <td>0.027402</td>\n      <td>0.803647</td>\n      <td>0.803409</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.020200</td>\n      <td>0.029042</td>\n      <td>0.804674</td>\n      <td>0.810496</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.0, 'label_smoothing': 0.18891778650060695, 'learning_rate': 0.0002940519922101875, 'use_focal_loss': True} => F1=0.8047\n 37%|███▋      | 11/30 [38:58<54:31, 172.21s/trial, best loss: -0.80467388224386]    ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3104/3104 01:41, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.608400</td>\n      <td>0.581426</td>\n      <td>0.782261</td>\n      <td>0.794128</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.564300</td>\n      <td>0.578800</td>\n      <td>0.787592</td>\n      <td>0.798178</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.0, 'label_smoothing': 0.1492101758865726, 'learning_rate': 5.874867261779884e-05, 'use_focal_loss': False} => F1=0.7876\n 40%|████      | 12/30 [40:46<45:52, 152.89s/trial, best loss: -0.80467388224386]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4656' max='4656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4656/4656 02:31, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.586500</td>\n      <td>0.547985</td>\n      <td>0.783525</td>\n      <td>0.793790</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.525200</td>\n      <td>0.540888</td>\n      <td>0.790020</td>\n      <td>0.799865</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.507400</td>\n      <td>0.543335</td>\n      <td>0.793697</td>\n      <td>0.800877</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 3, 'gamma': 5.0, 'label_smoothing': 0.1039113173167458, 'learning_rate': 5.136767899570798e-05, 'use_focal_loss': False} => F1=0.7937\n 43%|████▎     | 13/30 [43:25<43:50, 154.76s/trial, best loss: -0.80467388224386]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:42, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.199600</td>\n      <td>0.188551</td>\n      <td>0.776113</td>\n      <td>0.781471</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.168800</td>\n      <td>0.169152</td>\n      <td>0.791876</td>\n      <td>0.795478</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.157900</td>\n      <td>0.185759</td>\n      <td>0.794793</td>\n      <td>0.797503</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.152800</td>\n      <td>0.207669</td>\n      <td>0.796377</td>\n      <td>0.802059</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 1.5, 'label_smoothing': 0.14049237387885763, 'learning_rate': 2.6546375788382142e-05, 'use_focal_loss': True} => F1=0.7964\n 47%|████▋     | 14/30 [52:15<1:11:28, 268.05s/trial, best loss: -0.80467388224386]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6206' max='6206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6206/6206 02:34, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.588400</td>\n      <td>0.575557</td>\n      <td>0.799904</td>\n      <td>0.801046</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.537700</td>\n      <td>0.582531</td>\n      <td>0.797158</td>\n      <td>0.802059</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 2, 'gamma': 2.5, 'label_smoothing': 0.1501581473984195, 'learning_rate': 0.0001694997765515589, 'use_focal_loss': False} => F1=0.7999\n 50%|█████     | 15/30 [54:57<59:01, 236.12s/trial, best loss: -0.80467388224386]  ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3104/3104 01:41, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.164300</td>\n      <td>0.151979</td>\n      <td>0.800987</td>\n      <td>0.806446</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.114900</td>\n      <td>0.159792</td>\n      <td>0.797238</td>\n      <td>0.804421</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 1.5, 'label_smoothing': 0.024914529251201568, 'learning_rate': 0.0004197092558764367, 'use_focal_loss': True} => F1=0.8010\n 53%|█████▎    | 16/30 [56:46<46:08, 197.72s/trial, best loss: -0.80467388224386]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12410' max='12410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12410/12410 04:20, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.560300</td>\n      <td>0.545129</td>\n      <td>0.791216</td>\n      <td>0.797165</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.507600</td>\n      <td>0.560702</td>\n      <td>0.794191</td>\n      <td>0.802734</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 2, 'gamma': 4.0, 'label_smoothing': 0.09701321232039915, 'learning_rate': 8.451230032623634e-05, 'use_focal_loss': False} => F1=0.7942\n 57%|█████▋    | 17/30 [1:01:13<47:23, 218.70s/trial, best loss: -0.80467388224386]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9309' max='9309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9309/9309 03:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.118600</td>\n      <td>0.113702</td>\n      <td>0.795459</td>\n      <td>0.793959</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.088700</td>\n      <td>0.111443</td>\n      <td>0.795968</td>\n      <td>0.795815</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.073200</td>\n      <td>0.134395</td>\n      <td>0.797786</td>\n      <td>0.801552</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 8, 'epochs': 3, 'gamma': 2.0, 'label_smoothing': 0.056643147754697504, 'learning_rate': 0.00011831588529408005, 'use_focal_loss': True} => F1=0.7978\n 60%|██████    | 18/30 [1:05:12<44:57, 224.77s/trial, best loss: -0.80467388224386]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3104/3104 01:39, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.033200</td>\n      <td>0.029981</td>\n      <td>0.768531</td>\n      <td>0.784340</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.027000</td>\n      <td>0.029444</td>\n      <td>0.779130</td>\n      <td>0.790078</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.0, 'label_smoothing': 0.015568230030244434, 'learning_rate': 5.197559784390158e-05, 'use_focal_loss': True} => F1=0.7791\n 63%|██████▎   | 19/30 [1:06:59<34:42, 189.33s/trial, best loss: -0.80467388224386]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3104/3104 01:38, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.026600</td>\n      <td>0.026568</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.026100</td>\n      <td>0.026161</td>\n      <td>0.607102</td>\n      <td>0.723253</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.5, 'label_smoothing': 0.13635106686204593, 'learning_rate': 1.1577999041738486e-05, 'use_focal_loss': True} => F1=0.6071\n 67%|██████▋   | 20/30 [1:08:45<27:22, 164.21s/trial, best loss: -0.80467388224386]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:29, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.614200</td>\n      <td>0.605654</td>\n      <td>0.800933</td>\n      <td>0.808134</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.571400</td>\n      <td>0.609624</td>\n      <td>0.804129</td>\n      <td>0.808302</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.541400</td>\n      <td>0.620283</td>\n      <td>0.797789</td>\n      <td>0.802396</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.522600</td>\n      <td>0.636327</td>\n      <td>0.792747</td>\n      <td>0.796153</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 3.0, 'label_smoothing': 0.19991770119774901, 'learning_rate': 0.00047023484216293963, 'use_focal_loss': False} => F1=0.8041\n 70%|███████   | 21/30 [1:17:22<40:31, 270.13s/trial, best loss: -0.80467388224386]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:33, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.613900</td>\n      <td>0.602272</td>\n      <td>0.798172</td>\n      <td>0.809821</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.572600</td>\n      <td>0.606532</td>\n      <td>0.806976</td>\n      <td>0.811002</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.543700</td>\n      <td>0.617318</td>\n      <td>0.799700</td>\n      <td>0.805265</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.522900</td>\n      <td>0.633303</td>\n      <td>0.792945</td>\n      <td>0.795309</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 3.0, 'label_smoothing': 0.1970405195193367, 'learning_rate': 0.0004955511800197659, 'use_focal_loss': False} => F1=0.8070\n 73%|███████▎  | 22/30 [1:26:02<46:02, 345.33s/trial, best loss: -0.8069762909611161]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:34, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.615200</td>\n      <td>0.607267</td>\n      <td>0.794578</td>\n      <td>0.804927</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.577100</td>\n      <td>0.608177</td>\n      <td>0.804170</td>\n      <td>0.807796</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.550700</td>\n      <td>0.617685</td>\n      <td>0.799634</td>\n      <td>0.807796</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.532800</td>\n      <td>0.628420</td>\n      <td>0.796925</td>\n      <td>0.801046</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 3.5, 'label_smoothing': 0.19841756680684028, 'learning_rate': 0.00028938735323599964, 'use_focal_loss': False} => F1=0.8042\n 77%|███████▋  | 23/30 [1:34:44<46:28, 398.31s/trial, best loss: -0.8069762909611161]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:24, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.040900</td>\n      <td>0.038743</td>\n      <td>0.803888</td>\n      <td>0.807627</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.027900</td>\n      <td>0.040352</td>\n      <td>0.803754</td>\n      <td>0.807627</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.018100</td>\n      <td>0.058803</td>\n      <td>0.795166</td>\n      <td>0.797334</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.009900</td>\n      <td>0.098109</td>\n      <td>0.790256</td>\n      <td>0.793115</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 3.5, 'label_smoothing': 0.18188580660630393, 'learning_rate': 0.00032756957638555706, 'use_focal_loss': True} => F1=0.8039\n 80%|████████  | 24/30 [1:43:16<43:13, 432.22s/trial, best loss: -0.8069762909611161]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:24, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.014900</td>\n      <td>0.013771</td>\n      <td>0.803215</td>\n      <td>0.807121</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.010500</td>\n      <td>0.013932</td>\n      <td>0.809048</td>\n      <td>0.811509</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.007700</td>\n      <td>0.018103</td>\n      <td>0.802930</td>\n      <td>0.804590</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.005400</td>\n      <td>0.026158</td>\n      <td>0.799684</td>\n      <td>0.801721</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 5.0, 'label_smoothing': 0.17681928136354222, 'learning_rate': 0.00018668565591363687, 'use_focal_loss': True} => F1=0.8090\n 83%|████████▎ | 25/30 [1:51:47<38:00, 456.07s/trial, best loss: -0.8090475209245707]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:42, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.606700</td>\n      <td>0.599574</td>\n      <td>0.796695</td>\n      <td>0.799359</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.566600</td>\n      <td>0.600935</td>\n      <td>0.796328</td>\n      <td>0.803746</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.540400</td>\n      <td>0.610150</td>\n      <td>0.799745</td>\n      <td>0.803409</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.522900</td>\n      <td>0.621269</td>\n      <td>0.798700</td>\n      <td>0.802227</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 5.0, 'label_smoothing': 0.18133233736063667, 'learning_rate': 0.0001880236236852539, 'use_focal_loss': False} => F1=0.7997\n 87%|████████▋ | 26/30 [2:00:38<31:53, 478.34s/trial, best loss: -0.8090475209245707]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:34, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.014700</td>\n      <td>0.014372</td>\n      <td>0.802764</td>\n      <td>0.810496</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.010000</td>\n      <td>0.014579</td>\n      <td>0.801859</td>\n      <td>0.806784</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.006000</td>\n      <td>0.020717</td>\n      <td>0.794356</td>\n      <td>0.795984</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.002600</td>\n      <td>0.044044</td>\n      <td>0.790644</td>\n      <td>0.792946</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 5.0, 'label_smoothing': 0.12037570496993175, 'learning_rate': 0.0004927922027484601, 'use_focal_loss': True} => F1=0.8028\n 90%|█████████ | 27/30 [2:09:20<24:34, 491.58s/trial, best loss: -0.8090475209245707]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:53, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.604400</td>\n      <td>0.596161</td>\n      <td>0.790733</td>\n      <td>0.795478</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.566800</td>\n      <td>0.596527</td>\n      <td>0.799646</td>\n      <td>0.806446</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.546900</td>\n      <td>0.609937</td>\n      <td>0.790557</td>\n      <td>0.794296</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.532300</td>\n      <td>0.619751</td>\n      <td>0.791326</td>\n      <td>0.796153</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:07]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 1.0, 'label_smoothing': 0.17277028367989872, 'learning_rate': 9.945677193172808e-05, 'use_focal_loss': False} => F1=0.7996\n 93%|█████████▎| 28/30 [2:18:22<16:53, 506.61s/trial, best loss: -0.8090475209245707]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.020900</td>\n      <td>0.019351</td>\n      <td>0.804071</td>\n      <td>0.807290</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.014700</td>\n      <td>0.019644</td>\n      <td>0.807885</td>\n      <td>0.810159</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.010700</td>\n      <td>0.025832</td>\n      <td>0.801661</td>\n      <td>0.803071</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.007300</td>\n      <td>0.037228</td>\n      <td>0.799253</td>\n      <td>0.801046</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 4.5, 'label_smoothing': 0.1671975277624146, 'learning_rate': 0.0001932750781421523, 'use_focal_loss': True} => F1=0.8079\n 97%|█████████▋| 29/30 [2:26:51<08:27, 507.39s/trial, best loss: -0.8090475209245707]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:19, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.020900</td>\n      <td>0.019298</td>\n      <td>0.802784</td>\n      <td>0.807459</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.014800</td>\n      <td>0.019315</td>\n      <td>0.807871</td>\n      <td>0.810327</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.011000</td>\n      <td>0.024953</td>\n      <td>0.803230</td>\n      <td>0.804927</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.007900</td>\n      <td>0.035246</td>\n      <td>0.802317</td>\n      <td>0.804084</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [741/741 00:06]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[Hyperopt] params={'batch_size': 4, 'epochs': 4, 'gamma': 4.5, 'label_smoothing': 0.17123828114873232, 'learning_rate': 0.00016460269780220149, 'use_focal_loss': True} => F1=0.8079\n100%|██████████| 30/30 [2:35:18<00:00, 310.61s/trial, best loss: -0.8090475209245707]\n\nHyperopt best param indices: {'batch_size': 0, 'epochs': 2, 'gamma': 5.0, 'label_smoothing': 0.17681928136354222, 'learning_rate': 0.00018668565591363687, 'use_focal_loss': 1}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 8.1 Interpret Best Hyperparams\n# ----------------------------------------------------------------------------\nEPOCH_OPTIONS = [2, 3, 4]\nBATCH_OPTIONS = [4, 8, 16]\nUSE_FOCAL_OPTIONS = [False, True]\n\nfinal_params = {\n    \"learning_rate\":    best[\"learning_rate\"],\n    \"epochs\":           EPOCH_OPTIONS[ best[\"epochs\"] ],\n    \"batch_size\":       BATCH_OPTIONS[ best[\"batch_size\"] ],\n    \"use_focal_loss\":   USE_FOCAL_OPTIONS[ best[\"use_focal_loss\"] ],\n    \"gamma\":            best[\"gamma\"],\n    \"label_smoothing\":  best[\"label_smoothing\"]\n}\n\nif TESTING_FLAG:\n    print(\"Interpreted best hyperparams:\\n\", final_params)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T21:12:52.344498Z","iopub.execute_input":"2025-04-01T21:12:52.344772Z","iopub.status.idle":"2025-04-01T21:12:52.349967Z","shell.execute_reply.started":"2025-04-01T21:12:52.344747Z","shell.execute_reply":"2025-04-01T21:12:52.349331Z"}},"outputs":[{"name":"stdout","text":"Interpreted best hyperparams:\n {'learning_rate': 0.00018668565591363687, 'epochs': 4, 'batch_size': 4, 'use_focal_loss': True, 'gamma': 5.0, 'label_smoothing': 0.17681928136354222}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 9. Train Final Model\n# ----------------------------------------------------------------------------\nbest_model = CustomBiLSTMModel(\n    vocab_size=len(tokenizer.get_vocab()),\n    embed_dim=EMBED_DIM,\n    hidden_dim=HIDDEN_DIM,\n    num_layers=NUM_LAYERS,\n    dropout=DROPOUT,\n    use_attention=USE_ATTENTION,\n    num_labels=2,\n    use_focal_loss=final_params[\"use_focal_loss\"],\n    gamma=final_params[\"gamma\"],\n    label_smoothing=final_params[\"label_smoothing\"],\n    embedding_matrix=embedding_matrix_tensor\n)\nbest_model.to(device)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./final-enhanced-bilstm-model\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    learning_rate=final_params[\"learning_rate\"],\n    num_train_epochs=final_params[\"epochs\"],\n    per_device_train_batch_size=final_params[\"batch_size\"],\n    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n    load_best_model_at_end=True,\n    metric_for_best_model=BEST_MODEL_METRIC,\n    greater_is_better=True,\n    save_total_limit=1,\n    report_to=\"none\",\n    logging_steps=1\n)\n\ntrainer = Trainer(\n    model=best_model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"dev\"],\n    processing_class=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\nresults_dev = trainer.evaluate(encoded_dataset[\"dev\"])\nif TESTING_FLAG:\n    print(\"Final Dev Results:\", results_dev)\n\npreds_output = trainer.predict(encoded_dataset[\"dev\"])\ndev_preds = np.argmax(preds_output.predictions, axis=1)\ndev_labels = preds_output.label_ids\nif TESTING_FLAG:\n    print(\"\\nDetailed Classification Report (Dev):\")\n    print(classification_report(dev_labels, dev_preds, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T21:12:52.350765Z","iopub.execute_input":"2025-04-01T21:12:52.351043Z","iopub.status.idle":"2025-04-01T21:21:27.556406Z","shell.execute_reply.started":"2025-04-01T21:12:52.351016Z","shell.execute_reply":"2025-04-01T21:21:27.555691Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24820' max='24820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24820/24820 08:20, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.014900</td>\n      <td>0.013771</td>\n      <td>0.803215</td>\n      <td>0.807121</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.010500</td>\n      <td>0.013932</td>\n      <td>0.809048</td>\n      <td>0.811509</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.007700</td>\n      <td>0.018103</td>\n      <td>0.802930</td>\n      <td>0.804590</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.005400</td>\n      <td>0.026158</td>\n      <td>0.799684</td>\n      <td>0.801721</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Final Dev Results: {'eval_loss': 0.013931580819189548, 'eval_f1': 0.8090475209245707, 'eval_accuracy': 0.8115086061424233, 'eval_runtime': 6.5113, 'eval_samples_per_second': 910.116, 'eval_steps_per_second': 113.803, 'epoch': 4.0}\n\nDetailed Classification Report (Dev):\n              precision    recall  f1-score   support\n\n           0     0.8589    0.8847    0.8716      4286\n           1     0.6731    0.6201    0.6455      1640\n\n    accuracy                         0.8115      5926\n   macro avg     0.7660    0.7524    0.7586      5926\nweighted avg     0.8075    0.8115    0.8090      5926\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 10. Inference on Test Set\n# ----------------------------------------------------------------------------\ntest_predictions = trainer.predict(encoded_dataset[\"test\"])\ntest_preds = np.argmax(test_predictions.predictions, axis=1)\ntest_df[\"label\"] = test_preds\ntest_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T21:21:27.558526Z","iopub.execute_input":"2025-04-01T21:21:27.558768Z","iopub.status.idle":"2025-04-01T21:21:51.176503Z","shell.execute_reply.started":"2025-04-01T21:21:27.558748Z","shell.execute_reply":"2025-04-01T21:21:51.175676Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                          claim  \\\n0           We should introduce school vouchers   \n1            We should legalize insider trading   \n2  We should subsidize investigative journalism   \n3       We should further exploit nuclear power   \n4                         We should ban whaling   \n\n                                            evidence  label  \n0  Among the many educational reform efforts, suc...      0  \n1  The U.S. Securities and Exchange Commission wa...      0  \n2  The film won an Emmy Award (1980), George Polk...      0  \n3  a 2001 survey by the European Commission found...      1  \n4  The US and several other nations are whaling u...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>claim</th>\n      <th>evidence</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>We should introduce school vouchers</td>\n      <td>Among the many educational reform efforts, suc...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>We should legalize insider trading</td>\n      <td>The U.S. Securities and Exchange Commission wa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>We should subsidize investigative journalism</td>\n      <td>The film won an Emmy Award (1980), George Polk...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>We should further exploit nuclear power</td>\n      <td>a 2001 survey by the European Commission found...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>We should ban whaling</td>\n      <td>The US and several other nations are whaling u...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 10.1 Save Predictions\n# ----------------------------------------------------------------------------\ntest_df.to_csv(OUTPUT_PATH, index=False)\nif TESTING_FLAG:\n    print(f\"Test predictions saved to: {OUTPUT_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T21:21:51.177622Z","iopub.execute_input":"2025-04-01T21:21:51.177935Z","iopub.status.idle":"2025-04-01T21:21:51.294278Z","shell.execute_reply.started":"2025-04-01T21:21:51.177901Z","shell.execute_reply":"2025-04-01T21:21:51.293642Z"}},"outputs":[{"name":"stdout","text":"Test predictions saved to: test_predictions.csv\n","output_type":"stream"}],"execution_count":14}]}