{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:44:42.457578Z",
     "iopub.status.busy": "2025-04-01T17:44:42.457314Z",
     "iopub.status.idle": "2025-04-01T17:45:03.004922Z",
     "shell.execute_reply": "2025-04-01T17:45:03.004134Z",
     "shell.execute_reply.started": "2025-04-01T17:44:42.457550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Global Variables and Flags\n",
    "# ----------------------------------------------------------------------------\n",
    "TESTING_FLAG = True  # If True, print debug info\n",
    "DOWNLOAD_FLAG = True # If True, handle NLTK data\n",
    "\n",
    "NLTK_DATA_DIR = \"data\\\\nltk_data\"\n",
    "TRAIN_PATH = \"data\\\\train.csv\"\n",
    "DEV_PATH   = \"data\\\\dev.csv\"\n",
    "TEST_PATH  = \"data\\\\test.csv\" \n",
    "\n",
    "BEST_MODEL_PATH = \"data\\\\ED_B_Model.pt\"\n",
    "OUTPUT_PATH = \"data\\\\predictions.csv\"\n",
    "\n",
    "AUGMENTED_COPY_CHANCE = 0.15\n",
    "EPOCH_OPTIONS = [2, 3, 4]\n",
    "BATCH_OPTIONS  = [4, 8, 16]\n",
    "USE_FOCAL_OPTIONS = [False, True]\n",
    "\n",
    "SEARCH_SPACE = {\n",
    "    \"learning_rate\":   hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(5e-4)),\n",
    "    \"epochs\":          hp.choice(\"epochs\", EPOCH_OPTIONS),\n",
    "    \"batch_size\":      hp.choice(\"batch_size\", BATCH_OPTIONS),\n",
    "    \"use_focal_loss\":  hp.choice(\"use_focal_loss\", USE_FOCAL_OPTIONS),\n",
    "    \"gamma\":           hp.quniform(\"gamma\", 1.0, 5.0, 0.5),\n",
    "    \"label_smoothing\": hp.uniform(\"label_smoothing\", 0.0, 0.2)\n",
    "}\n",
    "\n",
    "MAX_EVALS = 10  \n",
    "EVAL_BATCH_SIZE = 8\n",
    "BEST_MODEL_METRIC = \"f1\"\n",
    "\n",
    "GLOVE_PATH = \"data\\\\glove.6B.300d.txt\"  \n",
    "EMBED_DIM = 300         \n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3           \n",
    "USE_ATTENTION = True  \n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if TESTING_FLAG:\n",
    "    print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:45:03.006576Z",
     "iopub.status.busy": "2025-04-01T17:45:03.005758Z",
     "iopub.status.idle": "2025-04-01T18:37:20.034394Z",
     "shell.execute_reply": "2025-04-01T18:37:20.033499Z",
     "shell.execute_reply.started": "2025-04-01T17:45:03.006551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded NLTK data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Backe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Backe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 1. Fetch NLTK data\n",
    "# ----------------------------------------------------------------------------\n",
    "if DOWNLOAD_FLAG:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "\n",
    "    #nltk.data.path.append(NLTK_DATA_DIR)\n",
    "    #nltk.download(\"wordnet\", download_dir=NLTK_DATA_DIR)\n",
    "    #nltk.download(\"omw-1.4\", download_dir=NLTK_DATA_DIR)\n",
    "    #!unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora/\n",
    "    #!unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora/\n",
    "    print(\"Downloaded NLTK data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:20.035832Z",
     "iopub.status.busy": "2025-04-01T18:37:20.035498Z",
     "iopub.status.idle": "2025-04-01T18:37:20.336769Z",
     "shell.execute_reply": "2025-04-01T18:37:20.335907Z",
     "shell.execute_reply.started": "2025-04-01T18:37:20.035806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 21508\n",
      "Dev samples: 5926\n",
      "Test samples: 4688\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 2. Load Data\n",
    "# ----------------------------------------------------------------------------\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "dev_df   = pd.read_csv(DEV_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "train_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "dev_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "test_df.rename(columns={\"Claim\": \"claim\", \"Evidence\": \"evidence\"}, inplace=True)\n",
    "\n",
    "if TESTING_FLAG:\n",
    "    print(\"Train samples:\", len(train_df))\n",
    "    print(\"Dev samples:\", len(dev_df))\n",
    "    print(\"Test samples:\", len(test_df))\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(int)\n",
    "dev_df[\"label\"]   = dev_df[\"label\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:20.339215Z",
     "iopub.status.busy": "2025-04-01T18:37:20.338987Z",
     "iopub.status.idle": "2025-04-01T18:37:23.600658Z",
     "shell.execute_reply": "2025-04-01T18:37:23.599880Z",
     "shell.execute_reply.started": "2025-04-01T18:37:20.339194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: 21508 => After augmentation: 24819\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 3. Data Augmentation (Synonym Replacement)\n",
    "# ----------------------------------------------------------------------------\n",
    "random.seed(42)\n",
    "\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    if len(words) < 2:\n",
    "        return sentence\n",
    "    indices_to_replace = random.sample(range(len(words)), k=min(n, len(words)))\n",
    "    new_words = words[:]\n",
    "    for i in indices_to_replace:\n",
    "        word = words[i]\n",
    "        syns = wordnet.synsets(word)\n",
    "        if not syns:\n",
    "            continue\n",
    "        lemmas = syns[0].lemma_names()\n",
    "        lemmas = [l for l in lemmas if l.lower() != word.lower()]\n",
    "        if len(lemmas) == 0:\n",
    "            continue\n",
    "        new_words[i] = random.choice(lemmas)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def augment_dataframe(df):\n",
    "    augmented_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        augmented_rows.append(row.to_dict())\n",
    "        if random.random() < AUGMENTED_COPY_CHANCE:\n",
    "            new_row = row.copy()\n",
    "            if random.random() < 0.5:\n",
    "                new_row[\"claim\"] = synonym_replacement(row[\"claim\"], n=1)\n",
    "            else:\n",
    "                new_row[\"evidence\"] = synonym_replacement(row[\"evidence\"], n=1)\n",
    "            augmented_rows.append(new_row.to_dict())\n",
    "    return pd.DataFrame(augmented_rows)\n",
    "\n",
    "augmented_train_df = augment_dataframe(train_df)\n",
    "if TESTING_FLAG:\n",
    "    print(\"Original train size:\", len(train_df),\n",
    "          \"=> After augmentation:\", len(augmented_train_df))\n",
    "\n",
    "train_df = augmented_train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:23.602490Z",
     "iopub.status.busy": "2025-04-01T18:37:23.602190Z",
     "iopub.status.idle": "2025-04-01T18:37:23.723215Z",
     "shell.execute_reply": "2025-04-01T18:37:23.722511Z",
     "shell.execute_reply.started": "2025-04-01T18:37:23.602466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['claim', 'evidence', 'label'],\n",
      "        num_rows: 24819\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['claim', 'evidence', 'label'],\n",
      "        num_rows: 5926\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['claim', 'evidence'],\n",
      "        num_rows: 4688\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 4. Create Hugging Face Datasets\n",
    "# ----------------------------------------------------------------------------\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset   = Dataset.from_pandas(dev_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"dev\":   dev_dataset,\n",
    "    \"test\":  test_dataset\n",
    "})\n",
    "\n",
    "if TESTING_FLAG:\n",
    "    print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:23.724235Z",
     "iopub.status.busy": "2025-04-01T18:37:23.724033Z",
     "iopub.status.idle": "2025-04-01T18:37:33.660871Z",
     "shell.execute_reply": "2025-04-01T18:37:33.659994Z",
     "shell.execute_reply.started": "2025-04-01T18:37:23.724217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7dce947c8f480692ef933569478812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833ada05e850408e9e98d1cd04b92aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d85831a3514ecdac6a2e55605beb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 5. Tokenization\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "TOKENIZER_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"claim\"],\n",
    "        examples[\"evidence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "encoded_dataset = dataset_dict.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:33.662050Z",
     "iopub.status.busy": "2025-04-01T18:37:33.661785Z",
     "iopub.status.idle": "2025-04-01T18:37:33.674074Z",
     "shell.execute_reply": "2025-04-01T18:37:33.673443Z",
     "shell.execute_reply.started": "2025-04-01T18:37:33.662016Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 5.1. Format for PyTorch\n",
    "# ----------------------------------------------------------------------------\n",
    "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].rename_column(\"label\", \"labels\")\n",
    "encoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].rename_column(\"label\", \"labels\")\n",
    "\n",
    "encoded_dataset[\"train\"] = encoded_dataset[\"train\"].remove_columns([\"claim\", \"evidence\"])\n",
    "encoded_dataset[\"dev\"]   = encoded_dataset[\"dev\"].remove_columns([\"claim\", \"evidence\"])\n",
    "encoded_dataset[\"test\"]  = encoded_dataset[\"test\"].remove_columns([\"claim\", \"evidence\"])\n",
    "\n",
    "encoded_dataset[\"train\"].set_format(\"torch\")\n",
    "encoded_dataset[\"dev\"].set_format(\"torch\")\n",
    "encoded_dataset[\"test\"].set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:33.675178Z",
     "iopub.status.busy": "2025-04-01T18:37:33.674972Z",
     "iopub.status.idle": "2025-04-01T18:37:34.016712Z",
     "shell.execute_reply": "2025-04-01T18:37:34.015818Z",
     "shell.execute_reply.started": "2025-04-01T18:37:33.675160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe from data\\glove.6B.300d.txt...\n",
      "Initialized embedding_matrix with 26695 GloVe tokens matched out of 30522\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 6. Load GloVe embeddings & Build an Embedding Matrix\n",
    "# ----------------------------------------------------------------------------\n",
    "def load_glove_embeddings(glove_file, vocab, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Load GloVe 300d vectors and align them with the given vocab.\n",
    "    vocab: a dict {token_string: token_index}\n",
    "    Returns: a numpy array [vocab_size, embedding_dim]\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.random.normal(\n",
    "        scale=0.1, \n",
    "        size=(len(vocab), embedding_dim)\n",
    "    ).astype(np.float32)\n",
    "    found = 0\n",
    "\n",
    "    if not os.path.isfile(glove_file):\n",
    "        print(f\"GloVe file not found at {glove_file}, using random init.\")\n",
    "        return embedding_matrix, found\n",
    "\n",
    "    print(f\"Loading GloVe from {glove_file}...\")\n",
    "    glove_dict = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            if len(values) != embedding_dim + 1:\n",
    "                continue\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove_dict[word] = coefs\n",
    "\n",
    "    # For each token in the BERT-based vocab, see if it matches a GloVe word\n",
    "    for token, idx in vocab.items():\n",
    "        normalized = token.replace(\"##\", \"\").lower()\n",
    "        if normalized in glove_dict:\n",
    "            embedding_matrix[idx] = glove_dict[normalized]\n",
    "            found += 1\n",
    "\n",
    "    print(f\"Initialized embedding_matrix with {found} GloVe tokens matched out of {len(vocab)}\")\n",
    "    return embedding_matrix, found\n",
    "\n",
    "vocab_dict = tokenizer.get_vocab()  # {token_str: token_id}\n",
    "embedding_matrix_np, glove_found = load_glove_embeddings(GLOVE_PATH, vocab_dict, EMBED_DIM)\n",
    "embedding_matrix_tensor = torch.tensor(embedding_matrix_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:34.017986Z",
     "iopub.status.busy": "2025-04-01T18:37:34.017647Z",
     "iopub.status.idle": "2025-04-01T18:37:34.030585Z",
     "shell.execute_reply": "2025-04-01T18:37:34.029921Z",
     "shell.execute_reply.started": "2025-04-01T18:37:34.017954Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 7. Custom BiLSTM with optional attention\n",
    "# ----------------------------------------------------------------------------\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple additive attention: \n",
    "    score = tanh(W1*H + W2*h_context), \n",
    "    then softmax over time steps, \n",
    "    output = sum of weighted hidden states\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.W = nn.Linear(2 * hidden_dim, 2 * hidden_dim)\n",
    "        self.v = nn.Linear(2 * hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        lstm_outputs: (B, L, 2*hidden_dim)\n",
    "        mask: (B, L) if needed (1 for real tokens, 0 for pad)\n",
    "        Returns: (B, 2*hidden_dim) - the weighted sum\n",
    "        \"\"\"\n",
    "        # Score calculation\n",
    "        score = torch.tanh(self.W(lstm_outputs))  # (B, L, 2H)\n",
    "        score = self.v(score).squeeze(-1)         # (B, L)\n",
    "        \n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_weights = F.softmax(score, dim=-1)   # (B, L)\n",
    "\n",
    "        # Weighted sum\n",
    "        attn_weights = attn_weights.unsqueeze(1)  # (B, 1, L)\n",
    "        context = torch.bmm(attn_weights, lstm_outputs)  # (B, 1, 2H)\n",
    "        context = context.squeeze(1)              # (B, 2H)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class CustomBiLSTMModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embed_dim=300, \n",
    "                 hidden_dim=256, \n",
    "                 num_labels=2, \n",
    "                 num_layers=2,\n",
    "                 dropout=0.3,\n",
    "                 use_attention=True,\n",
    "                 use_focal_loss=False, \n",
    "                 gamma=2.0, \n",
    "                 label_smoothing=0.0,\n",
    "                 embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            with torch.no_grad():\n",
    "                self.embedding.weight.copy_(embedding_matrix)\n",
    "\n",
    "        # BiLSTM with multiple layers & dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Optional attention\n",
    "        if self.use_attention:\n",
    "            self.attn = SimpleAttention(self.hidden_dim)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(2 * self.hidden_dim, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        # Embeddings\n",
    "        embeds = self.embedding(input_ids)\n",
    "        # zero out padding\n",
    "        if attention_mask is not None:\n",
    "            expand_mask = attention_mask.unsqueeze(-1).float()\n",
    "            embeds = embeds * expand_mask\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_outputs, (h, c) = self.lstm(embeds)\n",
    "        # shape of lstm_outputs: (B, L, 2H)\n",
    "\n",
    "        if self.use_attention:\n",
    "            # Weighted sum of outputs\n",
    "            context = self.attn(lstm_outputs, mask=attention_mask)\n",
    "        else:\n",
    "            # h shape: (num_layers*2, B, H)\n",
    "            h_forward = h[-2]  # last layer's forward state\n",
    "            h_backward = h[-1] # last layer's backward state\n",
    "            context = torch.cat((h_forward, h_backward), dim=-1)  # (B, 2H)\n",
    "\n",
    "        logits = self.classifier(context)\n",
    "\n",
    "        # Loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.use_focal_loss:\n",
    "                loss = self.focal_loss(logits, labels, self.gamma)\n",
    "            else:\n",
    "                loss = self.label_smoothing_loss(logits, labels, self.label_smoothing)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def focal_loss(self, logits, targets, gamma=2.0):\n",
    "        ce = nn.CrossEntropyLoss(reduction='none')(logits, targets)\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = (1 - pt)**gamma * ce\n",
    "        return focal.mean()\n",
    "\n",
    "    def label_smoothing_loss(self, logits, targets, smoothing=0.0):\n",
    "        if smoothing == 0.0:\n",
    "            return nn.CrossEntropyLoss()(logits, targets)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        n_class = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(smoothing / (n_class - 1))\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T18:37:34.031558Z",
     "iopub.status.busy": "2025-04-01T18:37:34.031361Z",
     "iopub.status.idle": "2025-04-01T21:12:52.343743Z",
     "shell.execute_reply": "2025-04-01T21:12:52.343084Z",
     "shell.execute_reply.started": "2025-04-01T18:37:34.031541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 06:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.572300</td>\n",
       "      <td>0.548748</td>\n",
       "      <td>0.725859</td>\n",
       "      <td>0.756666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.533937</td>\n",
       "      <td>0.756262</td>\n",
       "      <td>0.770503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.515400</td>\n",
       "      <td>0.531805</td>\n",
       "      <td>0.759093</td>\n",
       "      <td>0.774553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>0.527692</td>\n",
       "      <td>0.766745</td>\n",
       "      <td>0.776578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 1.0, 'label_smoothing': 0.06681121703375936, 'learning_rate': 2.26853860592903e-05, 'use_focal_loss': False} => F1=0.7667\n",
      "  3%|▎         | 1/30 [06:47<3:16:49, 407.23s/trial, best loss: -0.7667449412571927]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3104' max='3104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3104/3104 03:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.588400</td>\n",
       "      <td>0.574721</td>\n",
       "      <td>0.807303</td>\n",
       "      <td>0.805940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.544400</td>\n",
       "      <td>0.563702</td>\n",
       "      <td>0.813054</td>\n",
       "      <td>0.820790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 2, 'gamma': 4.0, 'label_smoothing': 0.15302443824756262, 'learning_rate': 0.0003175164324673178, 'use_focal_loss': False} => F1=0.8131\n",
      "  7%|▋         | 2/30 [10:45<2:23:34, 307.68s/trial, best loss: -0.813054053785342] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6208' max='6208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6208/6208 06:04, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.031084</td>\n",
       "      <td>0.744076</td>\n",
       "      <td>0.765947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.029849</td>\n",
       "      <td>0.768259</td>\n",
       "      <td>0.777759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.029595</td>\n",
       "      <td>0.766371</td>\n",
       "      <td>0.781809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>0.776099</td>\n",
       "      <td>0.784003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 4, 'gamma': 4.0, 'label_smoothing': 0.08629548205814437, 'learning_rate': 2.7229621451696198e-05, 'use_focal_loss': True} => F1=0.7761\n",
      " 10%|█         | 3/30 [17:03<2:32:55, 339.85s/trial, best loss: -0.813054053785342]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 19:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.543100</td>\n",
       "      <td>0.534424</td>\n",
       "      <td>0.766883</td>\n",
       "      <td>0.781303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>0.518958</td>\n",
       "      <td>0.783978</td>\n",
       "      <td>0.794803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.493500</td>\n",
       "      <td>0.530159</td>\n",
       "      <td>0.785153</td>\n",
       "      <td>0.796153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 2.0, 'label_smoothing': 0.05624491659909645, 'learning_rate': 3.692355129532768e-05, 'use_focal_loss': False} => F1=0.7852\n",
      " 13%|█▎        | 4/30 [37:01<4:54:08, 678.79s/trial, best loss: -0.813054053785342]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 30:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>0.563561</td>\n",
       "      <td>0.742869</td>\n",
       "      <td>0.763415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.547400</td>\n",
       "      <td>0.553198</td>\n",
       "      <td>0.758994</td>\n",
       "      <td>0.770165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.537800</td>\n",
       "      <td>0.559933</td>\n",
       "      <td>0.761026</td>\n",
       "      <td>0.776747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 4.0, 'label_smoothing': 0.08316079296536183, 'learning_rate': 1.617464213164228e-05, 'use_focal_loss': False} => F1=0.7610\n",
      " 17%|█▋        | 5/30 [1:07:56<7:39:31, 1102.87s/trial, best loss: -0.813054053785342]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 36:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.614200</td>\n",
       "      <td>0.603588</td>\n",
       "      <td>0.802042</td>\n",
       "      <td>0.808134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.585600</td>\n",
       "      <td>0.595846</td>\n",
       "      <td>0.812853</td>\n",
       "      <td>0.820621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>0.598522</td>\n",
       "      <td>0.816363</td>\n",
       "      <td>0.821465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 4.0, 'label_smoothing': 0.19409398476588627, 'learning_rate': 0.0001286741755187964, 'use_focal_loss': False} => F1=0.8164\n",
      " 20%|██        | 6/30 [1:44:29<9:49:26, 1473.61s/trial, best loss: -0.8163627008008161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 17:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>0.123541</td>\n",
       "      <td>0.762632</td>\n",
       "      <td>0.774047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.115615</td>\n",
       "      <td>0.789968</td>\n",
       "      <td>0.797671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.121139</td>\n",
       "      <td>0.790363</td>\n",
       "      <td>0.799528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 2.0, 'label_smoothing': 0.12234191285484404, 'learning_rate': 2.8460418852571993e-05, 'use_focal_loss': True} => F1=0.7904\n",
      " 23%|██▎       | 7/30 [2:01:57<8:31:28, 1334.29s/trial, best loss: -0.8163627008008161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18615' max='18615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18615/18615 16:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.553200</td>\n",
       "      <td>0.537159</td>\n",
       "      <td>0.753606</td>\n",
       "      <td>0.769490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>0.528220</td>\n",
       "      <td>0.773611</td>\n",
       "      <td>0.780796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.542757</td>\n",
       "      <td>0.765226</td>\n",
       "      <td>0.779109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 4, 'epochs': 3, 'gamma': 4.5, 'label_smoothing': 0.05864078807441606, 'learning_rate': 2.4503233661545793e-05, 'use_focal_loss': False} => F1=0.7736\n",
      " 27%|██▋       | 8/30 [2:18:29<7:29:20, 1225.46s/trial, best loss: -0.8163627008008161]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4656' max='4656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4656/4656 03:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.027922</td>\n",
       "      <td>0.795607</td>\n",
       "      <td>0.796153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.027044</td>\n",
       "      <td>0.807738</td>\n",
       "      <td>0.806615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.027703</td>\n",
       "      <td>0.811973</td>\n",
       "      <td>0.816065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='741' max='741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [741/741 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hyperopt] params={'batch_size': 16, 'epochs': 3, 'gamma': 4.0, 'label_smoothing': 0.07819207728669558, 'learning_rate': 0.000163256156163078, 'use_focal_loss': True} => F1=0.8120\n",
      " 30%|███       | 9/30 [2:22:31<5:21:19, 918.08s/trial, best loss: -0.8163627008008161] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Backe\\AppData\\Local\\Temp\\ipykernel_10416\\3536190419.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1419' max='12412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1419/12412 00:54 < 07:04, 25.92 it/s, Epoch 0.46/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [2:23:27<5:34:44, 956.40s/trial, best loss: -0.8163627008008161]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39mf1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n\u001b[0;32m     73\u001b[0m trials \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[1;32m---> 74\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEARCH_SPACE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_EVALS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TESTING_FLAG:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHyperopt best param indices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best)\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\hyperopt\\fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\hyperopt\\base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[1;32mIn[8], line 65\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(space)\u001b[0m\n\u001b[0;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     64\u001b[0m trainer \u001b[38;5;241m=\u001b[39m make_trainer(model, encoded_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], encoded_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m], space)\n\u001b[1;32m---> 65\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(encoded_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     67\u001b[0m f1 \u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\transformers\\trainer.py:2611\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2607\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m   2609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2611\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2615\u001b[0m \u001b[38;5;66;03m# get leaning rate before update\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\accelerate\\optimizer.py:178\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\torch\\optim\\adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    233\u001b[0m         group,\n\u001b[0;32m    234\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m         state_steps,\n\u001b[0;32m    241\u001b[0m     )\n\u001b[1;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\torch\\optim\\adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Backe\\Documents\\Manchester Uni\\Year 3\\NLU - Project\\venv\\lib\\site-packages\\torch\\optim\\adamw.py:699\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    697\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 699\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    701\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    702\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 8. Build Trainer & Hyperopt\n",
    "# ----------------------------------------------------------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "def make_trainer(model, train_ds, dev_ds, space):\n",
    "    learning_rate = space[\"learning_rate\"]\n",
    "    epochs        = int(space[\"epochs\"])\n",
    "    batch_size    = int(space[\"batch_size\"])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./enhanced-bilstm-ed-checkpoints\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=BEST_MODEL_METRIC,\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        logging_steps=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=dev_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def objective(space):\n",
    "    use_focal_loss = space[\"use_focal_loss\"]\n",
    "    gamma = space[\"gamma\"]\n",
    "    label_smoothing = space[\"label_smoothing\"]\n",
    "\n",
    "    # Build model\n",
    "    model = CustomBiLSTMModel(\n",
    "        vocab_size=len(tokenizer.get_vocab()),\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_labels=2,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        use_attention=USE_ATTENTION,\n",
    "        use_focal_loss=use_focal_loss,\n",
    "        gamma=gamma,\n",
    "        label_smoothing=label_smoothing,\n",
    "        embedding_matrix=embedding_matrix_tensor\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    trainer = make_trainer(model, encoded_dataset[\"train\"], encoded_dataset[\"dev\"], space)\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate(encoded_dataset[\"dev\"])\n",
    "    f1 = metrics[\"eval_f1\"]\n",
    "\n",
    "    if TESTING_FLAG:\n",
    "        print(f\"[Hyperopt] params={space} => F1={f1:.4f}\")\n",
    "    return {\"loss\": -f1, \"status\": STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=SEARCH_SPACE,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=MAX_EVALS,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "if TESTING_FLAG:\n",
    "    print(\"\\nHyperopt best param indices:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-01T21:12:52.344772Z",
     "iopub.status.busy": "2025-04-01T21:12:52.344498Z",
     "iopub.status.idle": "2025-04-01T21:12:52.349967Z",
     "shell.execute_reply": "2025-04-01T21:12:52.349331Z",
     "shell.execute_reply.started": "2025-04-01T21:12:52.344747Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m BATCH_OPTIONS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m]\n\u001b[0;32m      6\u001b[0m USE_FOCAL_OPTIONS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[0;32m      8\u001b[0m final_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m:    \u001b[43mbest\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m:           EPOCH_OPTIONS[ best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] ],\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m:       BATCH_OPTIONS[ best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] ],\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_focal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:   USE_FOCAL_OPTIONS[ best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_focal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] ],\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m:            best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_smoothing\u001b[39m\u001b[38;5;124m\"\u001b[39m:  best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_smoothing\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TESTING_FLAG:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterpreted best hyperparams:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, final_params)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best' is not defined"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 8.1 Interpret Best Hyperparams\n",
    "# ----------------------------------------------------------------------------\n",
    "EPOCH_OPTIONS = [2, 3, 4]\n",
    "BATCH_OPTIONS = [4, 8, 16]\n",
    "USE_FOCAL_OPTIONS = [False, True]\n",
    "\n",
    "final_params = {\n",
    "    \"learning_rate\":    best[\"learning_rate\"],\n",
    "    \"epochs\":           EPOCH_OPTIONS[ best[\"epochs\"] ],\n",
    "    \"batch_size\":       BATCH_OPTIONS[ best[\"batch_size\"] ],\n",
    "    \"use_focal_loss\":   USE_FOCAL_OPTIONS[ best[\"use_focal_loss\"] ],\n",
    "    \"gamma\":            best[\"gamma\"],\n",
    "    \"label_smoothing\":  best[\"label_smoothing\"]\n",
    "}\n",
    "\n",
    "if TESTING_FLAG:\n",
    "    print(\"Interpreted best hyperparams:\\n\", final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:12:52.351043Z",
     "iopub.status.busy": "2025-04-01T21:12:52.350765Z",
     "iopub.status.idle": "2025-04-01T21:21:27.556406Z",
     "shell.execute_reply": "2025-04-01T21:21:27.555691Z",
     "shell.execute_reply.started": "2025-04-01T21:12:52.351016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4656' max='4656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4656/4656 04:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.212856</td>\n",
       "      <td>0.801472</td>\n",
       "      <td>0.798515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.173900</td>\n",
       "      <td>0.196101</td>\n",
       "      <td>0.819288</td>\n",
       "      <td>0.819271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.208608</td>\n",
       "      <td>0.823521</td>\n",
       "      <td>0.827202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dev Results: {'eval_loss': 0.20860803127288818, 'eval_f1': 0.8235211803073061, 'eval_accuracy': 0.8272021599730003, 'eval_runtime': 13.1419, 'eval_samples_per_second': 450.926, 'eval_steps_per_second': 56.385, 'epoch': 3.0}\n",
      "\n",
      "Detailed Classification Report (Dev):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8634    0.9041    0.8833      4286\n",
      "           1     0.7142    0.6262    0.6673      1640\n",
      "\n",
      "    accuracy                         0.8272      5926\n",
      "   macro avg     0.7888    0.7652    0.7753      5926\n",
      "weighted avg     0.8221    0.8272    0.8235      5926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 9. Train Final Model\n",
    "# ----------------------------------------------------------------------------\n",
    "best_model = CustomBiLSTMModel(\n",
    "    vocab_size=len(tokenizer.get_vocab()),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    use_attention=USE_ATTENTION,\n",
    "    num_labels=2,\n",
    "    use_focal_loss=final_params[\"use_focal_loss\"],\n",
    "    gamma=final_params[\"gamma\"],\n",
    "    label_smoothing=final_params[\"label_smoothing\"],\n",
    "    embedding_matrix=embedding_matrix_tensor\n",
    ")\n",
    "best_model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final-enhanced-bilstm-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=final_params[\"learning_rate\"],\n",
    "    num_train_epochs=final_params[\"epochs\"],\n",
    "    per_device_train_batch_size=final_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=BEST_MODEL_METRIC,\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"dev\"],\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "results_dev = trainer.evaluate(encoded_dataset[\"dev\"])\n",
    "if TESTING_FLAG:\n",
    "    print(\"Final Dev Results:\", results_dev)\n",
    "\n",
    "preds_output = trainer.predict(encoded_dataset[\"dev\"])\n",
    "dev_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "dev_labels = preds_output.label_ids\n",
    "if TESTING_FLAG:\n",
    "    print(\"\\nDetailed Classification Report (Dev):\")\n",
    "    print(classification_report(dev_labels, dev_preds, digits=4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to data\\ED_B_Model.pt\n"
     ]
    }
   ],
   "source": [
    "#Save the best model\n",
    "\n",
    "# 3. Prepare the dictionary to save\n",
    "save_dict = {\n",
    "    \"model_state_dict\": trainer.model.state_dict(),   # (A) model weights\n",
    "    \"hyperparams\": {                                  # (B) essential model config\n",
    "        \"vocab_size\": len(tokenizer.get_vocab()),\n",
    "        \"embed_dim\": EMBED_DIM,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_labels\": 2,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"use_attention\": USE_ATTENTION,\n",
    "        \"use_focal_loss\": final_params[\"use_focal_loss\"],\n",
    "        \"gamma\": final_params[\"gamma\"],\n",
    "        \"label_smoothing\": final_params[\"label_smoothing\"]\n",
    "    },\n",
    "    \"embedding_matrix\": embedding_matrix_tensor.cpu()  # (C) embedding weights\n",
    "}\n",
    "\n",
    "# 4. Save the entire dictionary to BEST_MODEL_PATH\n",
    "torch.save(save_dict, BEST_MODEL_PATH)\n",
    "print(f\"Best model + hyperparams + embedding matrix saved to: {BEST_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded state_dict into loaded_model.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully loaded state_dict into loaded_model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m loaded_model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "#Loading the best model\n",
    "\n",
    "loaded_model = CustomBiLSTMModel(\n",
    "    vocab_size=len(tokenizer.get_vocab()),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    use_attention=USE_ATTENTION,\n",
    "    num_labels=2,\n",
    "    use_focal_loss=True, # final_params[\"use_focal_loss\"],\n",
    "    gamma=np.float64(1.0), # final_params[\"gamma\"],\n",
    "    label_smoothing= np.float64(0.03442125718345023), #final_params[\"label_smoothing\"] ,\n",
    "    embedding_matrix=embedding_matrix_tensor\n",
    ")\n",
    "\n",
    "loaded_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "print(\"Successfully loaded state_dict into loaded_model.\")\n",
    "\n",
    "trainer.model = loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:21:27.558768Z",
     "iopub.status.busy": "2025-04-01T21:21:27.558526Z",
     "iopub.status.idle": "2025-04-01T21:21:51.176503Z",
     "shell.execute_reply": "2025-04-01T21:21:51.175676Z",
     "shell.execute_reply.started": "2025-04-01T21:21:27.558748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 10. Inference on Test Set\n",
    "# ----------------------------------------------------------------------------\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#You need to build the encoded dataset again, if you want to use the same tokeniser.\n",
    "test_loader = DataLoader(encoded_dataset[\"test\"], batch_size=8)\n",
    "\n",
    "all_preds = []\n",
    "loaded_model.eval()\n",
    "\n",
    "for batch in test_loader:\n",
    "    for key in batch:\n",
    "        batch[key] = batch[key].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**batch)       \n",
    "    logits = outputs[\"logits\"]               \n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "# Convert to a DataFrame and save\n",
    "test_pred_df = pd.DataFrame({\"prediction\": all_preds})\n",
    "test_pred_df.to_csv(OUTPUT_PATH, index=False, header=True)\n",
    "print(\"Saved predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dev predictions to dev_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Inference on Dev Set (codebench debugging)\n",
    "# ----------------------------------------------------------------------------\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dev_loader = DataLoader(encoded_dataset[\"dev\"], batch_size=8)\n",
    "\n",
    "all_dev_preds = []\n",
    "loaded_model.eval()\n",
    "\n",
    "for batch in dev_loader:\n",
    "    for key in batch:\n",
    "        batch[key] = batch[key].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**batch) \n",
    "    logits = outputs[\"logits\"]          \n",
    "    preds = torch.argmax(logits, dim=1) \n",
    "    all_dev_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "\n",
    "dev_pred_df = pd.DataFrame({\"prediction\": all_dev_preds})\n",
    "dev_pred_df.to_csv(\"dev_predictions.csv\", index=False, header=True)\n",
    "\n",
    "print(\"Saved dev predictions to dev_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6952367,
     "sourceId": 11144868,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
